[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Statistiques, Data Science & Projets Num√©riques - Djamal TOE",
    "section": "",
    "text": "Bienvenue sur mon site personnel.\nJe suis Djamal Y. TOE, statisticien passionn√© par les analyses avanc√©es, la visualisation de donn√©es, et la r√©solution de probl√®mes complexes √† travers des approches quantitatives. Ce site pr√©sente mes projets, mes recherches, et mes contributions dans le domaine des statistiques et de la science des donn√©es. Je suis titulaire d‚Äôune licence professionnelle en statistiques-informatique et actuellement √©l√®ve ing√©nieur en Data Science √† l‚ÄôEcole Nationale de la statistique et de l‚ÄôAnalyse de l‚ÄôInformation √† Bruz Rennes, France."
  },
  {
    "objectID": "about.html#√†-propos-de-moi",
    "href": "about.html#√†-propos-de-moi",
    "title": "Statistiques, Data Science & Projets Num√©riques - Djamal TOE",
    "section": "√Ä propos de moi",
    "text": "√Ä propos de moi\nJe combine mes comp√©tences en statistiques, programmation et analyse de donn√©es pour transformer des ensembles de donn√©es en informations exploitables. Mon objectif est d‚Äôam√©liorer la prise de d√©cision gr√¢ce √† des mod√®les et des m√©thodes robustes. Ayant effectuer des stages en entreprises, j‚Äôai appris beaucoup de choses notamment en bio-statistiques et sur les mod√©lisations qui y sont utilis√©es. J‚Äôai √©galement des connaissance en cartographie (avec R)."
  },
  {
    "objectID": "about.html#exp√©rience",
    "href": "about.html#exp√©rience",
    "title": "Statistiques, Data Science & Projets Num√©riques - Djamal TOE",
    "section": "Exp√©rience",
    "text": "Exp√©rience\n\n\nProjet Statistique | Ecole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation, Rennes Frances\ndur√©e : 5 mois Fin D√©cembre 2024 ‚Äì √† D√©but Mai 2025\nTravail effectu√© :\n\nR√©daction de rapports d‚Äôanalyse statistique, incluant la m√©thodologie et l‚Äôinterpr√©tation des r√©sultats.\nAnalyse exploratoire des donn√©es pour identifier les tendances et structures sous-jacentes.\nR√©alisation de tests statistiques afin de valider les hypoth√®ses et √©valuer la significativit√© des r√©sultats.\nAnalyse des correspondances multiples (ACM) pour explorer les relations entre variables cat√©gorielles.\nGithub pour le travail en √©quipe\n\nProjet Traitement de donn√©es | Ecole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation, Rennes Frances\ndur√©e : 3 mois F√©vrier 2025 ‚Äì √† Mai 2025\nTravail effectu√© :\n\nPython (Pandas, Numpy, Scikit-learn)\nVisualisation des donn√©es seaborn, matplotlib\nD√©v√©loppement d‚Äôune application pour les utilisateurs avec pyhton shiny\nGithub pour le travail en √©quipe\nR√©daction de rapports des traitements effectu√©s sur les donn√©es (Rmarkdown)\n\nProjet d‚Äô√©conomie | Ecole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation, Rennes Frances\ndur√©e : 4 mois Fin Novembre 2024 ‚Äì √† Mars 2025\nTravail effectu√© :\n\nR√©vue de litt√©rature\nSynth√®se et r√©sum√© des √©tudes\nRedaction du rapport\nDiscussion des m√©thodes √©conom√©triques utiilis√©es\nTravail en √©quipe\n\nStagiaire au Centre de M√©thodologie et de Gestion de donn√©es au Centre MURAZ sis Bobo-Dioulasso, Burkina Faso & l‚ÄôInstitut National de recherche en Science de la Sant√© sis Bobo-Dioulasso, Burkina Faso\n\ndur√©e : 10 mois Fin Juillet 2023 - Fin Avril 2024\nTravail effectu√© :\n\nAnalyse exploratoire de donn√©es\nTests statistiques et Mod√©lisations\nSyst√®me d‚Äôinformation g√©ographique\nRedaction automatique de rapports\nTravail en equipe (d√©coupage de la ville de Bobo-Dioulasso en plusieurs sites pour le deploiement des agents de collecte de donn√©es) sur le projet d‚Äôaide des personnes ag√©es (MAAKOROBA)\nGestion des analyses de routines du service (ACP, AFC, ACM, Regressions)\nM√©thode d‚Äôanalyse factorielles pour la reduction de dimensionnalit√©s, calcul des taux d‚Äôinerties modifi√©s de Jean-Paul BENZECRI\nApplication de l‚Äôalgorithme du K-Nearest Neighbour pour la classification\n\n\nJourn√©es Internationales de la S√©curit√© Sanitaire des Aliments 2024\n\ndur√©e : 1 mois Mars 2024 - Avril 2024\nTravail effectu√© :\n\nConception d‚Äôun formulaire interactif pour la collecte des drafts.\nD√©veloppement d‚Äôune application d√©di√©e au traitement et √† l‚Äôexport des donn√©es au format appropri√©.\nAutomatisation du processus pour optimiser l‚Äôanalyse et la gestion des drafts.\nPython, KoboCollect, Microsoft Word et Microsoft Excel"
  },
  {
    "objectID": "about.html#projets-personnels",
    "href": "about.html#projets-personnels",
    "title": "Statistiques, Data Science & Projets Num√©riques - Djamal TOE",
    "section": "Projets personnels",
    "text": "Projets personnels\n\n\n1. Analyses Factorielles et Visualisations\n\nAnalyses factorielles (ACP, AFC, ACM, AFM, AFD) pour comprendre les structures complexes des donn√©es.\nVisualisation interactive des r√©sultats pour une meilleure interpr√©tation.\n\n\n\n\n2. Mod√®les de R√©gression et Pr√©vision\n\n\nR√©gression lin√©aire, logistique, et mixte\nPr√©visions √† l‚Äôaide de mod√®les de s√©ries temporelles (pas trop avanc√©)\n\n\n\n\n3. Applications Statistiques\n\n\nD√©veloppement d‚Äôoutils interactifs pour l‚Äôanalyse de donn√©es (Shiny, Quarto)\nRapports automatis√©s (Rmarkdown, Bookdown)\n\n\n\n\n4. Applications Bureau et Web\n\n\nD√©v√©loppement de logiciel bureau pour la gestion des caisses\nD√©v√©loppement de sites web avec python&Django (pas trop avanc√©)\n\n\n\n\n5. Computer vision\n\nJe d√©bute dans la vision par ordinateur avec :\n\nLa SVM (Support Vector Machine)\nLe KNN (K- Nearest Neighbour)\nL‚ÄôACP (L‚ÄôAnalyse en Composante Principale)\nLes reseaux de neurones convolutionnels (en cours d‚Äôapprentissage)\n\n\n\n\n6. Langages de programmtion et outils statistiques\n\n\nPython, Java, C++ & C\nR, Stata, SPSS (Moyen)\nHtml, Css\nPower Bi\nOffice et Suites\nSyst√®me de Gestion de donn√©es :\n\nMySql\nOracle SQL"
  },
  {
    "objectID": "about.html#derni√®res-publications",
    "href": "about.html#derni√®res-publications",
    "title": "Statistiques, Data Science & Projets Num√©riques - Djamal TOE",
    "section": "Derni√®res publications üìö",
    "text": "Derni√®res publications üìö\n\nüìà Mod√©lisation statistique | Machine Learning | Deep Learning\n\nPr√©dire la dur√©e de carri√®re des joueurs NBA\n\nUne approche par r√©gression lin√©aire supervis√©e\n\nPr√©dire le diab√®te chez les femmes √† l‚Äôaide d‚Äôune r√©gression logistique\n\nOptimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning\n\n√âvaluation de l‚Äôimpact d‚Äôune intervention sur les cas de paludisme\n\nMod√©lisation des donn√©es de comptage par r√©gression de Poisson\n\nDiagnostic tumeurs c√©r√©brales - Reseau de neurones\n\nClassification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation d‚Äôun reseau de neuronnes convolutionnel\n\nReduction de dimensionnalit√©, clustering non supervis√©\n\nUtilisation de l‚ÄôAnalyse en Correspondance Principale et des K-moyennes pour classifier 167 pays selon leurs caract√©ristiques socio-√©conomiques\n\nDe l‚Äôanalyse multivari√©e √† la pr√©diction : un parcours combin√© entre ACP, KNN et r√©gression logistique avec des donn√©es m√©dicales\n\nMod√®le √† variable d√©pendante binaire appliqu√© √† un jeu de donn√©es m√©dical et machine learning\n\nClasser les gestes de la main (pierre, feuille, ciseau)\n\nUtilisation du reseau de neurones convolutionnel et de yolov8\n\n\n\n\n\nü§ñ Programmation et projets interactifs\n\nCr√©er un assistant virtuel avec commandes vocales en Python\n\nMini-projet m√™lant reconnaissance vocale, traduction et intelligence artificielle\n\nKedjeBoost ‚Äì Votre resto, en mode turbo !\n\nApplication desktop avec Java et Mysql\n\n\n\n\n\nüîç Analyse exploratoire et visualisation\n\nExploration des techniques d‚Äôanalyse factorielle\n\nPCA, AFC, ACM sur des jeux de donn√©es\n\nCr√©ation de cartes th√©matiques avec R (choropl√®thes, proportions)\n\nUtilisation des packages sf, tmap, leaflet\n\n\n\n\n\nüßë‚Äçüè´ Formations et bonnes pratiques\n\nR√©aliser des pr√©sentations dynamiques avec R et RStudio Utilisation de Quarto, Reveal.js et astuces pour pr√©senter efficacement"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "",
    "text": "Dans un monde o√π les indicateurs de d√©veloppement sont multiples (revenu, sant√©, √©ducation, acc√®s aux services, environnement), il devient crucial de synth√©tiser l‚Äôinformation pour comprendre les grands profils qui distinguent les pays. Pour ce faire, nous mobilisons deux techniques statistiques puissantes et compl√©mentaires :\n\nL‚Äôanalyse en composantes principales (ACP), qui permet de r√©duire la dimension des donn√©es en identifiant les axes principaux de variation entre les pays,\nLe clustering par K-means, qui regroupe les pays selon leurs profils de d√©veloppement similaires dans l‚Äôespace d√©fini par l‚ÄôACP.\n\nCette approche nous permettra : - D‚Äôidentifier visuellement les dimensions cl√©s du d√©veloppement, - De regrouper les pays en classes homog√®nes, facilitant ainsi l‚Äôanalyse comparative.\nNous appliquerons cette d√©marche sur un ensemble de variables d√©crivant les niveaux de vie, l‚Äô√©ducation, la sant√©, l‚Äôenvironnement et l‚Äôacc√®s aux services, dans le but de dresser une cartographie synth√©tique et interpr√©table des grandes cat√©gories de pays √† travers le monde.\n\n\n\n\n\nLe jeu de donn√©es utilis√© dans cette analyse provient de Kaggle et regroupe plusieurs indicateurs socio-√©conomiques et de sant√© pour 167 pays.\n\n\n\nL‚ÄôACP est une m√©thode statistique de r√©duction de dimensionnalit√©. Elle transforme un grand nombre de variables corr√©l√©es en un nombre plus petit de variables non corr√©l√©es appel√©es composantes principales. Ces composantes capturent l‚Äôessentiel de la variation pr√©sente dans les donn√©es originales.\nPourquoi utiliser l‚ÄôACP ?\n- Simplifier l‚Äôanalyse en r√©duisant la complexit√© des donn√©es multidimensionnelles,\n- Visualiser facilement les relations entre observations et variables,\n- Mettre en √©vidence les structures sous-jacentes dans les donn√©es.\n\n\n\n¬†¬†¬†¬†¬†¬†K-means est une m√©thode de classification non supervis√©e qui consiste √† regrouper un ensemble d‚Äôobservations en K clusters (groupes), o√π chaque observation appartient au cluster dont elle est la plus proche selon une mesure de distance (souvent euclidienne).\nPourquoi utiliser K-means ?\n\nIdentifier des groupes homog√®nes dans les donn√©es,\n\nFaciliter l‚Äôinterpr√©tation en cat√©gorisant les observations,\n\nD√©tecter des profils ou comportements similaires.\n\n\n\n\nL‚ÄôACP et le K-means sont souvent utilis√©s conjointement car ils se compl√®tent parfaitement :\n- ACP pr√©pare les donn√©es en r√©duisant leur dimension, en supprimant le bruit et les redondances, ce qui facilite la visualisation et la compr√©hension,\n- K-means exploite l‚Äôespace r√©duit par l‚ÄôACP pour effectuer un regroupement plus robuste et plus interpr√©table, √©vitant les probl√®mes li√©s √† la mal√©diction de la dimension.\nAinsi, l‚Äôassociation ACP + K-means permet d‚Äôanalyser efficacement des donn√©es complexes, en identifiant √† la fois les principales dimensions d‚Äôinfluence et les groupes d‚Äôobservations partageant des caract√©ristiques communes.\n\n\n\n\n\n\n\nCode\nrm(list=ls())\n\n##--- package √† installer\npackages &lt;- c(\n  \"dplyr\",\"cluster\",\n  \"ggplot2\",\"factoextra\",\n  \"FactoMineR\", \"pheatmap\",\n  \"ggrepel\", \"patchwork\"\n)\n\n##-- Boucle pour installer et charger les packages\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}\n\n\n\n\n\n\n\nCode\ndf &lt;- read.csv('data_country.csv')\n# v√©rification\nglimpse(df)\n\n\nRows: 167\nColumns: 10\n$ Country    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Antigua and‚Ä¶\n$ child_mort &lt;dbl&gt; 90.2, 16.6, 27.3, 119.0, 10.3, 14.5, 18.1, 4.8, 4.3, 39.2, ‚Ä¶\n$ exports    &lt;dbl&gt; 10.0, 28.0, 38.4, 62.3, 45.5, 18.9, 20.8, 19.8, 51.3, 54.3,‚Ä¶\n$ health     &lt;dbl&gt; 7.58, 6.55, 4.17, 2.85, 6.03, 8.10, 4.40, 8.73, 11.00, 5.88‚Ä¶\n$ imports    &lt;dbl&gt; 44.9, 48.6, 31.4, 42.9, 58.9, 16.0, 45.3, 20.9, 47.8, 20.7,‚Ä¶\n$ income     &lt;int&gt; 1610, 9930, 12900, 5900, 19100, 18700, 6700, 41400, 43200, ‚Ä¶\n$ inflation  &lt;dbl&gt; 9.440, 4.490, 16.100, 22.400, 1.440, 20.900, 7.770, 1.160, ‚Ä¶\n$ life_expec &lt;dbl&gt; 56.2, 76.3, 76.5, 60.1, 76.8, 75.8, 73.3, 82.0, 80.5, 69.1,‚Ä¶\n$ total_fer  &lt;dbl&gt; 5.82, 1.65, 2.89, 6.16, 2.13, 2.37, 1.69, 1.93, 1.44, 1.92,‚Ä¶\n$ gdpp       &lt;int&gt; 553, 4090, 4460, 3530, 12200, 10300, 3220, 51900, 46900, 58‚Ä¶\n\n\n\n\n\nDescription des donn√©es\n\n\n\nLe jeu de donn√©es contient les indicateurs suivants avec leurs √©ventuelles significations pour 167 pays :\n\nchild_mort : taux de mortalit√© infantile (pour 1000 naissances vivantes)\nexports et imports : en % du PIB\nhealth : d√©penses de sant√© en % du PIB\nincome : revenu moyen par personne\ninflation : taux d‚Äôinflation\nlife_expec : esp√©rance de vie\ntotal_fer : taux de f√©condit√©\ngdpp : PIB par habitant\n\nLes formats des variables semblent √™tre ad√©quats.\nPour aller plus loin dans le descriptis des donn√©es, on peut taper la commande suivante :\n\n\nCode\nsummary(df)\n\n\n   Country            child_mort        exports            health      \n Length:167         Min.   :  2.60   Min.   :  0.109   Min.   : 1.810  \n Class :character   1st Qu.:  8.25   1st Qu.: 23.800   1st Qu.: 4.920  \n Mode  :character   Median : 19.30   Median : 35.000   Median : 6.320  \n                    Mean   : 38.27   Mean   : 41.109   Mean   : 6.816  \n                    3rd Qu.: 62.10   3rd Qu.: 51.350   3rd Qu.: 8.600  \n                    Max.   :208.00   Max.   :200.000   Max.   :17.900  \n    imports             income         inflation         life_expec   \n Min.   :  0.0659   Min.   :   609   Min.   : -4.210   Min.   :32.10  \n 1st Qu.: 30.2000   1st Qu.:  3355   1st Qu.:  1.810   1st Qu.:65.30  \n Median : 43.3000   Median :  9960   Median :  5.390   Median :73.10  \n Mean   : 46.8902   Mean   : 17145   Mean   :  7.782   Mean   :70.56  \n 3rd Qu.: 58.7500   3rd Qu.: 22800   3rd Qu.: 10.750   3rd Qu.:76.80  \n Max.   :174.0000   Max.   :125000   Max.   :104.000   Max.   :82.80  \n   total_fer          gdpp       \n Min.   :1.150   Min.   :   231  \n 1st Qu.:1.795   1st Qu.:  1330  \n Median :2.410   Median :  4660  \n Mean   :2.948   Mean   : 12964  \n 3rd Qu.:3.880   3rd Qu.: 14050  \n Max.   :7.490   Max.   :105000  \n\n\nOn voit qu‚Äôon des donn√©es qui sont propres ,ce qui est rarement le cas dans des situations r√©√©lles.\n\n\n\nVisualisation des informations sur la distribution des variables\n\n\n\n\n\nCode\nplot_box_plot &lt;- function(variable_name_str, title = \"\") {\n  # V√©rification que la variable existe dans df\n  if(!variable_name_str %in% colnames(df)) {\n    stop(\"La variable sp√©cifi√©e n'existe pas dans la table\")\n  }\n\n  \n  var_titles &lt;- list(\n    \"child_mort\" = \"Taux de mortalit√© infantile\",\n    \"exports\"    = \"Exportations (% du PIB)\",\n    \"health\"     = \"D√©penses de sant√© (% du PIB)\",\n    \"imports\"    = \"Importations (% du PIB)\",\n    \"income\"     = \"Revenu par habitant (en USD)\",\n    \"inflation\"  = \"Taux d'inflation (%)\",\n    \"life_expec\" = \"Esp√©rance de vie (en ann√©es)\",\n    \"total_fer\"  = \"Taux de f√©condit√© (enfants par femme)\",\n    \"gdpp\"       = \"PIB par habitant (en USD)\"\n  )\n  \n  title &lt;- var_titles[[variable_name_str]]\n  # titre par d√©faut\n  if (title == \"\") {\n    title &lt;- paste(\"Distribution de la variable\", variable_name_str)\n  }\n\n  # Cr√©ation du graphique\n  plt &lt;- ggplot(df, aes(y = .data[[variable_name_str]])) +\n    geom_boxplot(fill = \"skyblue\", color = \"darkblue\") +\n    theme_light() +\n    labs(title = title, y = variable_name_str)+\n  theme(plot.title = element_text(size = 9)) \n\n  return(plt)\n}\n\nplots &lt;- lapply(colnames(df[,-1]), plot_box_plot)\nwrap_plots(plots, ncol = 3)\n\n\n\n\n\nDistribution des variables du jeu de donn√©es\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†¬†¬†On calcule les corr√©lation de Pearson quand on suspecte des relations lin√©aires entre les variables, quand celles -ci sont sous forme d‚Äô√©chelle ou de ratio. Ici en plus de ploter le heatmap des variables, nous afficherons celles qui sont significativement corr√©l√©es (Test de corr√©lation de Pearson cf.¬†Annexe 1)\n\n\nCode\n# calcul de la matrice de corr√©lations de pearson\ncor_matrix &lt;- cor(df[, -1], method = \"pearson\")\n\n# fonction pour extraire les p-values\nget_pval &lt;- function(x, y) {\n  res &lt;- suppressWarnings(cor.test(x, y, method = \"pearson\"))\n  return(res$p.value)\n}\n\n# matrice des p-values\nn &lt;- ncol(df[, -1])\npval_matrix &lt;- matrix(NA, nrow = n, ncol = n)\nrownames(pval_matrix) &lt;- colnames(df[, -1])\ncolnames(pval_matrix) &lt;- colnames(df[, -1])\nfor (i in 1:n) {\n  for (j in 1:n) {\n    pval_matrix[i, j] &lt;- get_pval(df[, -1][[i]], df[, -1][[j]])\n  }\n}\n\nget_signif_stars &lt;- function(p) {\n  if (p &lt; 0.01) {\n    return(\"***\")\n  } else if (p &lt; 0.05) {\n    return(\"**\")\n  } else if (p &lt; 0.1) {\n    return(\"*\")\n  } else {\n    return(\"\")\n  }\n}\n\n# Cr√©ation de la matrice des annotations\nnumber_labels &lt;- matrix(\"\", nrow = n, ncol = n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    rho &lt;- cor_matrix[i, j]\n    p &lt;- pval_matrix[i, j]\n    stars &lt;- get_signif_stars(p)\n    number_labels[i, j] &lt;- paste0(sprintf(\"%.2f\", rho), stars)\n  }\n}\nrownames(number_labels) &lt;- rownames(cor_matrix)\ncolnames(number_labels) &lt;- colnames(cor_matrix)\n\n\n# heatmat sans clustering (car par defaut la fonction essaie de faire une CAH)\nplt &lt;- pheatmap(\n  cor_matrix,\n  cluster_rows = FALSE,\n  cluster_cols = FALSE,\n  color = colorRampPalette(c(\"blue\", \"white\", \"red\"))(50),\n  display_numbers = number_labels,\n  number_format = \"\",\n  fontsize_number = 8,\n  main = \"Heatmap de la corr√©lation de Person\",\n  angle_col = 90 \n)\nplt\n\n\n\n\n\nHeatmap des varibales quantitative de la base de donn√©es\n\n\n\n\nInterpretation :\nComme on peut le constater, plusieurs variables pr√©sentent des corr√©lations significatives entre elles. Cela est mis en √©vidence par les √©toiles indiquant le niveau de signification statistique : *** pour une signification au seuil de 1 %, ** pour 5 %, et * pour 10 %. La coloration des cellules ‚Äî plus elles sont proches du rouge ou du bleu fonc√©, plus la corr√©lation est forte (positive ou n√©gative) ‚Äî renforce cette lecture. Par exemple le taux de mortalit√© infantile est n√©gativement corr√©l√© √† l‚Äôexp√©rance de vie (-0,89 ***). Un taux de mortalit√© infantile √©lev√© signifie que de nombreux d√©c√®s surviennent tr√®s t√¥t dans la vie. Comme l‚Äôesp√©rance de vie est une moyenne pond√©r√©e des √¢ges au d√©c√®s, la pr√©sence de nombreux d√©c√®s pr√©coces fait chuter la moyenne, d‚Äôo√π la corr√©lation n√©gative. Par contre le taux de mortalit√© infantile est positivement corr√©l√© √† le taux de f√©condit√© (0,85 ***). Cela peut s‚Äôexpliquer par le fait que, dans les pays o√π le taux de f√©condit√© est √©lev√©, le nombre total de naissances est plus important. D√®s lors, si les conditions sanitaires restent pr√©caires, cela augmente m√©caniquement le nombre d‚Äôenfants susceptibles de d√©c√©der en bas √¢ge. Ce ph√©nom√®ne s‚Äôapparente √† un processus binomial, o√π chaque naissance repr√©sente une ‚Äú√©preuve‚Äù avec une certaine probabilit√© de d√©c√®s. Plus il y a d‚Äô√©preuves, plus la fr√©quence des d√©c√®s peut √™tre √©lev√©e, ce qui se traduit par un taux de mortalit√© infantile plus important.\nCes interd√©pendances marqu√©es entre les variables justifient le recours √† une analyse en composantes principales (ACP), qui permettra de r√©sumer l‚Äôinformation contenue dans ces variables corr√©l√©es tout en r√©duisant la dimensionnalit√© du jeu de donn√©es.\n\n\n\n\n¬†¬†¬†¬†¬†¬†Plusieurs packages sur R permettre de mettre en oeuvre l‚Äôanalyse en composantes principales. Nous utiliserons les packages FactoMineR et factoextra. Tr√®s souvent, on standardise les donn√©es avant de r√©aliser une analyse en composantes principales. Cette √©tape permet de ramener toutes les variables √† une √©chelle comparable, en neutralisant les diff√©rences d‚Äôunit√©s ou d‚Äôamplitudes. Ainsi, chaque variable contribue √©quitablement √† la construction des composantes principales, sans que celles ayant une grande variance ne dominent l‚Äôanalyse.\n\ndf_acp &lt;- df[, -1]\nrownames(df_acp) &lt;- df[,1]\nacp_model &lt;- PCA(df_acp, scale.unit = TRUE, graph = FALSE)\n\n\nLes infos du mod√®le\n\n\nnames(acp_model)\n\n[1] \"eig\"  \"var\"  \"ind\"  \"svd\"  \"call\"\n\n\n\neig : Valeurs propres associ√©es aux composantes principales. Elles indiquent la variance expliqu√©e par chaque axe.\nvar : Informations sur les variables actives (coordonn√©es, contributions, qualit√©s de repr√©sentation (cos2)).\nind : Informations sur les individus (lignes) : coordonn√©es dans l‚Äôespace factoriel, contributions, cos2.\nsvd : R√©sultats de la d√©composition en valeurs singuli√®res (utile si vous voulez aller dans le d√©tail math√©matique).\ncall : L‚Äôappel de la fonction (PCA(...)), utile pour garder la trace de tes param√®tres d‚Äôappel.\n\nMais nous allons nous concentr√©s que sur eig, var et ind.\n\n\n\nValeurs proppres : Choix des dimensions d‚Äôanalyse\n\n\n\n\n\nCode\nfviz_eig(acp_model, geom = 'line') +\n  labs(title = \"Pourcentages des variances expliqu√©es par les composantes principales\",\n       y = \"Pourcentage d'inertie\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des variances expliqu√©es par les composantes principales\n\n\n\n\n¬†¬†¬†¬†¬†¬†Le screeplot (ou graphique des √©boulis) met en √©vidence une chute marqu√©e des valeurs propres entre la premi√®re et la deuxi√®me dimension. Un l√©ger coude appara√Æt ensuite entre la deuxi√®me et la troisi√®me composante. Au-del√†, la d√©croissance devient plus faible et progressive, avec un second coude observable autour de la sixi√®me dimension.\nPour d√©terminer le nombre optimal d‚Äôaxes √† retenir, nous nous appuierons sur deux crit√®res compl√©mentaires :\n\nLe crit√®re de Kaiser, qui recommande de ne retenir que les composantes dont la valeur propre est sup√©rieure √† 1.\nLe crit√®re du taux d‚Äôinertie, qui sugg√®re de conserver le nombre de dimensions n√©cessaires pour expliquer un seuil acceptable de la variance totale, souvent fix√© √† 70 % ou 80 % selon le contexte.\n\n\n\nCode\nfviz_eig(acp_model, choice = \"eigenvalue\" , geom = 'bar')+\n  geom_hline(yintercept = 1, linetype = 2, color = \"red\") +\n  labs(title = \"Valeurs propres des composantes principales\",\n       y = \"Valeur propre\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des valeurs propres issues de l‚ÄôACP\n\n\n\n\nEn se basant donc sur ces deux crit√®re nous pouvons s√©lectionner les trois premi√®res dimensions.\n\n\n\nAnalyses des variables\n\n\n\n\n\nCode\nfviz_pca_var(acp_model, col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corr√©lation des variables\",\n       y = \"Dimension 2\", x = \"Dimension 1\") + \n  theme_light()\n\nfviz_pca_var(acp_model, axes = c(1, 3), col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corr√©lation des variables\",\n       y = \"Dimension 3\", x = \"Dimension 1\") + \n  theme_light()\n\n\nfviz_pca_var(acp_model, axes = c(2, 3), col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corr√©lation des variables\",\n       y = \"Dimension 3\", x = \"Dimension 2\") + \n  theme_light()\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 1 et 2\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 1 et 3\n\n\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 2 et 3\n\n\n\n\nCartes de la representation des variables sur les dimensions\n\n\n\n\n\nGraphique a)\n\non observe que les variables health et inflation sont peu bien repr√©sent√©es sur le plan factoriel (valeurs de cos¬≤ faibles). √Ä l‚Äôinverse, la variable exports est celle qui est la mieux projet√©e sur ce plan.\n\nLes variables imports et exports sont davantage align√©es avec la dimension 2, ce qui sugg√®re que cet axe refl√®te essentiellement les activit√©s √©conomiques ext√©rieures (importations et exportations). Nous pourrions ainsi interpr√©ter l‚ÄôAxe 2 comme celui de l‚Äôouverture commerciale.\nConcernant l‚ÄôAxe 1, il oppose le taux de f√©condit√© et le taux de mortalit√© infantile (positivement corr√©l√©s entre eux) √† l‚Äôesp√©rance de vie, la croissance du PIB par t√™te, et aux revenus moyens par habitant. On peut donc interpr√©ter cet axe comme celui du niveau de d√©veloppement socio-√©conomique.\nEn r√©sum√© :\n\nAxe 1 : Niveau de d√©veloppement (f√©condit√© + mortalit√© infantile ‚ÜîÔ∏é esp√©rance de vie, revenus, PIB)\nAxe 2 : Activit√©s d‚Äôimportation et d‚Äôexportation\n\nGraphique b)\n\nSur ce plan (Axe 1-3), les importations ne sont pas bien repr√©sent√©es, ce qui signifie qu‚Äôelles ont une faible contribution √† la projection dans cet espace factoriel.\n\nLa dimension 1, comme pr√©c√©demment, semble caract√©riser un niveau de d√©veloppement socio-√©conomique, opposant les pays √† forte mortalit√© infantile et f√©condit√© √©lev√©e √† ceux pr√©sentant une esp√©rance de vie plus longue, un revenu moyen plus √©lev√©, les exportations et un PIB par t√™te plus important.\nQuant √† l‚ÄôAxe 3, il oppose principalement l‚Äôinflation aux d√©penses de sant√©. Ces deux variables apparaissent en opposition sur cet axe, sugg√©rant que dans les pays o√π l‚Äôinflation est forte, la part des d√©penses de sant√© dans le PIB tend √† √™tre plus faible, et inversement.\nEn r√©sum√© :\n\nAxe 1 : Niveau de d√©veloppement socio-√©conomique\nAxe 3 : Opposition entre taux d‚Äôinflation et d√©penses de sant√©\n\nGraphique b)\n\n¬†¬†¬†¬†¬†¬†Toutes les variables en bleues sont mal repr√©sent√©es. On voit encore une oppostion entre le taux d‚Äôinflation et les d√©penses de sant√© sur l‚Äôaxe 3. Quant √† l‚Äôaxe 2, il d√©crit les activit√©s d‚Äôimportation et d‚Äôexportation.\n\n\n\n\n\n\nInterpretation des variables\n\n\n\nOn pouvait choisir d‚Äôafficher le cercle de corr√©lation des variables avec leur contribution respective en lieu et place de leur qualit√© de repr√©sentation. Le plus souvent les variables qui sont bien repr√©sent√©es sont celles qui contribuent le plus √† la formation des axes factorielles. On peut √©galement combiner ces deux crit√®res.\nOn interpr√™te que les variables qui ont une bonne contribution (crit√®re : souvent sup√©rieure √† la contribution moyenne sur les axes choisis) ou une bonne qualit√© de repr√©sentations (cos2 &gt; 0,6, mais souvent subjectif).\n\n\n\n\n\nAnalyses des individus\n\n\n\n\n\nCode\nfviz_pca_ind(acp_model, \n             col.ind = \"cos2\",                # coloration des individus selon qualit√© derepr√©sentation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             geom = c(\"point\", \"text_repel\")) +   # points + labels repel\n  labs(title = \"Projection des individus (Dim 1 et 2)\",\n       x = \"Dimension 1\", y = \"Dimension 2\") +\n  theme_light()\n\nfviz_pca_ind(acp_model, axes = c(1, 3),\n             col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             geom = c(\"point\", \"text_repel\")) +\n  labs(title = \"Projection des individus (Dim 1 et 3)\",\n       x = \"Dimension 1\", y = \"Dimension 3\") +\n  theme_light()\n\nfviz_pca_ind(acp_model, axes = c(2, 3),\n             col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             geom = c(\"point\", \"text_repel\")) +\n  labs(title = \"Projection des individus (Dim 2 et 3)\",\n       x = \"Dimension 2\", y = \"Dimension 3\") +\n  theme_light()\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 1 et 2\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 1 et 3\n\n\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 2 et 3\n\n\n\n\nCartes de la representation des variables sur les dimension\n\n\n\n\nLe principe d‚Äôanalyse ne change pas. Les individus en bleu ciel sont mal repr√©sent√©s. On voit des points atypiques qui se d√©marquent. Ceux-ci contribuent fortement √† la formation des axes aux quels ils sont proches.\n\n\nCode\nfviz_pca_ind(acp_model, \n             col.ind = \"contrib\",                # coloration des individus selon qualit√© derepr√©sentation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +   # points + labels repel\n  labs(title = \"Projection des individus (Dim 1 et 2)\",\n       x = \"Dimension 1\", y = \"Dimension 2\") +\n  theme_light()\n\n\n\n\n\nAffichages des individus atypiques dimensions 1 et 2\n\n\n\n\nIls s‚Äôagit de Singapore, du luxembourg et √©ventuellemet de Malta. Qu‚Äôon les retire ou pas cela n‚Äôaurait pas chang√© quelle que chose, car les variables qu‚Äôon a ont probablement toutes des d√©nominateurs communs en fonction de leurs d√©finitions (la taille de la population pour les variables par hahbitants etc.).\n\ncontrib_ind_dim_1_2 &lt;- as.data.frame(acp_model$ind$contrib[, 1:2]) %&gt;% \n  filter(rownames(as.data.frame(acp_model$ind$contrib[, 1:2])) %in% c(\"Singapore\", \"Luxembourg\", \"Malta\"))\nknitr::kable(contrib_ind_dim_1_2, format = \"html\")\n\n\n\n\n\nDim.1\nDim.2\n\n\n\n\nLuxembourg\n6.928982\n9.108194\n\n\nMalta\n1.960319\n8.794095\n\n\nSingapore\n4.842860\n17.290257\n\n\n\n\n\n\n\nPour les autres plans, le processus est pareil.\n\n\n\n\n¬†¬†¬†¬†¬†¬†Ici on applique directement les kmeans aux coordonn√©es factorielles issues de l‚ÄôACP. Et on avait d√©cider de travailler que sur les trois premi√®res dimensions. Ceci est une illustration de la r√©duction de dimentionalit√©s avant de passer √† l‚Äôapplication d‚Äôune m√©thode qui servira √† r√©soudre un probl√®me de classifications.\nEtant donn√© qu‚Äôon veut choisir le nombre de cluster qui non seulement minimise l‚Äôinertie intra-classe (donc maximise l‚Äôinertie inter-classe), mais √† partir du quel son incr√©mentation ne change presque plus ou de peu l‚Äôinertie intra-classe. En d‚Äôautres termes ou on n‚Äôa plus de chute brutale. Chute car plus le nombre clusters augmente, plus l‚Äôinertie-intra classe diminue.\n\n\nCode\nset.seed(123) # fixe la graine g√©n√©rative\n\ndataTocluster &lt;- scale(acp_model$ind$coord[,1:3]) # selection des coordonn√©es factorielles sur les 3 premiers axes\n\nresKmeans &lt;- list() # pour stocker les r√©sultats de l'algorithme\nCPtheta &lt;- rep(0, 8) # pour stocker les inerties intra-classes (8 classes choisies de mani√®re subjective)\nfor (K in 1:8){\n  resKmeans[[K]] &lt;- kmeans(dataTocluster, K, nstart = 50)\n  CPtheta[K] &lt;- resKmeans[[K]]$tot.withinss\n}\n\n\n\n\nCode\ndf_ev_inertie &lt;- data.frame(\n  InertieIntraclasse = CPtheta,\n  NombreClusters = 1:8\n)\n\nggplot(df_ev_inertie, aes(x = NombreClusters, y = InertieIntraclasse)) +\n  geom_line() +\n  geom_point(shape = 21, fill = \"white\", color = \"black\", size = 3) +\n  labs(\n    x = \"Nombre de clusters\",\n    y = \"Inertie intra-classe\"\n  ) + theme_light()\n\n\n\n\n\nEvolution de l‚Äôinertie intra classe\n\n\n\n\nD‚Äôapr√®s le crit√®re du coude, \\(K=4\\) (√©ventuellement \\(k=5\\)) est un choix pertinent. Affichons ainsi un descriptif des r√©sultats du k-means.\n\n\nCode\nclassif_4 &lt;- resKmeans[[4]]\nstr(classif_4)\n\n\nList of 9\n $ cluster     : Named int [1:167] 2 4 3 3 4 3 4 4 4 3 ...\n  ..- attr(*, \"names\")= chr [1:167] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ centers     : num [1:4, 1:3] 2.204 -0.907 -0.236 0.66 3.856 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:3] \"Dim.1\" \"Dim.2\" \"Dim.3\"\n $ totss       : num 498\n $ withinss    : num [1:4] 10.7 54.5 91.6 74.6\n $ tot.withinss: num 231\n $ betweenss   : num 267\n $ size        : int [1:4] 4 52 39 72\n $ iter        : int 3\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\nCes chiffres signifient que :\n\nLa variance totale des donn√©es est de 498.\nLa variance intra-classe totale est de 231, ce qui mesure la compacit√© des clusters.\nLa variance inter-classe est de 267, indiquant la s√©paration entre les groupes.\nEnviron 54 % de la variance totale est expliqu√©e par la partition.\nLes tailles des clusters sont in√©gales, allant de 4 √† 72 individus.\nL‚Äôalgorithme a converg√© rapidement en 3 it√©rations.\n\nLa partition en 4 clusters pr√©senterait donc un bon √©quilibre entre homog√©n√©it√© interne et s√©paration externe.\n\n\nCode\nclassif_5 &lt;- resKmeans[[5]]\nstr(classif_5)\n\n\nList of 9\n $ cluster     : Named int [1:167] 5 3 1 1 4 3 3 3 3 1 ...\n  ..- attr(*, \"names\")= chr [1:167] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ centers     : num [1:5, 1:3] -7.97e-05 2.68 7.00e-01 2.53e-01 -1.09 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:3] \"Dim.1\" \"Dim.2\" \"Dim.3\"\n $ totss       : num 498\n $ withinss    : num [1:5] 54.3 4.79 36.17 53.68 43.34\n $ tot.withinss: num 192\n $ betweenss   : num 306\n $ size        : int [1:5] 20 3 45 51 48\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\nCes chiffres signifient que :\n\nLa variance totale des donn√©es est de 498.\nLa variance intra-classe totale est de 192, indiquant la compacit√© des clusters.\nLa variance inter-classe est de 306, mesurant la s√©paration entre les groupes.\nEnviron 61% de la variance totale est expliqu√©e par la partition, ce qui est une am√©lioration par rapport √† k=4.\nLes tailles des clusters varient de 3 √† 51 individus, indiquant une r√©partition in√©gale.\nL‚Äôalgorithme a converg√© en 4 it√©rations.\n\n¬†¬†¬†¬†¬†¬†La partition en 5 clusters am√©liore la s√©paration entre groupes et la compacit√© interne.\n\n\nCode\ntable(classif_5$cluster)\n\n\n\n 1  2  3  4  5 \n20  3 45 51 48 \n\n\nOn choisit \\(k=5\\) en combinant crit√®re du coude et variance expliqu√©e.\n\n\n\n\n\n\nChoix de k (nombre de clusters)\n\n\n\nLe choix du k est un peu subjctif mais on a plusieurs r√®gles permettant de le choisir. Il s‚Äôagit entre autre du crit√®re du taux d‚Äôinertie, crit√®re du coude etc. On aurait donc bien pu prendre k=4.\n\n\n\n\n\nRepr√©sentations graphiques\n\n\n\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = as.factor(classif_5$cluster))\n\nggplot(dataTocluster, aes (x = Dim.1, y = Dim.2, color = classe)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(dataTocluster)), size = 3) + \n  labs(\n    title = \"Classification des pays (Axe 1 - Axe 3)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 3\",\n    color = \"Classe/Cluster\"\n  )+\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs donn√©es socio-√©conomiques\n\n\n\n\n¬†¬†¬†¬†¬†¬†Certains points sont non labellis√©s pour √©viter que les noms des pays ne se chevauchent, ce qui pourrait nuire √† la lisibilit√© du graphique. Vous trouverez en annexe 4 les tables d√©taillant les diff√©rentes classes.\nSur le premier plan factoriel, on observe que Singapour, Malte et le Luxembourg appartiennent √† la m√™me classe. Cette proximit√© pourrait s‚Äôexpliquer par plusieurs facteurs communs √† ces pays (cf. annexe 3):\n\nEsp√©rance de vie √©lev√©e : Ces pays affichent une esp√©rance de vie parmi les plus hautes au monde, refl√©tant une qualit√© de vie et des syst√®mes de sant√© performants.\nFaible taux de mortalit√© infantile : Les taux de mortalit√© infantile y sont tr√®s bas, indiquant un acc√®s g√©n√©ralis√© aux soins pr√©natals et postnatals de qualit√©.\nRevenu par habitant √©lev√© : Le PIB par habitant est significativement sup√©rieur √† la moyenne mondiale, traduisant une √©conomie d√©velopp√©e et stable.\nD√©penses de sant√© √©lev√©es en pourcentage du PIB : Ces pays investissent une part importante de leur PIB dans la sant√©, ce qui se traduit par des infrastructures m√©dicales avanc√©es et un personnel soignant bien form√©.\nFaible taux de f√©condit√© : Le taux de f√©condit√© y est inf√©rieur au seuil de renouvellement des g√©n√©rations, ce qui est caract√©ristique des pays d√©velopp√©s avec un niveau d‚Äô√©ducation √©lev√© et une urbanisation importante.\n\nCes similitudes dans les indicateurs socio-√©conomiques et sanitaires justifient leur regroupement sur le plan factoriel de l‚ÄôACP.\nVous pouvez analyser le graphique sur le plan 1-3.\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = as.factor(classif_5$cluster))\n\nggplot(dataTocluster, aes(x = Dim.1, y = Dim.3, color = classe)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(dataTocluster)), size = 3) +  # cex ‚âà size = 3\n  labs(\n    title = \"Classification des pays (Axe 1 - Axe 3)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 3\",\n    color = \"Classe/Cluster\"\n  ) +\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs donn√©es socio-√©conomiques"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#contexte",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#contexte",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "",
    "text": "Dans un monde o√π les indicateurs de d√©veloppement sont multiples (revenu, sant√©, √©ducation, acc√®s aux services, environnement), il devient crucial de synth√©tiser l‚Äôinformation pour comprendre les grands profils qui distinguent les pays. Pour ce faire, nous mobilisons deux techniques statistiques puissantes et compl√©mentaires :\n\nL‚Äôanalyse en composantes principales (ACP), qui permet de r√©duire la dimension des donn√©es en identifiant les axes principaux de variation entre les pays,\nLe clustering par K-means, qui regroupe les pays selon leurs profils de d√©veloppement similaires dans l‚Äôespace d√©fini par l‚ÄôACP.\n\nCette approche nous permettra : - D‚Äôidentifier visuellement les dimensions cl√©s du d√©veloppement, - De regrouper les pays en classes homog√®nes, facilitant ainsi l‚Äôanalyse comparative.\nNous appliquerons cette d√©marche sur un ensemble de variables d√©crivant les niveaux de vie, l‚Äô√©ducation, la sant√©, l‚Äôenvironnement et l‚Äôacc√®s aux services, dans le but de dresser une cartographie synth√©tique et interpr√©table des grandes cat√©gories de pays √† travers le monde."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#pr√©sentation-de-lacp-et-du-k-means",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#pr√©sentation-de-lacp-et-du-k-means",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "",
    "text": "Le jeu de donn√©es utilis√© dans cette analyse provient de Kaggle et regroupe plusieurs indicateurs socio-√©conomiques et de sant√© pour 167 pays.\n\n\n\nL‚ÄôACP est une m√©thode statistique de r√©duction de dimensionnalit√©. Elle transforme un grand nombre de variables corr√©l√©es en un nombre plus petit de variables non corr√©l√©es appel√©es composantes principales. Ces composantes capturent l‚Äôessentiel de la variation pr√©sente dans les donn√©es originales.\nPourquoi utiliser l‚ÄôACP ?\n- Simplifier l‚Äôanalyse en r√©duisant la complexit√© des donn√©es multidimensionnelles,\n- Visualiser facilement les relations entre observations et variables,\n- Mettre en √©vidence les structures sous-jacentes dans les donn√©es.\n\n\n\n¬†¬†¬†¬†¬†¬†K-means est une m√©thode de classification non supervis√©e qui consiste √† regrouper un ensemble d‚Äôobservations en K clusters (groupes), o√π chaque observation appartient au cluster dont elle est la plus proche selon une mesure de distance (souvent euclidienne).\nPourquoi utiliser K-means ?\n\nIdentifier des groupes homog√®nes dans les donn√©es,\n\nFaciliter l‚Äôinterpr√©tation en cat√©gorisant les observations,\n\nD√©tecter des profils ou comportements similaires.\n\n\n\n\nL‚ÄôACP et le K-means sont souvent utilis√©s conjointement car ils se compl√®tent parfaitement :\n- ACP pr√©pare les donn√©es en r√©duisant leur dimension, en supprimant le bruit et les redondances, ce qui facilite la visualisation et la compr√©hension,\n- K-means exploite l‚Äôespace r√©duit par l‚ÄôACP pour effectuer un regroupement plus robuste et plus interpr√©table, √©vitant les probl√®mes li√©s √† la mal√©diction de la dimension.\nAinsi, l‚Äôassociation ACP + K-means permet d‚Äôanalyser efficacement des donn√©es complexes, en identifiant √† la fois les principales dimensions d‚Äôinfluence et les groupes d‚Äôobservations partageant des caract√©ristiques communes."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#installation-des-pacakges",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#installation-des-pacakges",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "",
    "text": "Code\nrm(list=ls())\n\n##--- package √† installer\npackages &lt;- c(\n  \"dplyr\",\"cluster\",\n  \"ggplot2\",\"factoextra\",\n  \"FactoMineR\", \"pheatmap\",\n  \"ggrepel\", \"patchwork\"\n)\n\n##-- Boucle pour installer et charger les packages\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#chargement-de-la-base-de-donn√©es-et-informations",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#chargement-de-la-base-de-donn√©es-et-informations",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "",
    "text": "Code\ndf &lt;- read.csv('data_country.csv')\n# v√©rification\nglimpse(df)\n\n\nRows: 167\nColumns: 10\n$ Country    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Antigua and‚Ä¶\n$ child_mort &lt;dbl&gt; 90.2, 16.6, 27.3, 119.0, 10.3, 14.5, 18.1, 4.8, 4.3, 39.2, ‚Ä¶\n$ exports    &lt;dbl&gt; 10.0, 28.0, 38.4, 62.3, 45.5, 18.9, 20.8, 19.8, 51.3, 54.3,‚Ä¶\n$ health     &lt;dbl&gt; 7.58, 6.55, 4.17, 2.85, 6.03, 8.10, 4.40, 8.73, 11.00, 5.88‚Ä¶\n$ imports    &lt;dbl&gt; 44.9, 48.6, 31.4, 42.9, 58.9, 16.0, 45.3, 20.9, 47.8, 20.7,‚Ä¶\n$ income     &lt;int&gt; 1610, 9930, 12900, 5900, 19100, 18700, 6700, 41400, 43200, ‚Ä¶\n$ inflation  &lt;dbl&gt; 9.440, 4.490, 16.100, 22.400, 1.440, 20.900, 7.770, 1.160, ‚Ä¶\n$ life_expec &lt;dbl&gt; 56.2, 76.3, 76.5, 60.1, 76.8, 75.8, 73.3, 82.0, 80.5, 69.1,‚Ä¶\n$ total_fer  &lt;dbl&gt; 5.82, 1.65, 2.89, 6.16, 2.13, 2.37, 1.69, 1.93, 1.44, 1.92,‚Ä¶\n$ gdpp       &lt;int&gt; 553, 4090, 4460, 3530, 12200, 10300, 3220, 51900, 46900, 58‚Ä¶\n\n\n\n\n\nDescription des donn√©es\n\n\n\nLe jeu de donn√©es contient les indicateurs suivants avec leurs √©ventuelles significations pour 167 pays :\n\nchild_mort : taux de mortalit√© infantile (pour 1000 naissances vivantes)\nexports et imports : en % du PIB\nhealth : d√©penses de sant√© en % du PIB\nincome : revenu moyen par personne\ninflation : taux d‚Äôinflation\nlife_expec : esp√©rance de vie\ntotal_fer : taux de f√©condit√©\ngdpp : PIB par habitant\n\nLes formats des variables semblent √™tre ad√©quats.\nPour aller plus loin dans le descriptis des donn√©es, on peut taper la commande suivante :\n\n\nCode\nsummary(df)\n\n\n   Country            child_mort        exports            health      \n Length:167         Min.   :  2.60   Min.   :  0.109   Min.   : 1.810  \n Class :character   1st Qu.:  8.25   1st Qu.: 23.800   1st Qu.: 4.920  \n Mode  :character   Median : 19.30   Median : 35.000   Median : 6.320  \n                    Mean   : 38.27   Mean   : 41.109   Mean   : 6.816  \n                    3rd Qu.: 62.10   3rd Qu.: 51.350   3rd Qu.: 8.600  \n                    Max.   :208.00   Max.   :200.000   Max.   :17.900  \n    imports             income         inflation         life_expec   \n Min.   :  0.0659   Min.   :   609   Min.   : -4.210   Min.   :32.10  \n 1st Qu.: 30.2000   1st Qu.:  3355   1st Qu.:  1.810   1st Qu.:65.30  \n Median : 43.3000   Median :  9960   Median :  5.390   Median :73.10  \n Mean   : 46.8902   Mean   : 17145   Mean   :  7.782   Mean   :70.56  \n 3rd Qu.: 58.7500   3rd Qu.: 22800   3rd Qu.: 10.750   3rd Qu.:76.80  \n Max.   :174.0000   Max.   :125000   Max.   :104.000   Max.   :82.80  \n   total_fer          gdpp       \n Min.   :1.150   Min.   :   231  \n 1st Qu.:1.795   1st Qu.:  1330  \n Median :2.410   Median :  4660  \n Mean   :2.948   Mean   : 12964  \n 3rd Qu.:3.880   3rd Qu.: 14050  \n Max.   :7.490   Max.   :105000  \n\n\nOn voit qu‚Äôon des donn√©es qui sont propres ,ce qui est rarement le cas dans des situations r√©√©lles.\n\n\n\nVisualisation des informations sur la distribution des variables\n\n\n\n\n\nCode\nplot_box_plot &lt;- function(variable_name_str, title = \"\") {\n  # V√©rification que la variable existe dans df\n  if(!variable_name_str %in% colnames(df)) {\n    stop(\"La variable sp√©cifi√©e n'existe pas dans la table\")\n  }\n\n  \n  var_titles &lt;- list(\n    \"child_mort\" = \"Taux de mortalit√© infantile\",\n    \"exports\"    = \"Exportations (% du PIB)\",\n    \"health\"     = \"D√©penses de sant√© (% du PIB)\",\n    \"imports\"    = \"Importations (% du PIB)\",\n    \"income\"     = \"Revenu par habitant (en USD)\",\n    \"inflation\"  = \"Taux d'inflation (%)\",\n    \"life_expec\" = \"Esp√©rance de vie (en ann√©es)\",\n    \"total_fer\"  = \"Taux de f√©condit√© (enfants par femme)\",\n    \"gdpp\"       = \"PIB par habitant (en USD)\"\n  )\n  \n  title &lt;- var_titles[[variable_name_str]]\n  # titre par d√©faut\n  if (title == \"\") {\n    title &lt;- paste(\"Distribution de la variable\", variable_name_str)\n  }\n\n  # Cr√©ation du graphique\n  plt &lt;- ggplot(df, aes(y = .data[[variable_name_str]])) +\n    geom_boxplot(fill = \"skyblue\", color = \"darkblue\") +\n    theme_light() +\n    labs(title = title, y = variable_name_str)+\n  theme(plot.title = element_text(size = 9)) \n\n  return(plt)\n}\n\nplots &lt;- lapply(colnames(df[,-1]), plot_box_plot)\nwrap_plots(plots, ncol = 3)\n\n\n\n\n\nDistribution des variables du jeu de donn√©es"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#corr√©logramme-des-variables",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#corr√©logramme-des-variables",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "",
    "text": "On calcule les corr√©lation de Pearson quand on suspecte des relations lin√©aires entre les variables, quand celles -ci sont sous forme d‚Äô√©chelle ou de ratio. Ici en plus de ploter le heatmap des variables, nous afficherons celles qui sont significativement corr√©l√©es (Test de corr√©lation de Pearson cf.¬†Annexe 1)\n\n\nCode\n# calcul de la matrice de corr√©lations de pearson\ncor_matrix &lt;- cor(df[, -1], method = \"pearson\")\n\n# fonction pour extraire les p-values\nget_pval &lt;- function(x, y) {\n  res &lt;- suppressWarnings(cor.test(x, y, method = \"pearson\"))\n  return(res$p.value)\n}\n\n# matrice des p-values\nn &lt;- ncol(df[, -1])\npval_matrix &lt;- matrix(NA, nrow = n, ncol = n)\nrownames(pval_matrix) &lt;- colnames(df[, -1])\ncolnames(pval_matrix) &lt;- colnames(df[, -1])\nfor (i in 1:n) {\n  for (j in 1:n) {\n    pval_matrix[i, j] &lt;- get_pval(df[, -1][[i]], df[, -1][[j]])\n  }\n}\n\nget_signif_stars &lt;- function(p) {\n  if (p &lt; 0.01) {\n    return(\"***\")\n  } else if (p &lt; 0.05) {\n    return(\"**\")\n  } else if (p &lt; 0.1) {\n    return(\"*\")\n  } else {\n    return(\"\")\n  }\n}\n\n# Cr√©ation de la matrice des annotations\nnumber_labels &lt;- matrix(\"\", nrow = n, ncol = n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    rho &lt;- cor_matrix[i, j]\n    p &lt;- pval_matrix[i, j]\n    stars &lt;- get_signif_stars(p)\n    number_labels[i, j] &lt;- paste0(sprintf(\"%.2f\", rho), stars)\n  }\n}\nrownames(number_labels) &lt;- rownames(cor_matrix)\ncolnames(number_labels) &lt;- colnames(cor_matrix)\n\n\n# heatmat sans clustering (car par defaut la fonction essaie de faire une CAH)\nplt &lt;- pheatmap(\n  cor_matrix,\n  cluster_rows = FALSE,\n  cluster_cols = FALSE,\n  color = colorRampPalette(c(\"blue\", \"white\", \"red\"))(50),\n  display_numbers = number_labels,\n  number_format = \"\",\n  fontsize_number = 8,\n  main = \"Heatmap de la corr√©lation de Person\",\n  angle_col = 90 \n)\nplt\n\n\n\n\n\nHeatmap des varibales quantitative de la base de donn√©es\n\n\n\n\nInterpretation :\nComme on peut le constater, plusieurs variables pr√©sentent des corr√©lations significatives entre elles. Cela est mis en √©vidence par les √©toiles indiquant le niveau de signification statistique : *** pour une signification au seuil de 1 %, ** pour 5 %, et * pour 10 %. La coloration des cellules ‚Äî plus elles sont proches du rouge ou du bleu fonc√©, plus la corr√©lation est forte (positive ou n√©gative) ‚Äî renforce cette lecture. Par exemple le taux de mortalit√© infantile est n√©gativement corr√©l√© √† l‚Äôexp√©rance de vie (-0,89 ***). Un taux de mortalit√© infantile √©lev√© signifie que de nombreux d√©c√®s surviennent tr√®s t√¥t dans la vie. Comme l‚Äôesp√©rance de vie est une moyenne pond√©r√©e des √¢ges au d√©c√®s, la pr√©sence de nombreux d√©c√®s pr√©coces fait chuter la moyenne, d‚Äôo√π la corr√©lation n√©gative. Par contre le taux de mortalit√© infantile est positivement corr√©l√© √† le taux de f√©condit√© (0,85 ***). Cela peut s‚Äôexpliquer par le fait que, dans les pays o√π le taux de f√©condit√© est √©lev√©, le nombre total de naissances est plus important. D√®s lors, si les conditions sanitaires restent pr√©caires, cela augmente m√©caniquement le nombre d‚Äôenfants susceptibles de d√©c√©der en bas √¢ge. Ce ph√©nom√®ne s‚Äôapparente √† un processus binomial, o√π chaque naissance repr√©sente une ‚Äú√©preuve‚Äù avec une certaine probabilit√© de d√©c√®s. Plus il y a d‚Äô√©preuves, plus la fr√©quence des d√©c√®s peut √™tre √©lev√©e, ce qui se traduit par un taux de mortalit√© infantile plus important.\nCes interd√©pendances marqu√©es entre les variables justifient le recours √† une analyse en composantes principales (ACP), qui permettra de r√©sumer l‚Äôinformation contenue dans ces variables corr√©l√©es tout en r√©duisant la dimensionnalit√© du jeu de donn√©es."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#mise-en-oeuvre-de-lacp",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#mise-en-oeuvre-de-lacp",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "",
    "text": "Plusieurs packages sur R permettre de mettre en oeuvre l‚Äôanalyse en composantes principales. Nous utiliserons les packages FactoMineR et factoextra. Tr√®s souvent, on standardise les donn√©es avant de r√©aliser une analyse en composantes principales. Cette √©tape permet de ramener toutes les variables √† une √©chelle comparable, en neutralisant les diff√©rences d‚Äôunit√©s ou d‚Äôamplitudes. Ainsi, chaque variable contribue √©quitablement √† la construction des composantes principales, sans que celles ayant une grande variance ne dominent l‚Äôanalyse.\n\ndf_acp &lt;- df[, -1]\nrownames(df_acp) &lt;- df[,1]\nacp_model &lt;- PCA(df_acp, scale.unit = TRUE, graph = FALSE)\n\n\nLes infos du mod√®le\n\n\nnames(acp_model)\n\n[1] \"eig\"  \"var\"  \"ind\"  \"svd\"  \"call\"\n\n\n\neig : Valeurs propres associ√©es aux composantes principales. Elles indiquent la variance expliqu√©e par chaque axe.\nvar : Informations sur les variables actives (coordonn√©es, contributions, qualit√©s de repr√©sentation (cos2)).\nind : Informations sur les individus (lignes) : coordonn√©es dans l‚Äôespace factoriel, contributions, cos2.\nsvd : R√©sultats de la d√©composition en valeurs singuli√®res (utile si vous voulez aller dans le d√©tail math√©matique).\ncall : L‚Äôappel de la fonction (PCA(...)), utile pour garder la trace de tes param√®tres d‚Äôappel.\n\nMais nous allons nous concentr√©s que sur eig, var et ind.\n\n\n\nValeurs proppres : Choix des dimensions d‚Äôanalyse\n\n\n\n\n\nCode\nfviz_eig(acp_model, geom = 'line') +\n  labs(title = \"Pourcentages des variances expliqu√©es par les composantes principales\",\n       y = \"Pourcentage d'inertie\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des variances expliqu√©es par les composantes principales\n\n\n\n\n¬†¬†¬†¬†¬†¬†Le screeplot (ou graphique des √©boulis) met en √©vidence une chute marqu√©e des valeurs propres entre la premi√®re et la deuxi√®me dimension. Un l√©ger coude appara√Æt ensuite entre la deuxi√®me et la troisi√®me composante. Au-del√†, la d√©croissance devient plus faible et progressive, avec un second coude observable autour de la sixi√®me dimension.\nPour d√©terminer le nombre optimal d‚Äôaxes √† retenir, nous nous appuierons sur deux crit√®res compl√©mentaires :\n\nLe crit√®re de Kaiser, qui recommande de ne retenir que les composantes dont la valeur propre est sup√©rieure √† 1.\nLe crit√®re du taux d‚Äôinertie, qui sugg√®re de conserver le nombre de dimensions n√©cessaires pour expliquer un seuil acceptable de la variance totale, souvent fix√© √† 70 % ou 80 % selon le contexte.\n\n\n\nCode\nfviz_eig(acp_model, choice = \"eigenvalue\" , geom = 'bar')+\n  geom_hline(yintercept = 1, linetype = 2, color = \"red\") +\n  labs(title = \"Valeurs propres des composantes principales\",\n       y = \"Valeur propre\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des valeurs propres issues de l‚ÄôACP\n\n\n\n\nEn se basant donc sur ces deux crit√®re nous pouvons s√©lectionner les trois premi√®res dimensions.\n\n\n\nAnalyses des variables\n\n\n\n\n\nCode\nfviz_pca_var(acp_model, col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corr√©lation des variables\",\n       y = \"Dimension 2\", x = \"Dimension 1\") + \n  theme_light()\n\nfviz_pca_var(acp_model, axes = c(1, 3), col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corr√©lation des variables\",\n       y = \"Dimension 3\", x = \"Dimension 1\") + \n  theme_light()\n\n\nfviz_pca_var(acp_model, axes = c(2, 3), col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corr√©lation des variables\",\n       y = \"Dimension 3\", x = \"Dimension 2\") + \n  theme_light()\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 1 et 2\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 1 et 3\n\n\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 2 et 3\n\n\n\n\nCartes de la representation des variables sur les dimensions\n\n\n\n\n\nGraphique a)\n\non observe que les variables health et inflation sont peu bien repr√©sent√©es sur le plan factoriel (valeurs de cos¬≤ faibles). √Ä l‚Äôinverse, la variable exports est celle qui est la mieux projet√©e sur ce plan.\n\nLes variables imports et exports sont davantage align√©es avec la dimension 2, ce qui sugg√®re que cet axe refl√®te essentiellement les activit√©s √©conomiques ext√©rieures (importations et exportations). Nous pourrions ainsi interpr√©ter l‚ÄôAxe 2 comme celui de l‚Äôouverture commerciale.\nConcernant l‚ÄôAxe 1, il oppose le taux de f√©condit√© et le taux de mortalit√© infantile (positivement corr√©l√©s entre eux) √† l‚Äôesp√©rance de vie, la croissance du PIB par t√™te, et aux revenus moyens par habitant. On peut donc interpr√©ter cet axe comme celui du niveau de d√©veloppement socio-√©conomique.\nEn r√©sum√© :\n\nAxe 1 : Niveau de d√©veloppement (f√©condit√© + mortalit√© infantile ‚ÜîÔ∏é esp√©rance de vie, revenus, PIB)\nAxe 2 : Activit√©s d‚Äôimportation et d‚Äôexportation\n\nGraphique b)\n\nSur ce plan (Axe 1-3), les importations ne sont pas bien repr√©sent√©es, ce qui signifie qu‚Äôelles ont une faible contribution √† la projection dans cet espace factoriel.\n\nLa dimension 1, comme pr√©c√©demment, semble caract√©riser un niveau de d√©veloppement socio-√©conomique, opposant les pays √† forte mortalit√© infantile et f√©condit√© √©lev√©e √† ceux pr√©sentant une esp√©rance de vie plus longue, un revenu moyen plus √©lev√©, les exportations et un PIB par t√™te plus important.\nQuant √† l‚ÄôAxe 3, il oppose principalement l‚Äôinflation aux d√©penses de sant√©. Ces deux variables apparaissent en opposition sur cet axe, sugg√©rant que dans les pays o√π l‚Äôinflation est forte, la part des d√©penses de sant√© dans le PIB tend √† √™tre plus faible, et inversement.\nEn r√©sum√© :\n\nAxe 1 : Niveau de d√©veloppement socio-√©conomique\nAxe 3 : Opposition entre taux d‚Äôinflation et d√©penses de sant√©\n\nGraphique b)\n\n¬†¬†¬†¬†¬†¬†Toutes les variables en bleues sont mal repr√©sent√©es. On voit encore une oppostion entre le taux d‚Äôinflation et les d√©penses de sant√© sur l‚Äôaxe 3. Quant √† l‚Äôaxe 2, il d√©crit les activit√©s d‚Äôimportation et d‚Äôexportation.\n\n\n\n\n\n\nInterpretation des variables\n\n\n\nOn pouvait choisir d‚Äôafficher le cercle de corr√©lation des variables avec leur contribution respective en lieu et place de leur qualit√© de repr√©sentation. Le plus souvent les variables qui sont bien repr√©sent√©es sont celles qui contribuent le plus √† la formation des axes factorielles. On peut √©galement combiner ces deux crit√®res.\nOn interpr√™te que les variables qui ont une bonne contribution (crit√®re : souvent sup√©rieure √† la contribution moyenne sur les axes choisis) ou une bonne qualit√© de repr√©sentations (cos2 &gt; 0,6, mais souvent subjectif).\n\n\n\n\n\nAnalyses des individus\n\n\n\n\n\nCode\nfviz_pca_ind(acp_model, \n             col.ind = \"cos2\",                # coloration des individus selon qualit√© derepr√©sentation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             geom = c(\"point\", \"text_repel\")) +   # points + labels repel\n  labs(title = \"Projection des individus (Dim 1 et 2)\",\n       x = \"Dimension 1\", y = \"Dimension 2\") +\n  theme_light()\n\nfviz_pca_ind(acp_model, axes = c(1, 3),\n             col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             geom = c(\"point\", \"text_repel\")) +\n  labs(title = \"Projection des individus (Dim 1 et 3)\",\n       x = \"Dimension 1\", y = \"Dimension 3\") +\n  theme_light()\n\nfviz_pca_ind(acp_model, axes = c(2, 3),\n             col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             geom = c(\"point\", \"text_repel\")) +\n  labs(title = \"Projection des individus (Dim 2 et 3)\",\n       x = \"Dimension 2\", y = \"Dimension 3\") +\n  theme_light()\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 1 et 2\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 1 et 3\n\n\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 2 et 3\n\n\n\n\nCartes de la representation des variables sur les dimension\n\n\n\n\nLe principe d‚Äôanalyse ne change pas. Les individus en bleu ciel sont mal repr√©sent√©s. On voit des points atypiques qui se d√©marquent. Ceux-ci contribuent fortement √† la formation des axes aux quels ils sont proches.\n\n\nCode\nfviz_pca_ind(acp_model, \n             col.ind = \"contrib\",                # coloration des individus selon qualit√© derepr√©sentation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +   # points + labels repel\n  labs(title = \"Projection des individus (Dim 1 et 2)\",\n       x = \"Dimension 1\", y = \"Dimension 2\") +\n  theme_light()\n\n\n\n\n\nAffichages des individus atypiques dimensions 1 et 2\n\n\n\n\nIls s‚Äôagit de Singapore, du luxembourg et √©ventuellemet de Malta. Qu‚Äôon les retire ou pas cela n‚Äôaurait pas chang√© quelle que chose, car les variables qu‚Äôon a ont probablement toutes des d√©nominateurs communs en fonction de leurs d√©finitions (la taille de la population pour les variables par hahbitants etc.).\n\ncontrib_ind_dim_1_2 &lt;- as.data.frame(acp_model$ind$contrib[, 1:2]) %&gt;% \n  filter(rownames(as.data.frame(acp_model$ind$contrib[, 1:2])) %in% c(\"Singapore\", \"Luxembourg\", \"Malta\"))\nknitr::kable(contrib_ind_dim_1_2, format = \"html\")\n\n\n\n\n\nDim.1\nDim.2\n\n\n\n\nLuxembourg\n6.928982\n9.108194\n\n\nMalta\n1.960319\n8.794095\n\n\nSingapore\n4.842860\n17.290257\n\n\n\n\n\n\n\nPour les autres plans, le processus est pareil."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#clustering-des-pays-√†-laide-des-k-means",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#clustering-des-pays-√†-laide-des-k-means",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "",
    "text": "Ici on applique directement les kmeans aux coordonn√©es factorielles issues de l‚ÄôACP. Et on avait d√©cider de travailler que sur les trois premi√®res dimensions. Ceci est une illustration de la r√©duction de dimentionalit√©s avant de passer √† l‚Äôapplication d‚Äôune m√©thode qui servira √† r√©soudre un probl√®me de classifications.\nEtant donn√© qu‚Äôon veut choisir le nombre de cluster qui non seulement minimise l‚Äôinertie intra-classe (donc maximise l‚Äôinertie inter-classe), mais √† partir du quel son incr√©mentation ne change presque plus ou de peu l‚Äôinertie intra-classe. En d‚Äôautres termes ou on n‚Äôa plus de chute brutale. Chute car plus le nombre clusters augmente, plus l‚Äôinertie-intra classe diminue.\n\n\nCode\nset.seed(123) # fixe la graine g√©n√©rative\n\ndataTocluster &lt;- scale(acp_model$ind$coord[,1:3]) # selection des coordonn√©es factorielles sur les 3 premiers axes\n\nresKmeans &lt;- list() # pour stocker les r√©sultats de l'algorithme\nCPtheta &lt;- rep(0, 8) # pour stocker les inerties intra-classes (8 classes choisies de mani√®re subjective)\nfor (K in 1:8){\n  resKmeans[[K]] &lt;- kmeans(dataTocluster, K, nstart = 50)\n  CPtheta[K] &lt;- resKmeans[[K]]$tot.withinss\n}\n\n\n\n\nCode\ndf_ev_inertie &lt;- data.frame(\n  InertieIntraclasse = CPtheta,\n  NombreClusters = 1:8\n)\n\nggplot(df_ev_inertie, aes(x = NombreClusters, y = InertieIntraclasse)) +\n  geom_line() +\n  geom_point(shape = 21, fill = \"white\", color = \"black\", size = 3) +\n  labs(\n    x = \"Nombre de clusters\",\n    y = \"Inertie intra-classe\"\n  ) + theme_light()\n\n\n\n\n\nEvolution de l‚Äôinertie intra classe\n\n\n\n\nD‚Äôapr√®s le crit√®re du coude, \\(K=4\\) (√©ventuellement \\(k=5\\)) est un choix pertinent. Affichons ainsi un descriptif des r√©sultats du k-means.\n\n\nCode\nclassif_4 &lt;- resKmeans[[4]]\nstr(classif_4)\n\n\nList of 9\n $ cluster     : Named int [1:167] 2 4 3 3 4 3 4 4 4 3 ...\n  ..- attr(*, \"names\")= chr [1:167] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ centers     : num [1:4, 1:3] 2.204 -0.907 -0.236 0.66 3.856 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:3] \"Dim.1\" \"Dim.2\" \"Dim.3\"\n $ totss       : num 498\n $ withinss    : num [1:4] 10.7 54.5 91.6 74.6\n $ tot.withinss: num 231\n $ betweenss   : num 267\n $ size        : int [1:4] 4 52 39 72\n $ iter        : int 3\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\nCes chiffres signifient que :\n\nLa variance totale des donn√©es est de 498.\nLa variance intra-classe totale est de 231, ce qui mesure la compacit√© des clusters.\nLa variance inter-classe est de 267, indiquant la s√©paration entre les groupes.\nEnviron 54 % de la variance totale est expliqu√©e par la partition.\nLes tailles des clusters sont in√©gales, allant de 4 √† 72 individus.\nL‚Äôalgorithme a converg√© rapidement en 3 it√©rations.\n\nLa partition en 4 clusters pr√©senterait donc un bon √©quilibre entre homog√©n√©it√© interne et s√©paration externe.\n\n\nCode\nclassif_5 &lt;- resKmeans[[5]]\nstr(classif_5)\n\n\nList of 9\n $ cluster     : Named int [1:167] 5 3 1 1 4 3 3 3 3 1 ...\n  ..- attr(*, \"names\")= chr [1:167] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ centers     : num [1:5, 1:3] -7.97e-05 2.68 7.00e-01 2.53e-01 -1.09 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:3] \"Dim.1\" \"Dim.2\" \"Dim.3\"\n $ totss       : num 498\n $ withinss    : num [1:5] 54.3 4.79 36.17 53.68 43.34\n $ tot.withinss: num 192\n $ betweenss   : num 306\n $ size        : int [1:5] 20 3 45 51 48\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\nCes chiffres signifient que :\n\nLa variance totale des donn√©es est de 498.\nLa variance intra-classe totale est de 192, indiquant la compacit√© des clusters.\nLa variance inter-classe est de 306, mesurant la s√©paration entre les groupes.\nEnviron 61% de la variance totale est expliqu√©e par la partition, ce qui est une am√©lioration par rapport √† k=4.\nLes tailles des clusters varient de 3 √† 51 individus, indiquant une r√©partition in√©gale.\nL‚Äôalgorithme a converg√© en 4 it√©rations.\n\n¬†¬†¬†¬†¬†¬†La partition en 5 clusters am√©liore la s√©paration entre groupes et la compacit√© interne.\n\n\nCode\ntable(classif_5$cluster)\n\n\n\n 1  2  3  4  5 \n20  3 45 51 48 \n\n\nOn choisit \\(k=5\\) en combinant crit√®re du coude et variance expliqu√©e.\n\n\n\n\n\n\nChoix de k (nombre de clusters)\n\n\n\nLe choix du k est un peu subjctif mais on a plusieurs r√®gles permettant de le choisir. Il s‚Äôagit entre autre du crit√®re du taux d‚Äôinertie, crit√®re du coude etc. On aurait donc bien pu prendre k=4.\n\n\n\n\n\nRepr√©sentations graphiques\n\n\n\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = as.factor(classif_5$cluster))\n\nggplot(dataTocluster, aes (x = Dim.1, y = Dim.2, color = classe)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(dataTocluster)), size = 3) + \n  labs(\n    title = \"Classification des pays (Axe 1 - Axe 3)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 3\",\n    color = \"Classe/Cluster\"\n  )+\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs donn√©es socio-√©conomiques\n\n\n\n\n¬†¬†¬†¬†¬†¬†Certains points sont non labellis√©s pour √©viter que les noms des pays ne se chevauchent, ce qui pourrait nuire √† la lisibilit√© du graphique. Vous trouverez en annexe 4 les tables d√©taillant les diff√©rentes classes.\nSur le premier plan factoriel, on observe que Singapour, Malte et le Luxembourg appartiennent √† la m√™me classe. Cette proximit√© pourrait s‚Äôexpliquer par plusieurs facteurs communs √† ces pays (cf. annexe 3):\n\nEsp√©rance de vie √©lev√©e : Ces pays affichent une esp√©rance de vie parmi les plus hautes au monde, refl√©tant une qualit√© de vie et des syst√®mes de sant√© performants.\nFaible taux de mortalit√© infantile : Les taux de mortalit√© infantile y sont tr√®s bas, indiquant un acc√®s g√©n√©ralis√© aux soins pr√©natals et postnatals de qualit√©.\nRevenu par habitant √©lev√© : Le PIB par habitant est significativement sup√©rieur √† la moyenne mondiale, traduisant une √©conomie d√©velopp√©e et stable.\nD√©penses de sant√© √©lev√©es en pourcentage du PIB : Ces pays investissent une part importante de leur PIB dans la sant√©, ce qui se traduit par des infrastructures m√©dicales avanc√©es et un personnel soignant bien form√©.\nFaible taux de f√©condit√© : Le taux de f√©condit√© y est inf√©rieur au seuil de renouvellement des g√©n√©rations, ce qui est caract√©ristique des pays d√©velopp√©s avec un niveau d‚Äô√©ducation √©lev√© et une urbanisation importante.\n\nCes similitudes dans les indicateurs socio-√©conomiques et sanitaires justifient leur regroupement sur le plan factoriel de l‚ÄôACP.\nVous pouvez analyser le graphique sur le plan 1-3.\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = as.factor(classif_5$cluster))\n\nggplot(dataTocluster, aes(x = Dim.1, y = Dim.3, color = classe)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(dataTocluster)), size = 3) +  # cex ‚âà size = 3\n  labs(\n    title = \"Classification des pays (Axe 1 - Axe 3)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 3\",\n    color = \"Classe/Cluster\"\n  ) +\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs donn√©es socio-√©conomiques"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-1-test-de-corr√©lation-de-pearson",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-1-test-de-corr√©lation-de-pearson",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "Annexe 1 : Test de corr√©lation de Pearson",
    "text": "Annexe 1 : Test de corr√©lation de Pearson\nLe test de corr√©lation de Pearson est utilis√© pour √©valuer la pr√©sence d‚Äôune relation lin√©aire significative entre deux variables quantitatives continues. Il est adapt√© lorsque les donn√©es suivent une distribution normale ou sont suffisamment sym√©triques.\n\nObjectif du test\nD√©terminer s‚Äôil existe une corr√©lation lin√©aire significative entre deux variables \\(X\\) et \\(Y\\).\n\n\nParam√®tre test√©\nLe coefficient de corr√©lation lin√©aire \\(\\rho\\), mesurant l‚Äôintensit√© et le sens de la relation lin√©aire entre \\(X\\) et \\(Y\\) dans la population.\n\\[\n\\rho = \\text{Corr}(X, Y)\n\\]\n\n\nHypoth√®ses\n\n\n\nHypoth√®se nulle \\(H_0\\) :\n\\[\n\\rho = 0\n\\] (pas de corr√©lation lin√©aire entre \\(X\\) et \\(Y\\)\nHypoth√®se alternative \\(H_1\\) :\n\\[\n\\rho \\ne 0\n\\] (corr√©lation lin√©aire significative)\n\n\n\nStatistique de test\n\n\n√Ä partir du coefficient de corr√©lation de l‚Äô√©chantillon \\(\\rho\\), la statistique de test est :\n\\[\nt = \\frac{\\rho \\sqrt{n - 2}}{\\sqrt{1 - \\rho^2}}\n\\]\n\n\nDistribution de la statistique sous \\(H_0\\)\n\n\nSous \\(H_0\\), la statistique suit une loi de Student √† \\(n - 2\\) degr√©s de libert√© :\n\\[\nt \\sim \\mathcal{T}(n - 2)\n\\]\n\n\nR√®gle de d√©cision\n\n\n\nChoisir un seuil de signification \\(\\alpha\\) (g√©n√©ralement \\(\\alpha = 0{,}05\\)).\nCalculer la valeur critique ou la p-value.\nRejeter \\(H_0\\) si :\n\n\\[\n|t| &gt; t_{\\alpha/2, n - 2}\n\\]\nou\n\\[\n\\text{p-value} &lt; \\alpha\n\\]\n\n\nR√©gion de rejet\n\n\nPour un test bilat√©ral :\n\\[\n\\mathcal{R} = \\left\\{ t : |t| &gt; t_{\\alpha/2, n - 2} \\right\\}\n\\]\n\n\nConditions d‚Äôapplication\n\n\n\nLes deux variables doivent √™tre quantitatives continues.\nRelation suppos√©e lin√©aire (√† v√©rifier avec un nuage de points).\nNormalit√© des variables ou grande taille de l‚Äô√©chantillon."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-2-algorithme-des-k-moyennes-et-d√©termination-du-nombre-optimal-de-groupes",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-2-algorithme-des-k-moyennes-et-d√©termination-du-nombre-optimal-de-groupes",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "Annexe 2 : Algorithme des k-moyennes et d√©termination du nombre optimal de groupes",
    "text": "Annexe 2 : Algorithme des k-moyennes et d√©termination du nombre optimal de groupes\nL‚Äôalgorithme des k-moyennes (ou nu√©es dynamiques) est une m√©thode de classification non supervis√©e permettant de regrouper des individus en k groupes homog√®nes. Il repose sur la minimisation de l‚Äôinertie intra-classe, c‚Äôest-√†-dire la somme des distances quadratiques entre chaque individu et le centre de son groupe.\nL‚Äôalgorithme fonctionne de la mani√®re suivante :\n\nChoisir le nombre de groupes \\(k\\) √† former (d√©termin√© √† l‚Äôavance).\nInitialiser \\(k\\) centres al√©atoires.\nAffecter chaque individu au centre le plus proche.\nRecalculer les centres (barycentres) des groupes.\nR√©p√©ter les √©tapes 3 et 4 jusqu‚Äô√† stabilisation des centres.\n\n\n\n\nAlgorithme 1 : Algorithme de Lloyd\n\n\n\nDonn√©es : \\(x_1, \\dots, x_n\\)\nInitialisation : poser \\(m = 0\\) et tirer au hasard \\(K\\) points de \\(\\mathbb{R}^p\\) comme centres initiaux : \\[\n\\mu_1^{[m]}, \\dots, \\mu_K^{[m]}\n\\]\n\nTant que la partition n‚Äôest pas stable :\n\nIncr√©mentation du compteur : \\[\nm \\leftarrow m + 1\n\\]\nMise √† jour de la partition √† centres fix√©s :\n\nAffecter chaque individu √† la classe dont le centre est le plus proche : \\[\nP_k^{[m]} = \\left\\{ i : d_M(x_i, \\mu_k^{[m-1]}) \\leq d_M(x_i, \\mu_{k'}^{[m-1]}) \\ \\forall k' = 1, \\dots, K \\right\\}\n\\]\n\nMise √† jour des centres √† partition fix√©e : \\[\n\\mu_k^{[m]} = \\frac{\\sum_{i=1}^n z_{ik}^{[m]} x_i}{\\sum_{i=1}^n z_{ik}^{[m]}}\n\\]\n\navec : \\[\nz_{ik}^{[m]} =\n\\begin{cases}\n1 & \\text{si } i \\in P_k^{[m]} \\\\\n0 & \\text{sinon}\n\\end{cases}\n\\]\n\nR√©sultat final :\nLa partition : \\[\n\\mathcal{P}^{[m]} = \\{P_1^{[m]}, \\dots, P_K^{[m]}\\}\n\\]\net les centres associ√©s : \\[\n\\mu_1^{[m]}, \\dots, \\mu_K^{[m]}\n\\]\n\nSource : Cours Kmeans ‚Äì ENSAI 1A, 2024-2025\nEnseignant : Javier GONZALEZ\n\n\n\n\n\n\nNote\n\n\n\nCe processus converge g√©n√©ralement rapidement et il est tr√®s efficace, m√™me sur de grands jeux de donn√©es.\n\n\n\nUtilisation apr√®s ACP\nL‚Äôalgorithme des k-moyennes s‚Äôapplique sur des variables quantitatives. On peut donc l‚Äôutiliser directement sur les coordonn√©es factorielles obtenues via une Analyse en Composantes Principales (ACP).\nCela pr√©sente plusieurs avantages :\n\nR√©duction de la dimension : seules les premi√®res composantes (celles expliquant le plus de variance) sont conserv√©es.\n√âlimination de la redondance : les variables sont orthogonales.\nMeilleure visualisation des structures.\n\nL‚Äôapplication des k-moyennes apr√®s ACP permet donc de classer les individus dans l‚Äôespace factoriel de mani√®re plus claire et plus efficace.\n\n\nD√©termination du nombre optimal de groupes \\(k\\)\nLe choix de \\(k\\) n‚Äôa pas besoin d‚Äô√™tre exact √† l‚Äôunit√© pr√®s car l‚Äôalgorithme reste robuste. Toutefois, on peut s‚Äôaider d‚Äôun crit√®re objectif : la courbe des inerties intra-classes en fonction de \\(k\\).\nLa courbe pr√©sente souvent une cassure (m√©thode du coude), indiquant un bon compromis entre complexit√© du mod√®le et qualit√© du regroupement."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-3-similitude-de-singapore-malta-et-luxembourg",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-3-similitude-de-singapore-malta-et-luxembourg",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "Annexe 3 : Similitude de Singapore, Malta et Luxembourg",
    "text": "Annexe 3 : Similitude de Singapore, Malta et Luxembourg\n\n\n\nCaract√©risiques socio-√©conomiques de Singapore, de Malta du Luxembourg \n\n\nCountry\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\ntotal_fer\ngdpp\n\n\n\n\nLuxembourg\n2.8\n175\n7.77\n142\n91700\n3.620\n81.3\n1.63\n105000\n\n\nMalta\n6.8\n153\n8.65\n154\n28300\n3.830\n80.3\n1.36\n21100\n\n\nSingapore\n2.8\n200\n3.96\n174\n72100\n-0.046\n82.7\n1.15\n46600\n\n\n\n\n\n\n\nOn peut voir que les caract√©ristiques sont tr√®s proches mises √† part quelques unes."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-4-tables-des-clusters-des-pays-du-premier-plan-factoriel",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-4-tables-des-clusters-des-pays-du-premier-plan-factoriel",
    "title": "Vers une meilleure compr√©hension des facteurs de d√©veloppement : r√©duction de dimension et classification des pays par K-means sur des indicateurs socio-√©conomiques mondiaux",
    "section": "Annexe 4 : Tables des clusters des pays du premier plan factoriel",
    "text": "Annexe 4 : Tables des clusters des pays du premier plan factoriel\n\n\n\nLes pays du cluster 1 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nAlgeria\n1\nAlgeria\n\n\nAngola\n1\nAngola\n\n\nAzerbaijan\n1\nAzerbaijan\n\n\nBahrain\n1\nBahrain\n\n\nBrunei\n1\nBrunei\n\n\nCongo, Rep.\n1\nCongo, Rep.\n\n\nEquatorial Guinea\n1\nEquatorial Guinea\n\n\nGabon\n1\nGabon\n\n\nIndonesia\n1\nIndonesia\n\n\nKazakhstan\n1\nKazakhstan\n\n\nKuwait\n1\nKuwait\n\n\nLibya\n1\nLibya\n\n\nMongolia\n1\nMongolia\n\n\nNigeria\n1\nNigeria\n\n\nOman\n1\nOman\n\n\nQatar\n1\nQatar\n\n\nSaudi Arabia\n1\nSaudi Arabia\n\n\nSri Lanka\n1\nSri Lanka\n\n\nUnited Arab Emirates\n1\nUnited Arab Emirates\n\n\nVenezuela\n1\nVenezuela\n\n\n\n\n\n\n\n\n\n\nLes pays du cluster 2 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nLuxembourg\n2\nLuxembourg\n\n\nMalta\n2\nMalta\n\n\nSingapore\n2\nSingapore\n\n\n\n\n\n\n\n\n\n\nLes pays du cluster 3 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nAlbania\n3\nAlbania\n\n\nArgentina\n3\nArgentina\n\n\nArmenia\n3\nArmenia\n\n\nAustralia\n3\nAustralia\n\n\nAustria\n3\nAustria\n\n\nBahamas\n3\nBahamas\n\n\nBarbados\n3\nBarbados\n\n\nBosnia and Herzegovina\n3\nBosnia and Herzegovina\n\n\nBrazil\n3\nBrazil\n\n\nCanada\n3\nCanada\n\n\nChile\n3\nChile\n\n\nChina\n3\nChina\n\n\nColombia\n3\nColombia\n\n\nCosta Rica\n3\nCosta Rica\n\n\nCroatia\n3\nCroatia\n\n\nCyprus\n3\nCyprus\n\n\nDenmark\n3\nDenmark\n\n\nDominican Republic\n3\nDominican Republic\n\n\nEcuador\n3\nEcuador\n\n\nEl Salvador\n3\nEl Salvador\n\n\nFinland\n3\nFinland\n\n\nFrance\n3\nFrance\n\n\nGermany\n3\nGermany\n\n\nGreece\n3\nGreece\n\n\nIceland\n3\nIceland\n\n\nIran\n3\nIran\n\n\nIsrael\n3\nIsrael\n\n\nItaly\n3\nItaly\n\n\nJapan\n3\nJapan\n\n\nNew Zealand\n3\nNew Zealand\n\n\nNorway\n3\nNorway\n\n\nPeru\n3\nPeru\n\n\nPoland\n3\nPoland\n\n\nPortugal\n3\nPortugal\n\n\nRomania\n3\nRomania\n\n\nRussia\n3\nRussia\n\n\nSerbia\n3\nSerbia\n\n\nSouth Korea\n3\nSouth Korea\n\n\nSpain\n3\nSpain\n\n\nSweden\n3\nSweden\n\n\nSwitzerland\n3\nSwitzerland\n\n\nTurkey\n3\nTurkey\n\n\nUnited Kingdom\n3\nUnited Kingdom\n\n\nUnited States\n3\nUnited States\n\n\nUruguay\n3\nUruguay\n\n\n\n\n\n\n\n\n\n\nLes pays du cluster 4 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nAntigua and Barbuda\n4\nAntigua and Barbuda\n\n\nBelarus\n4\nBelarus\n\n\nBelgium\n4\nBelgium\n\n\nBelize\n4\nBelize\n\n\nBhutan\n4\nBhutan\n\n\nBotswana\n4\nBotswana\n\n\nBulgaria\n4\nBulgaria\n\n\nCambodia\n4\nCambodia\n\n\nCape Verde\n4\nCape Verde\n\n\nCzech Republic\n4\nCzech Republic\n\n\nEstonia\n4\nEstonia\n\n\nFiji\n4\nFiji\n\n\nGeorgia\n4\nGeorgia\n\n\nGrenada\n4\nGrenada\n\n\nGuyana\n4\nGuyana\n\n\nHungary\n4\nHungary\n\n\nIreland\n4\nIreland\n\n\nJamaica\n4\nJamaica\n\n\nJordan\n4\nJordan\n\n\nKiribati\n4\nKiribati\n\n\nKyrgyz Republic\n4\nKyrgyz Republic\n\n\nLatvia\n4\nLatvia\n\n\nLebanon\n4\nLebanon\n\n\nLesotho\n4\nLesotho\n\n\nLiberia\n4\nLiberia\n\n\nLithuania\n4\nLithuania\n\n\nMacedonia, FYR\n4\nMacedonia, FYR\n\n\nMalaysia\n4\nMalaysia\n\n\nMaldives\n4\nMaldives\n\n\nMauritius\n4\nMauritius\n\n\nMicronesia, Fed. Sts.\n4\nMicronesia, Fed. Sts.\n\n\nMoldova\n4\nMoldova\n\n\nMontenegro\n4\nMontenegro\n\n\nMorocco\n4\nMorocco\n\n\nNamibia\n4\nNamibia\n\n\nNetherlands\n4\nNetherlands\n\n\nPanama\n4\nPanama\n\n\nParaguay\n4\nParaguay\n\n\nSamoa\n4\nSamoa\n\n\nSeychelles\n4\nSeychelles\n\n\nSlovak Republic\n4\nSlovak Republic\n\n\nSlovenia\n4\nSlovenia\n\n\nSolomon Islands\n4\nSolomon Islands\n\n\nSt. Vincent and the Grenadines\n4\nSt. Vincent and the Grenadines\n\n\nSuriname\n4\nSuriname\n\n\nThailand\n4\nThailand\n\n\nTunisia\n4\nTunisia\n\n\nTurkmenistan\n4\nTurkmenistan\n\n\nUkraine\n4\nUkraine\n\n\nVanuatu\n4\nVanuatu\n\n\nVietnam\n4\nVietnam\n\n\n\n\n\n\n\n\n\n\nLes pays du cluster 5 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nAfghanistan\n5\nAfghanistan\n\n\nBangladesh\n5\nBangladesh\n\n\nBenin\n5\nBenin\n\n\nBolivia\n5\nBolivia\n\n\nBurkina Faso\n5\nBurkina Faso\n\n\nBurundi\n5\nBurundi\n\n\nCameroon\n5\nCameroon\n\n\nCentral African Republic\n5\nCentral African Republic\n\n\nChad\n5\nChad\n\n\nComoros\n5\nComoros\n\n\nCongo, Dem. Rep.\n5\nCongo, Dem. Rep.\n\n\nCote d'Ivoire\n5\nCote d'Ivoire\n\n\nEgypt\n5\nEgypt\n\n\nEritrea\n5\nEritrea\n\n\nGambia\n5\nGambia\n\n\nGhana\n5\nGhana\n\n\nGuatemala\n5\nGuatemala\n\n\nGuinea\n5\nGuinea\n\n\nGuinea-Bissau\n5\nGuinea-Bissau\n\n\nHaiti\n5\nHaiti\n\n\nIndia\n5\nIndia\n\n\nIraq\n5\nIraq\n\n\nKenya\n5\nKenya\n\n\nLao\n5\nLao\n\n\nMadagascar\n5\nMadagascar\n\n\nMalawi\n5\nMalawi\n\n\nMali\n5\nMali\n\n\nMauritania\n5\nMauritania\n\n\nMozambique\n5\nMozambique\n\n\nMyanmar\n5\nMyanmar\n\n\nNepal\n5\nNepal\n\n\nNiger\n5\nNiger\n\n\nPakistan\n5\nPakistan\n\n\nPhilippines\n5\nPhilippines\n\n\nRwanda\n5\nRwanda\n\n\nSenegal\n5\nSenegal\n\n\nSierra Leone\n5\nSierra Leone\n\n\nSouth Africa\n5\nSouth Africa\n\n\nSudan\n5\nSudan\n\n\nTajikistan\n5\nTajikistan\n\n\nTanzania\n5\nTanzania\n\n\nTimor-Leste\n5\nTimor-Leste\n\n\nTogo\n5\nTogo\n\n\nTonga\n5\nTonga\n\n\nUganda\n5\nUganda\n\n\nUzbekistan\n5\nUzbekistan\n\n\nYemen\n5\nYemen\n\n\nZambia\n5\nZambia"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html",
    "href": "ANALYSES_FACTORIELLES/TP03.html",
    "title": "Reprise du TP02 M√©thodes d‚ÄôAnalyse factorielle",
    "section": "",
    "text": "packages_ &lt;- c(\"ggplot2\", \"dplyr\",\"readxl\",\"cowplot\")\n\nfor (pkg in packages_) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#quelques-d√©finitions",
    "href": "ANALYSES_FACTORIELLES/TP03.html#quelques-d√©finitions",
    "title": "Reprise du TP02 M√©thodes d‚ÄôAnalyse factorielle",
    "section": "Quelques d√©finitions",
    "text": "Quelques d√©finitions\n¬†¬†¬†¬†¬†¬†Le calcul de l‚Äôempreinte √©cologique et de la biocapacit√© nous aide √† r√©pondre √† la question de recherche fondamentale : Quelle est la demande des √™tres humains envers les surfaces biologiquement productives (empreinte √©cologique) par rapport √† la quantit√© que la plan√®te (ou la surface productive d‚Äôune r√©gion) peut r√©g√©n√©rer sur ces surfaces (biocapacit√©) ?\n\nHectare global (gha) : C‚Äôest l‚Äôunit√© choisie pour exprimer toutes les quantit√©s d‚Äôint√©r√™t concernant la consommation/√©mission de carbone. Une unit√© de surface correspondant √† la productivit√© moyenne d‚Äôun hectare de terres mondiales. Un hectare de terres agricoles vaudra plus d‚Äôhectares globaux qu‚Äôun hectare de d√©sert.\nEmpreinte √©cologique (en gha par personne) : Le nombre de gha requis pour produire les besoins et absorber les d√©chets d‚Äôun pays.\nBiocapacit√© (en gha) : La capacit√© d‚Äôun pays √† produire ce dont il a besoin et √† absorber ses d√©chets (r√©serve √©cologique).\nJour de d√©passement : Jour de l‚Äôann√©e o√π la demande d‚Äôun pays d√©passe sa biocapacit√© annuelle."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#chargement-des-donn√©es",
    "href": "ANALYSES_FACTORIELLES/TP03.html#chargement-des-donn√©es",
    "title": "Reprise du TP02 M√©thodes d‚ÄôAnalyse factorielle",
    "section": "Chargement des donn√©es",
    "text": "Chargement des donn√©es\n\n##-- Installer et Charger les packages requis\n###--- vecteurs des packages\npackages &lt;- c(\"factoextra\", \"corrr\", \"FactoMineR\", \"dplyr\",\"kableExtra\",\"corrplot\",\n              \"explor\")\n\n###--- Boucle pour installer et charger les packages\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}\n\n##-- charger la base de donn√©es via le lien web\nlink.to.data &lt;- \"https://marieetienne.github.io/datasets/overshootday_overview.csv\"\ndf &lt;- read.csv(link.to.data)"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#analyse-exploratoire-des-donn√©es",
    "href": "ANALYSES_FACTORIELLES/TP03.html#analyse-exploratoire-des-donn√©es",
    "title": "Reprise du TP02 M√©thodes d‚ÄôAnalyse factorielle",
    "section": "Analyse exploratoire des donn√©es",
    "text": "Analyse exploratoire des donn√©es\n\nnrow(df); ncol(df) ;dim(df)\n\n[1] 182\n\n\n[1] 13\n\n\n[1] 182  13\n\n\nLes donn√©es sont compos√©es de 182 lignes et de 13 colonnes."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#r√©sum√©-statitique-des-variables",
    "href": "ANALYSES_FACTORIELLES/TP03.html#r√©sum√©-statitique-des-variables",
    "title": "Reprise du TP02 M√©thodes d‚ÄôAnalyse factorielle",
    "section": "R√©sum√© statitique des variables",
    "text": "R√©sum√© statitique des variables\nOn utilise la commande summary(df) tout simplement, mais pour une question d‚Äôexth√©tique on utilise ce code.\n\n##-- summary pour les variable num√©riques\nsummary.df.num &lt;- sapply(df[sapply(df, is.numeric)], function(x) {\n  c(\n    min = min(x, na.rm = TRUE),\n    Q1 = quantile(x, 0.25, na.rm = TRUE),\n    Q3 = quantile(x, 0.75, na.rm = TRUE),\n    med = quantile(x, 0.5, na.rm = TRUE),\n    mean = mean(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n    count = sum(!is.na(x)),\n    sd = sd(x, na.rm = TRUE),\n    `NA's` = round(sum(is.na(x)),0)\n  )\n})\nsummary.df.num &lt;- as.data.frame(summary.df.num)\n\nEnsuite nous affichons ce resum√© dans un tableau :\n\n\n\nTableau 1 : R√©sum√© statistique des variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlife_expectancy\nhdi\nper_capita_gdp\npop\ntotal_prod\ntotal_cons\nbiocapacity\nnumber_of_countries_required\nnumber_of_earths_required\novershoot_day\n\n\n\n\nmin\n52,525000\n0,3850000\n732,836\n0,06200\n0,371747\n0,5540298\n0,1041268\n0,0180633\n0,3668548\n41,0000\n\n\nQ1.25%\n65,747000\n0,5945000\n4888,255\n2,64100\n1,156834\n1,2195240\n0,6633750\n0,8273357\n0,8075166\n143,0000\n\n\nQ3.75%\n76,400695\n0,8350000\n31670,000\n32,91550\n3,828778\n3,8418335\n2,6656718\n2,7330613\n2,5438978\n365,0000\n\n\nmed.50%\n71,900000\n0,7310000\n13548,200\n10,01950\n1,924223\n2,3197815\n1,3622344\n1,7280656\n1,5360601\n239,0000\n\n\nmean\n71,180320\n0,7177193\n21139,464\n43,47636\n2,879469\n2,9624675\n3,5569055\n2,9127705\n1,9616192\n239,7802\n\n\nmax\n84,445610\n0,9620000\n120505,000\n1480,63200\n13,394536\n13,1263342\n85,6461100\n55,1061868\n8,6916969\n365,0000\n\n\ncount\n175,000000\n171,0000000\n163,000\n182,00000\n182,000000\n181,0000000\n181,0000000\n181,0000000\n181,0000000\n182,0000\n\n\nsd\n7,615465\n0,1533110\n22330,819\n156,03751\n2,515235\n2,1957327\n10,0256869\n5,1916277\n1,4539202\n109,5507\n\n\nNA‚Äôs\n7,000000\n11,0000000\n19,000\n0,00000\n0,000000\n1,0000000\n1,0000000\n1,0000000\n1,0000000\n0,0000\n\n\n\nNote: aby Djamal Y. TOE\n\n\n¬†¬†Nous constatons que ceraines variables ont des donn√©es manquantes, nous pouvons d√©cider de soit les supprimer, soit les pr√©dire avec des m√©thodes d‚Äôimputation en fonction de leurs importances. Mais pour le moment nous allons juste les supprimer.\n\ndf &lt;- na.omit(df)\nnrow(df)\n\n[1] 162\n\n\nAinsi nous passons de 182 √† 162 lignes."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#contruction-de-lanalyse-en-composante-principale",
    "href": "ANALYSES_FACTORIELLES/TP03.html#contruction-de-lanalyse-en-composante-principale",
    "title": "Reprise du TP02 M√©thodes d‚ÄôAnalyse factorielle",
    "section": "Contruction de l‚ÄôAnalyse en composante principale",
    "text": "Contruction de l‚ÄôAnalyse en composante principale\n\nLe poids pour les pays : Les tailles respectives des populations de chaques pays car cela garantit que l‚Äôanalyse est repr√©sentative des diff√©rences globales, en tenant compte de l‚Äôimpact d√©mographique des pays.\nM√©trique : Normalisation des donn√©es car les variables ne sont pas toutes sur la m√™me √©chelle. Cela permet d‚Äô√©viter que les variables avec de grosses valeurs (grandes √©chelles) dominent l‚Äôanalyse.\nvariables sup :\n\nQuali sup : region, income_group\nQuanti sup : pop\n\n\n\nR√©alisation de l‚ÄôACP\n\nV√©rifions la corr√©lations entre les variables quantitatives\n\n\nnumeric.vars &lt;- as.data.frame(df[sapply(df, is.numeric)])\nM &lt;- round(cor(numeric.vars),2) #- Calculer la matrice de corr√©lation\n\n##-- cr√©er un objet qui contient une palette de couleur pour le gradiant dans le plot\ncol &lt;- colorRampPalette(c(\"#BB4444\", \"#EE9988\", \"#FFFFFF\", \"#77AADD\", \"#4477AA\"))\n\n##-- dessiner le graphique\ncorrplot(M, method=\"color\", col=col(200),  \n         type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", #- ajout des coefficients correlation\n         tl.col=\"black\", tl.srt=45, tl.cex = 1\n         , #- couleur, rotation et police de texte des libell√©s \n         ##-- ne pas afficher les coefficients de corr√©lations sur la diagonale (ils valent tous 1)\n         diag=FALSE \n         ) \n\n\n\n\nFigure 1 : Matrice de corr√©lations\n\n\n\n\n¬†¬†¬†¬†On voit qu‚Äôil y‚Äô a quand m√™me des variables qui sont &lt;&gt; (pour l‚Äôaffirmer avec plus d‚Äôassurance il serait judicieux de faire un test billat√©ral de corr√©lation de Pearson avec la commande cor.test(method = ‚Äúpearson‚Äù, alternative = ‚Äútwo.sided‚Äù)).\n\nCr√©ation du mod√®le de l‚ÄôACP\n\n\ndata.pca &lt;- df[,-1] #- s√©lectionner toute les variables sauf la variable pays\nrownames(data.pca) &lt;- df[,1] #- renommer les lignes avec les noms des pays (individus)\npoids &lt;- df$pop\npca.model &lt;- PCA(data.pca, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\"),\n                 graph = FALSE,\n                 row.w = data.pca$pop,\n                 quanti.sup = 6)\n\n##- explor(pca.model) pour une interface interactive\n\n\n\nRecup√©ration des valeurs propres et des variances\n\neigen.values &lt;- pca.model$eig\nknitr::kable(eigen.values[1:3,1:3], caption = capTab(\"Valeurs propres et inerties expliqu√©es par les 3 premiers axes\"))\n\n\nTableau 2 : Valeurs propres et inerties expliqu√©es par les 3 premiers axes\n\n\n\n\n\n\n\n\n\neigenvalue\npercentage of variance\ncumulative percentage of variance\n\n\n\n\ncomp 1\n6,278250\n69,758338\n69,75834\n\n\ncomp 2\n1,483684\n16,485374\n86,24371\n\n\ncomp 3\n0,437932\n4,865912\n91,10962\n\n\n\n\n\nOn remarque que les axes 1,2 et 3 repr√©sentent respectivement 69,76, 16,49 et 4,09, donc au total 91,11\nOn pourrait aussi visualiser le graphique des valeurs propres :\n\nplt.iner &lt;- fviz_eig(pca.model, title = \"Taux d'inertie avec Singapore\")\n\n\n\nQualit√© de representation des plans / sur les plans\n\nQualit√© de representation des plans\n\n¬†¬†Le premier plan a un taux d‚Äôinertie sup√©rieur √† 86 %, il capte une grande partie de l‚Äôinformation pr√©sente dans les donn√©es ce qui signifie qu‚Äôil √† une bonne qualit√© de representation alors que le second (1-3) en capte environ 74,63 % donc a une faible qualit√© de repr√©senatation compar√© au premier. En depit de ce fait, les deux plans ont quand m√™me qualit√© de repr√©sentation si mous fions au crit√®re du taux d‚Äôinertie.\n\nQualit√© de representation sur les plans\n\n(1-2)\n\n\nLES VARIABLES\n\ngraph.cos2.var &lt;- fviz_pca_var(pca.model,col.var=\"cos2\", gradient.cols=c(\"#F1C40F\",\"#2ECC71\",\"#8E44AD\"), repel=TRUE, ggtheme = theme_light())\n\ngraph.cos2.var\n\n\n\n\nFigure 3 : Qualit√©s de representation des variables\n\n\n\n\nConcernant les variables, on constate qu‚Äôelles toutes sont bien represent√©es avec des cosinus carr√©s qui ont une valeur minimale environ 0,8 √† part les variables biocapacity, life_expectancy, number_of_countries_required qui ont un cosinus carr√©s qui vaut environ 0,7.\nLES INDIVIDUS\n\nthreshold &lt;- 0.85\ndata.ind.cos2 &lt;- pca.model$ind$cos2\n\ndim1 &lt;- data.ind.cos2[,\"Dim.1\"]\ndim1 &lt;- dim1[dim1 &gt;= threshold]\ncountries.dim1 &lt;- names(dim1)\nnames(dim1) &lt;-  NULL\n\ndim2 &lt;- data.ind.cos2[,\"Dim.2\"]\ndim2 &lt;- dim2[dim2 &gt;= 0.6]\ncountries.dim2 &lt;- names(dim2)\nnames(dim2) &lt;-  NULL\n\n##-- cr√©tion des dataframes \ndim1.df &lt;- data.frame(\n  Country = countries.dim1,\n  `Cos carr√©` = dim1\n) %&gt;% arrange(desc(dim1))\n\n\ndim2.df &lt;- data.frame(\n  Country = countries.dim2,\n  `Cos carr√©` = dim2\n) %&gt;% arrange(desc(dim2))\n\n\n##-- cr√©ation des tableaux kableExtra\ndim1.tbl &lt;- kableExtra::kbl(dim1.df, caption = capTab(\"Individus ayant un cosinus carr√© sup√©rieur ou √©gal √† 0,85 sur l'axe 1\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des donn√©es :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\ndim2.tbl &lt;- kableExtra::kbl(dim2.df, caption = capTab(\"Individus ayant un cosinus carr√© sup√©rieur ou √©gal √† 0,6 sur l'axe 2\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))  %&gt;% add_footnote(label = \"Source des donn√©es :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\n\n\n\ndim1.tbl\n\n\n\nTableau 3 : Individus ayant un cosinus carr√© sup√©rieur ou √©gal √† 0,85 sur l'axe 1\n\n\nCountry\nCos.carr√©\n\n\n\n\nRwanda\n0,9810454\n\n\nNepal\n0,9772128\n\n\nHaiti\n0,9763067\n\n\nPakistan\n0,9745537\n\n\nSao Tome and Principe\n0,9617076\n\n\nIndia\n0,9558559\n\n\nKenya\n0,9448689\n\n\nTogo\n0,9425662\n\n\nMalawi\n0,9425394\n\n\nTanzania, United Republic of\n0,9419010\n\n\nEthiopia\n0,9390190\n\n\nGambia\n0,9377395\n\n\nPoland\n0,9272053\n\n\nYemen\n0,9228666\n\n\nCzech Republic\n0,9218239\n\n\nAustria\n0,9047663\n\n\nGuatemala\n0,9044971\n\n\nMyanmar\n0,8964483\n\n\nBurundi\n0,8916493\n\n\nCambodia\n0,8911437\n\n\nDenmark\n0,8896874\n\n\nUnited States of America\n0,8875048\n\n\nSlovenia\n0,8840522\n\n\nMalaysia\n0,8840507\n\n\nBenin\n0,8831711\n\n\nSudan\n0,8713134\n\n\nSenegal\n0,8705626\n\n\nTimor-Leste\n0,8640862\n\n\nBelgium\n0,8604596\n\n\nAngola\n0,8591836\n\n\nGhana\n0,8579621\n\n\nSierra Leone\n0,8542504\n\n\nSlovakia\n0,8524619\n\n\n\na Source des donn√©es : https://marieetienne.github.io/datasets/overshootday_overview.csv\n\n\n\n\n\n\n\n\n\n\n\n\ndim2.tbl \n\n\n\nTableau 4 : Individus ayant un cosinus carr√© sup√©rieur ou √©gal √† 0,6 sur l'axe 2\n\n\nCountry\nCos.carr√©\n\n\n\n\nNamibia\n0,7502317\n\n\nParaguay\n0,6807204\n\n\nBrazil\n0,6672407\n\n\nBolivia\n0,6609662\n\n\nBarbados\n0,6458843\n\n\n\na Source des donn√©es : https://marieetienne.github.io/datasets/overshootday_overview.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\nAXE 1 : On voit que les pays (individus) comme le Togo, le Yemen, les USA, le Rwanda sont tres bien represent√©s. RMRQ : Il y en a d‚Äôautres\nAXE 2 : Il n‚Äôy a que 6 pays qui sont bien repr√©sent√©s sur cet axe. Il s‚Äôagit de la Namibie, le Paraguay, le Br√©sil, la Bolivie et Barbados.\n\nREMARQUE :  Pour le plan form√© des axes 1 et 3, on peut proc√©der la m√™me que celle en amont\n\n\nCaract√©risation des axes\n\ngraph.contrib.var &lt;- fviz_pca_var(pca.model,col.var=\"contrib\", gradient.cols=c(\"#F1C40F\",\"#2ECC71\",\"#8E44AD\"), repel=TRUE, ggtheme = theme_light())\n\ngraph.contrib.var\n\n\n\n\nFigure 4 : Cercle de corr√©lation des variables et leur contribution √† la formation des axes\n\n\n\n\n\n\nComment l‚ÄôACP est-elle modifi√©e si on retire Singapour de l‚Äôanalyse ?\n\ndata.pca.sans.singapore &lt;- data.pca %&gt;% filter(rownames(data.pca) != \"Singapore\")\npoids &lt;- df$pop\npca.model.sans.singapore &lt;- PCA(data.pca.sans.singapore, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\"),\n                 graph = FALSE,\n                 row.w = data.pca.sans.singapore$pop,\n                 quanti.sup = 6)\n\n##-- explor(pca.model)\n\n\nplt.iner.sans.sing &lt;- fviz_eig(pca.model.sans.singapore, title = \"Taux d'inertie sans Singapore\") \ncomp.iner &lt;-  cowplot::plot_grid(\n  plt.iner,\n  plt.iner.sans.sing,\n  ncol = 2\n)+ theme_light()\ncomp.iner\n\n\n\n\nFigure 5 : Comparaison des % d‚Äôinertie issues de l‚ÄôACP aevc et sans Singapore\n\n\n\n\n¬†¬†On voit que rien ne se passe (pas de changement brusque) au niveau de la qualit√© des axes. Voyons de plus pr√™t ce qui se passe :\n\nplot.indiv.avec.sing &lt;- fviz_pca_ind(pca.model) + \n                        theme_light()\nplot.indiv.avec.sing\n\n\n\n\nFigure 6 : Affichage des pays sans Singapore\n\n\n\n\n¬†¬†On voit que Singapore est atypique. Cela pourrait signifier que Singapore participe fortement √† la formation de l‚Äôaxe 2 (point plus proche de l‚Äôaxe 1).\n\ndata &lt;- as.data.frame(pca.model$ind$contrib)\ndata &lt;-  data %&gt;% arrange(desc(Dim.2)) %&gt;% head(10)\nkableExtra::kbl(data, caption = capTab(\"Contribution des individus √† la formation des axes par contribution d√©croissante suivant l'axe 2\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))  %&gt;% add_footnote(label = \"Source des donn√©es :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\n\n\nTableau 5 : Contribution des individus √† la formation des axes par contribution d√©croissante suivant l'axe 2\n\n\n\nDim.1\nDim.2\nDim.3\nDim.4\nDim.5\n\n\n\n\nSingapore\n0,7259355\n15,139608\n28,5623680\n9,9266768\n5,6933884\n\n\nBrazil\n0,2363938\n11,748028\n0,2780798\n16,7474472\n0,1489935\n\n\nChina\n6,3338962\n10,346574\n0,9583694\n1,3599693\n36,9834265\n\n\nRussian Federation\n3,7284884\n10,223438\n4,1131775\n0,1010599\n3,8319553\n\n\nCanada\n3,7127169\n6,392768\n0,8335954\n3,8719645\n0,0267550\n\n\nJapan\n2,3347045\n4,109062\n0,8491595\n0,6848764\n2,2016647\n\n\nUnited States of America\n21,6581050\n3,434086\n3,0985377\n22,3391460\n5,4054986\n\n\nKorea, Republic of\n1,9711036\n2,758601\n0,8457859\n0,0962515\n0,0745840\n\n\nAustralia\n1,8672806\n2,568678\n0,0085414\n1,3778438\n0,3505079\n\n\nGuyana\n0,0605633\n2,548944\n1,0759061\n8,5175485\n0,9606747\n\n\n\na Source des donn√©es : https://marieetienne.github.io/datasets/overshootday_overview.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEt pourtant il contribue fortement √† la formation de l‚Äôaxe 2, il est m√™me celui qui contribue les plus √† la formation des axes. Le fait que Singapore contribue le plus √† la formation des axes et que rien ne change lorsqu‚Äôil est retir√© de l‚Äôanalyse s‚Äôexplique tout simplement par sa taille de population. En effet la taille de la population a √©t√© utilis√©e comme poids des individus qui sont ici les pays."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#identifications-des-pays-en-fonction-de-leur-groupe-de-revenu",
    "href": "ANALYSES_FACTORIELLES/TP03.html#identifications-des-pays-en-fonction-de-leur-groupe-de-revenu",
    "title": "Reprise du TP02 M√©thodes d‚ÄôAnalyse factorielle",
    "section": "Identifications des pays en fonction de leur groupe de revenu",
    "text": "Identifications des pays en fonction de leur groupe de revenu\n\nIl s‚Äôagit juste d‚Äôune parenth√®se qui n‚Äôa rien avoir avec l‚Äôobjectif de l‚Äô√©tude\n\n\n##-- D√©finitions des groupes de revenus\nincome_groups_definitions &lt;- c(\n  \"UM\" = \"Upper-Middle\",\n  \"LM\" = \"Lower-Middle\",\n  \"HI\" = \"High Income\",\n  \"LI\" = \"Low Income\"\n)\n\n\n##-- Ajouter une colonne avec les d√©finitions correspondantes\ndata.pca$income_group_def &lt;- as.factor(income_groups_definitions[data.pca$income_group])\n\n\ngraph_indiv &lt;- fviz_pca_ind(\n  pca.model,\n  select.ind = list(\n    contrib = 50\n  ),\n  invisible = c(\"quanti.sup\",\"ind.sup\"),\n  habillage = data.pca$income_group_def,\n  addEllipses = TRUE,\n  repel = TRUE,\n) + theme_light() \n\n\ngraph_indiv\n\n\n\n\nFigure 7 : Affichage des 40 individus qui contribuent le plus √† la formation des axes en fonction de leur groupe de revenu\n\n\n\n#hc.pca &lt;- HCPC(pca.model, nb.clust=3)\n\n¬†¬†¬†¬†¬†¬†On voit que les groupes ne sont pas bien s√©par√©s, raison pour laquelle les ellipses ont des partie qui co√Øncident. Cela pourrait signifier que les les groupes de revenus sont trop similaires pour etre clairement s√©par√©s sur les axes s√©lectionn√©s (dans le plan des composantes principales). Cela pourrait aussi fait cas d‚Äôh√©t√©rog√©n√©it√©, c‚Äôest-√†-dire que les groupes ne sont pas homog√®nes (grande variabilit√© intra-groupe).\n¬†¬†¬†¬†¬†¬†A bien regarder, nous aurions pu les regrouper en trois groupes de revenu, en combinant les Low income et les Low middle income, les Upper middle income (avec certains pays du High income) et enfin le dernier groupe les high income. Il faut noter que tout √ßa n‚Äôest que purement visuel m√™me si on a quand m√™me une grande partie de l‚Äôinformation contenue dans les donn√©es rien qu‚Äôavec ces deux plans (plus de 80%).\n\nDeux ACP diff√©rentes\n\nPourquoi r√©aliser deux ACP diff√©rentes ?\n\n¬†¬†Pour simplement calculer la 1-√®re valeur propre de chaque groupe de variables (empreinte √©cologique et de developpement) afin de les utiliser ponderer les variables afin qu‚Äôelles contribuent de mani√®re √©quitable √† la formation des axes. Pour plus de d√©tails aller √† la sous-section et sur le site de mon professeur de M√©thodes d‚ÄôAnalyses Factorielles en cliquanr sur ce lien https://marieetienne.github.io/MAF/01_afm.html#/title-slide.\nOn pr√©f√®re utiliser la premi√®re valeur propre (\\(\\lambda_{k1}\\)) car elle capturerait l‚Äôessentiel de l‚Äôinertie d‚Äôun groupe et permet une pond√©ration coh√©rente et √©quilibr√©e dans l‚ÄôAFM. La seconde valeur propre refl√®te des structures secondaires ou r√©siduelles qui ne sont pas pertinentes pour normaliser les contributions des groupes dans l‚Äôanalyse globale.\n\nvariables.empreinte &lt;- df[, c(\"total_prod\", \"total_cons\", \"biocapacity\", \"number_of_earths_required\", \"overshoot_day\", \"pop\")]\nrownames(variables.empreinte) &lt;- df$country\nvariables.developpement &lt;- df[, c(\"life_expectancy\", \"hdi\", \"per_capita_gdp\",\"pop\")]\nrownames(variables.developpement) &lt;- df$country\n\n\nACP sur les variables d‚Äôempruntes √©cologiques\n¬†¬†¬†¬†¬†¬†Il s‚Äôagit ici de faire l‚ÄôACP que sur les variables d‚Äôempruntes √©cologiques et de mettre les autres variables (de developpement) en quantitatives suppl√©mentaires.\n\ndata.pca &lt;- df[,-1] ## s√©lectionner toute les variables sauf la variable pays\nrownames(data.pca) &lt;- df[,1] ## renommer les lignes avec les noms des pays (individus)\npoids &lt;- df$pop\nacp_empreinte &lt;- PCA(data.pca, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\"),\n                 graph = FALSE,\n                 row.w = data.pca$pop,\n                 quanti.sup = c(6,1,2,3))\n\n##-- 1ere valeur propre\nacp_empreinte$eig[1,1]\n\n[1] 4,115324\n\n\nLa premi√®re valeur propre est : 4,12\n\n\nACP sur les variables d‚Äôempruntes √©cologiques\n\nacp.developpement &lt;- PCA(data.pca, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\"),\n                 graph = FALSE,\n                 row.w = data.pca$pop,\n                 quanti.sup = 6:12)\n\n##-- 1ere valeur propre\nacp.developpement$eig[1,1]\n\n[1] 2,592046\n\n\nLa premi√®re valeur propre est : 2,59\n\n\nR√©alisons l‚ÄôAFM manuellement\n\nvariables.empreinte.pond &lt;- variables.empreinte[,-ncol(variables.empreinte)]/sqrt(acp_empreinte$eig[1,1])\n\nvariables.developpement.pond &lt;- variables.developpement[,-ncol(variables.developpement)]/sqrt(\n  acp.developpement$eig[1,1]\n)\n\nvariables.empreinte.pond$group &lt;- \"Empreinte √©cologique\"\nvariables.developpement.pond$group &lt;- \"developpement\"\n\ndf.afm &lt;- cbind(variables.empreinte.pond, \n                variables.developpement.pond,\n                pop = df$pop,\n                region = df$region,\n                income_group = df$income_group)\n\nacp.afm &lt;- PCA(df.afm, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\", \"group\"),\n                 graph = FALSE,\n                 row.w = df.afm$pop,\n                 quanti.sup = 9)\n\nvariance.cum.val.prop.2acp &lt;- acp.afm$eig[, c(1,3)]\ncolnames(variance.cum.val.prop.2acp) &lt;- c(\"Valeur propres\", \"Pourcentage de variance cumul√©e\")\n\n\nkableExtra::kbl(variance.cum.val.prop.2acp, caption = capTab(\"Valeurs propres et variances cumul√©es de chaque axes issues d'une AFM manuelle\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des donn√©es :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\n\n\nTableau 6 : Valeurs propres et variances cumul√©es de chaque axes issues d'une AFM manuelle\n\n\n\nValeur propres\nPourcentage de variance cumul√©e\n\n\n\n\ncomp 1\n5,4257689\n67,82211\n\n\ncomp 2\n1,3223267\n84,35120\n\n\ncomp 3\n0,6522681\n92,50455\n\n\ncomp 4\n0,3973880\n97,47190\n\n\ncomp 5\n0,1005256\n98,72847\n\n\ncomp 6\n0,0689548\n99,59040\n\n\ncomp 7\n0,0327679\n100,00000\n\n\ncomp 8\n0,0000000\n100,00000\n\n\n\na Source des donn√©es : https://marieetienne.github.io/datasets/overshootday_overview.csv"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#afm",
    "href": "ANALYSES_FACTORIELLES/TP03.html#afm",
    "title": "Reprise du TP02 M√©thodes d‚ÄôAnalyse factorielle",
    "section": "REALISATION DE L‚ÄôAFM",
    "text": "REALISATION DE L‚ÄôAFM\n\nPourquoi r√©aliser une AFM au lieu d‚Äôune ACP tout court?\n\n¬†¬†¬†¬†¬†¬†L‚ÄôAnalyse Factorielle Multiple (AFM) permet d‚Äôaller au-del√† des limites d‚Äôune Analyse en Composantes Principales (ACP) classique, particuli√®rement lorsque les variables d‚Äôun jeu de donn√©es ne sont pas √† la m√™me √©chelle ou lorsqu‚Äôelles sont organis√©es en groupes. La normalisation dans l‚ÄôACP sert √† ramener toutes les variables √† une m√™me √©chelle, √©vitant ainsi que certaines variables dominent artificiellement l‚Äôanalyse en raison de leur variance plus √©lev√©e. Par exemple d‚Äôautres ont une contribution √©lev√©e que d‚Äôautres alors que c‚Äôest juste l‚Äôunit√© de m√©sure qui p√®se plus.\n¬†¬†Cependant, cette normalisation n‚Äôest pas suffisante lorsque les variables sont regroup√©es par th√©matique ou nature. Par exemple, supposons un jeu de donn√©es contenant \\(n\\) variables, parmi lesquelles \\(n - k\\) \\(\\text{avec k telque  } \\forall \\text{ j} \\neq \\text{k, }\\)\n\\(\\text{n - k} &gt; \\text{n - j o√π n - j est le nombre de variables dans tous les autres groupes ou dans un autre groupe j}\\) appartiennent √† un groupe \\(i\\) .Dans ce cas, le groupe \\(i\\) peut influencer de mani√®re disproportionn√©e les r√©sultats de l‚ÄôACP, simplement en raison de la taille du groupe. Cela signifie que, m√™me apr√®s normalisation, le poids collectif du groupe \\(i\\) dans la construction des composantes principales pourrait √™tre trop important par rapport aux autres groupes.\n¬†¬†L‚ÄôAFM r√©sout ce probl√®me en int√©grant un poids √©quilibr√© entre les groupes. Elle consid√®re chaque groupe comme une entit√©, ind√©pendamment du nombre de variables qu‚Äôil contient. Cela permet une contribution √©quitable des groupes aux axes factoriels. Par cons√©quent, l‚ÄôAFM est particuli√®rement adapt√©e dans des contextes o√π les variables appartiennent √† des th√©matiques distinctes (par exemple, des groupes li√©s √† des disciplines diff√©rentes : sant√©, √©conomie, environnement).\nIl est crucial de pr√©server l‚Äô√©quilibre des contributions entre ces th√©matiques pour √©viter les biais d‚Äôinterpr√©tation. Ainsi, l‚ÄôAFM fournit une perspective multidimensionnelle plus √©quilibr√©e et pertinente pour analyser des jeux de donn√©es complexes, tout en respectant la structure inh√©rente des variables\n\nR√©alisons l‚ÄôAFM √† pr√©sent\n\n\n#-- cr√©ation de la table pour l'AFM. Les vriables doivent √™tre rang√©es \n#-- suivant le groupe (variables du groupe 1 ensuite celles du groupe 2 ...)\ndata.afm &lt;- data.pca %&gt;%\n  select(\n    life_expectancy, hdi, per_capita_gdp,  ##-- Variables de developpement\n    total_prod, total_cons, biocapacity, ##------ Variables\n    number_of_earths_required, overshoot_day ##-- d'empreinte √©cologique\n)\n\nmodel.afm &lt;- MFA(\n    data.afm, \n    group = c(5, 3), ##-- Sp√©cifie le nombre de variables dans chaque groupe\n    type = rep(\"s\", 2), ##-- Indique que les variables doivent √™tre normalis√©es pour chaque groupe\n    name.group = c(\"Developpement\", \"Empreinte ecologique\"), ##-- Nommer les groupes\n    graph = F  ##-- G√©n√©rer un graphique\n)\nvariance.cum.val.prop.afm &lt;- model.afm$eig[, c(1,3)]\ncolnames(variance.cum.val.prop.afm) &lt;- c(\"Valeur propres\", \"Pourcentage de variance cumul√©e\")\n\n\nkableExtra::kbl(variance.cum.val.prop.afm, caption = capTab(\"Valeurs propres et variances cumul√©es de chaque axes issues d'une AFM avec R\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des donn√©es :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\n\n\nTableau 7 : Valeurs propres et variances cumul√©es de chaque axes issues d'une AFM avec R\n\n\n\nValeur propres\nPourcentage de variance cumul√©e\n\n\n\n\ncomp 1\n1,9203865\n67,85068\n\n\ncomp 2\n0,5123847\n85,95414\n\n\ncomp 3\n0,2050340\n93,19836\n\n\ncomp 4\n0,0790131\n95,99003\n\n\ncomp 5\n0,0659024\n98,31848\n\n\ncomp 6\n0,0327210\n99,47457\n\n\ncomp 7\n0,0148713\n100,00000\n\n\ncomp 8\n0,0000000\n100,00000\n\n\n\na Source des donn√©es : https://marieetienne.github.io/datasets/overshootday_overview.csv\n\n\n\n\n\n\n\n\n\n\n\nOn voit qu‚Äôil n‚Äôy a pas tr√®s grande diff√©rence entre les pourcentage de variances cumul√©es des deux AFM (manuellement @variance.cum.val.prop.2acp et avec R) parcontre les valeurs propres ne sont pas les m√™mes.\n\nOn peut visualiser les variables\n\n\nfviz_mfa_var(model.afm, axes = c(1,2), choice= \"quanti.var\", repel = T) + theme_light()\n\n\n\n\nFigure 8 : Visualisation des variables dans le plan (1,2) avec les r√©sultats de l‚ÄôAFM\n\n\n\n\n\nOn peut visualiser leur qualit√© de representation\n\n\ngraph.cos.var.afm &lt;- fviz_mfa_var(model.afm, axes = c(1,2), choice= \"quanti.var\", col.var=\"cos2\", gradient.cols=c(\"#F1C40F\",\"#2ECC71\",\"#8E44AD\"), repel = T, ggtheme = theme_light())\n\ngraph.cos.var.afm\n\n\n\n\nFigure 9 : Qualit√© de repr√©sentation des variables dans le plan (1,2) avec les r√©sultats de l‚ÄôAFM\n\n\n\n\n\nLeur contribution √† la formation des axes\n\n\ngraph.contrib.var.afm &lt;- fviz_mfa_var(model.afm,col.var=\"contrib\", gradient.cols=c(\"#F1C40F\",\"#2ECC71\",\"#8E44AD\"), repel=TRUE, ggtheme = theme_light())\n\ngraph.contrib.var.afm\n\n\n\n\nFigure 10 : Cercle de corr√©lation des variables et leur contribution √† la formation des axes"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#contexte",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#contexte",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Contexte",
    "text": "Contexte\n\nLe jeu de donn√©es mtcars est l‚Äôun des ensembles de donn√©es les plus connus en statistiques et science des donn√©es. Il contient des informations sur les sp√©cifications techniques et les performances de 32 mod√®les de voitures des ann√©es 1970. Ce dataset offre une opportunit√© unique d‚Äôexplorer des relations entre des variables m√©caniques, comme la consommation en carburant, la puissance ou encore le poids des v√©hicules."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#probl√©matique",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#probl√©matique",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Probl√©matique",
    "text": "Probl√©matique\n\nComment exploiter les relations entre les caract√©ristiques des voitures pour identifier des groupes ou des tendances qui pourraient aider √† la prise de d√©cision dans le secteur automobile ?"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectif-g√©n√©ral",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectif-g√©n√©ral",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Objectif g√©n√©ral",
    "text": "Objectif g√©n√©ral\n\n√âtudier les relations entre les caract√©ristiques techniques des voitures afin de d√©gager des tendances et des informations utiles pour la conception ou la s√©lection des v√©hicules."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectifs-sp√©cifiques",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectifs-sp√©cifiques",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Objectifs sp√©cifiques",
    "text": "Objectifs sp√©cifiques\n\n\nExplorer les relations entre la consommation en carburant (mpg) et les caract√©ristiques m√©caniques\n\n\n\n\nIdentifier des groupes de voitures ayant des caract√©ristiques similaires √† l‚Äôaide d‚Äôanalyses descriptives et graphiques."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#mat√©riels",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#mat√©riels",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Mat√©riels",
    "text": "Mat√©riels\n\nLogiciel utilis√© : RStudio avec les packages n√©cessaires (ggplot2, dplyr, cowplot, etc.)\nSource des donn√©es : Jeu de donn√©es int√©gr√© mtcars."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "M√©thode",
    "text": "M√©thode\n\nNettoyage des donn√©es : V√©rification des valeurs manquantes ou aberrantes.\nAnalyse descriptive : Moyennes, m√©dianes, √©cart-types pour chaque variable."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "M√©thode",
    "text": "M√©thode\nMod√©lisation multivari√©e : Variables utilis√©es\n\nmpg : Consommation de carburant en miles par gallon (variable d√©pendante).\nwt : Poids du v√©hicule (en milliers de livres).\ncyl : Nombre de cylindres du moteur.\nam: Type de transmission (0 = automatique, 1 = manuelle).\ncarb : Nombre de carburateurs.\nhp : Puissance brute du moteur (en chevaux-vapeur)."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-2",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-2",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "M√©thode",
    "text": "M√©thode\nMod√©lisation multivari√©e :\n\\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\no√π :\n\n\\(Y\\): Vecteur des valeurs observ√©es (d√©pendantes ici mpg)\n\\(X\\) : Matrice des variables explicatives (ind√©pendantes), incluant une colonne de 1 pour l‚Äôintercept.\n\\(\\beta\\) : Vecteur des coefficients estim√©s du mod√®le.\n\\(\\epsilon\\) : Vecteur des erreurs r√©siduelles."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-3",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-3",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "M√©thode",
    "text": "M√©thode\nTests de significativit√© des coefficients\nTest t de Student\n\nHypoth√®se nulle  \\(H_0\\) : le coefficient est √©gal √† z√©ro (c‚Äôest-√†-dire, la variable n‚Äôa pas d‚Äôeffet significatif).\nHypoth√®se alternative \\(H_a\\) : Le coefficient est diff√©rent de z√©ro.\n\nSi la p-valeur est inf√©rieure √† un seuil significatif \\(p &lt; 0.05\\), nous rejetons l‚Äôhypoth√®se nulle et concluons que la variable a un effet significatif sur la variable d√©pendante."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-4",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-4",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "M√©thode",
    "text": "M√©thode\nSignificativit√© globale du mod√®le : Test F\n\nHypoth√®se nulle \\(H_0\\): Tous les coefficients sont √©gaux √† z√©ro (pas de pouvoir explicatif).\nHypoth√®se alternative \\(H_a\\) : Au moins un coefficient est diff√©rent de z√©ro (le mod√®le est significatif).\n\nSi la p-valeur du test \\(F\\) est inf√©rieure √† \\(0.05\\), nous rejetons l‚Äôhypoth√®se nulle et concluons que le mod√®le est significatif."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-5",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-5",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "M√©thode",
    "text": "M√©thode\nR-carr√© : qualit√© d‚Äôajustement\n\n\\(R^2\\) varie entre 0 et 1 :\n\nUn \\(R^2\\) proche de 1 signifie que le mod√®le explique bien les variations de la variable d√©pendante.\nUn \\(R^2\\) proche de 0 indique que le mod√®le n‚Äôexplique que peu ou pas les variations de la variable d√©pendante."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-6",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#m√©thode-6",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "M√©thode",
    "text": "M√©thode\n\nVisualisations :\n\nGraphiques de dispersion (scatterplots) pour √©tudier les corr√©lations\nHistogrammes pour analyser la distribution des variables"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#pr√©sentation-de-l√©chantillon",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#pr√©sentation-de-l√©chantillon",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Pr√©sentation de l‚Äô√©chantillon",
    "text": "Pr√©sentation de l‚Äô√©chantillon\n\n\n\n\n\nTable 1 : Description des variables du jeu de donn√©es\n\n\nColonne\nNom\nDescription\n\n\n\n\n[,1]\nmpg\nMiles par gallon (US)\n\n\n[,2]\ncyl\nNombre de cylindres\n\n\n[,3]\ndisp\nCylindr√©e (en pouces cubes)\n\n\n[,4]\nhp\nPuissance brute (chevaux)\n\n\n[,5]\ndrat\nRapport du pont arri√®re\n\n\n[,6]\nwt\nPoids (en milliers de livres)\n\n\n[,7]\nqsec\nTemps pour parcourir 1/4 de mile\n\n\n[,8]\nvs\nType de moteur (0 = V, 1 = ligne droite)\n\n\n[,9]\nam\nType de transmission (0 = automatique, 1 = manuelle)\n\n\n[,10]\ngear\nNombre de vitesses avant\n\n\n[,11]\ncarb\nNombre de carburateurs\n\n\n\na R : mtcars"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#pr√©sentation-de-l√©chantillon-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#pr√©sentation-de-l√©chantillon-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Pr√©sentation de l‚Äô√©chantillon",
    "text": "Pr√©sentation de l‚Äô√©chantillon\nR√©sum√© statistiques\n\n\n\n\nTable 2 : R√©sum√© statistique des variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nmin\n10.400000\n4.000000\n71.1000\n52.00000\n2.7600000\n1.5130000\n14.500000\n0.0000000\n0.0000000\n3.0000000\n1.0000\n\n\nQ1.25%\n15.425000\n4.000000\n120.8250\n96.50000\n3.0800000\n2.5812500\n16.892500\n0.0000000\n0.0000000\n3.0000000\n2.0000\n\n\nQ3.75%\n22.800000\n8.000000\n326.0000\n180.00000\n3.9200000\n3.6100000\n18.900000\n1.0000000\n1.0000000\n4.0000000\n4.0000\n\n\nmed.50%\n19.200000\n6.000000\n196.3000\n123.00000\n3.6950000\n3.3250000\n17.710000\n0.0000000\n0.0000000\n4.0000000\n2.0000\n\n\nmean\n20.090625\n6.187500\n230.7219\n146.68750\n3.5965625\n3.2172500\n17.848750\n0.4375000\n0.4062500\n3.6875000\n2.8125\n\n\nmax\n33.900000\n8.000000\n472.0000\n335.00000\n4.9300000\n5.4240000\n22.900000\n1.0000000\n1.0000000\n5.0000000\n8.0000\n\n\ncount\n32.000000\n32.000000\n32.0000\n32.00000\n32.0000000\n32.0000000\n32.000000\n32.0000000\n32.0000000\n32.0000000\n32.0000\n\n\nsd\n6.026948\n1.785922\n123.9387\n68.56287\n0.5346787\n0.9784574\n1.786943\n0.5040161\n0.4989909\n0.7378041\n1.6152\n\n\nNA‚Äôs\n0.000000\n0.000000\n0.0000\n0.00000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.0000\n\n\n\nNote: aR : mtcars"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-principaux-mod√©lisation",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-principaux-mod√©lisation",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "R√©sultats principaux : Mod√©lisation",
    "text": "R√©sultats principaux : Mod√©lisation\nS√©lection de mod√®le en ajoutant ou en supprimant des variables pour minimiser l‚ÄôAIC\n\n\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI\n      p-value\n    \n  \n  \n    wt\n-3.9\n-5.4, -2.5\n&lt;0.001\n    am\n2.9\n0.05, 5.8\n0.047\n    qsec\n1.2\n0.63, 1.8\n&lt;0.001\n  \n  \n    \n      Abbreviation: CI = Confidence Interval"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-principaux-mod√©lisation-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-principaux-mod√©lisation-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "R√©sultats principaux : Mod√©lisation",
    "text": "R√©sultats principaux : Mod√©lisation\nVariable wt (poids du v√©hicule)\nüîª Coefficient : -3.9\nüìå Interpr√©tation :\n\nChaque augmentation d‚Äôune unit√© du poids diminue la consommation estim√©e de 3.9 mpg.\n\nüìâ Effet n√©gatif significatif :\n\np &lt; 0.001 : plus le v√©hicule est lourd, moins il est √©conome en carburant."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-principaux-mod√©lisation-2",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-principaux-mod√©lisation-2",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "R√©sultats principaux : Mod√©lisation",
    "text": "R√©sultats principaux : Mod√©lisation\nVariable am (type de transmission)\nüî∫ Coefficient : 2.9\nüìå Interpr√©tation : - Les v√©hicules √† transmission manuelle consomment en moyenne 2.9 mpg de plus que ceux √† transmission automatique, toutes choses √©gales par ailleurs.\n‚úÖ Effet significatif : - p = 0.047 (significatif au seuil de 5 %)"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-principaux-mod√©lisation-3",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-principaux-mod√©lisation-3",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "R√©sultats principaux : Mod√©lisation",
    "text": "R√©sultats principaux : Mod√©lisation\nVariable qsec (temps sur 1/4 de mile)\nüî∫ Coefficient : 1.2\nüìå Interpr√©tation : - Chaque seconde suppl√©mentaire pour faire le 1/4 mile est associ√©e √† une augmentation de 1.2 mpg.\n‚úÖ Effet positif et hautement significatif : - p &lt; 0.001"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-s√©condaires",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-s√©condaires",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "R√©sultats s√©condaires",
    "text": "R√©sultats s√©condaires\nR√©partition des voitures par cylindres\n\nLa majorit√© des voitures ont 4 ou 8 cylindres."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-s√©condaires-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#r√©sultats-s√©condaires-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "R√©sultats s√©condaires",
    "text": "R√©sultats s√©condaires\nR√©partition des voitures par transmission"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html",
    "href": "FORMATIONS/logistic_regression_diabetes.html",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "",
    "text": "Le mod√®le logistique est une technique statistique largement utilis√©e pour mod√©liser des variables d√©pendantes binaires ou des proportions. Il est fondamental en √©conom√©trie, en sciences sociales, en biostatistique et dans de nombreux autres domaines."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#importation-des-biblioth√®ques-necessaires",
    "href": "FORMATIONS/logistic_regression_diabetes.html#importation-des-biblioth√®ques-necessaires",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Importation des biblioth√®ques necessaires",
    "text": "Importation des biblioth√®ques necessaires\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nimport numpy as np"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-donn√©es-et-verification-suscinte-de-leur-qualit√©",
    "href": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-donn√©es-et-verification-suscinte-de-leur-qualit√©",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Chargement des donn√©es et verification suscinte de leur qualit√©",
    "text": "Chargement des donn√©es et verification suscinte de leur qualit√©\n¬†¬†¬†¬†¬†¬†Avant de commencer quoi que ce soit, il est n√©cessaire de pr√©ciser que les donn√©es proviennent de la plateforme kaggle (!cliquez ici pour y acceder).\n\ndf = pd.read_csv('diabetes-dataset.csv')\nprint('\\nAffichage des donn√©es\\n')\n\n\nAffichage des donn√©es\n\ndisplay(df.head(5))\n\n   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n0            6      148             72  ...                     0.627   50        1\n1            1       85             66  ...                     0.351   31        0\n2            8      183             64  ...                     0.672   32        1\n3            1       89             66  ...                     0.167   21        0\n4            0      137             40  ...                     2.288   33        1\n\n[5 rows x 9 columns]\n\nprint('\\nInformations sur les donn√©es\\n')\n\n\nInformations sur les donn√©es\n\ndisplay(df.info)\n\n&lt;bound method DataFrame.info of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\nprint('\\nResum√© statistique des donn√©es\\n')\n\n\nResum√© statistique des donn√©es\n\ndisplay(df.describe)\n\n&lt;bound method NDFrame.describe of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-donn√©es-et-verification-suscinte-de-leur-qualit√©-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-donn√©es-et-verification-suscinte-de-leur-qualit√©-1",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Chargement des donn√©es et verification suscinte de leur qualit√©",
    "text": "Chargement des donn√©es et verification suscinte de leur qualit√©\n\nAffichage des informations sur les donn√©es\n\ndf = pd.read_csv('diabetes-dataset.csv')\nprint('\\nAffichage des donn√©es\\n')\n\n\nAffichage des donn√©es\n\ndisplay(df.head(5))\n\n   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n0            6      148             72  ...                     0.627   50        1\n1            1       85             66  ...                     0.351   31        0\n2            8      183             64  ...                     0.672   32        1\n3            1       89             66  ...                     0.167   21        0\n4            0      137             40  ...                     2.288   33        1\n\n[5 rows x 9 columns]\n\nprint('\\nInformations sur les donn√©es\\n')\n\n\nInformations sur les donn√©es\n\ndisplay(df.info)\n\n&lt;bound method DataFrame.info of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\nprint('\\nResum√© statistique des donn√©es\\n')\n\n\nResum√© statistique des donn√©es\n\ndisplay(df.describe)\n\n&lt;bound method NDFrame.describe of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\n\n\n\nV√©rification des valeurs manquantes\n\ndf.columns.isna().sum()\n\n0\n\n\n¬†¬†¬†¬†¬†¬†Il y‚Äô a aucune valeur manquante car les donn√©es ont bien √©t√© nettoy√©es avant d‚Äô√™tre mise √† disposition sur kaggle.\n\n\nAffichage des statistiques des variables\n¬†¬†¬†¬†¬†¬†Etant donn√©es que les informations sur les variables sont en ce moment ou j‚Äô√©cris indisponibles sur kaggle.\n\nprint('Affichage des valeurs uniques des variables\\n')\n\nAffichage des valeurs uniques des variables\n\nfor variable in df.columns:\n    if variable != \"DiabetesPedigreeFunction\": # je saute car √ßa fait beaucoup long √† l'affichage\n      print(f'\\n {variable}\\n')\n      print(df[variable].unique())\n\n\n Pregnancies\n\n[ 6  1  8  0  5  3 10  2  4  7  9 11 13 15 17 12 14]\n\n Glucose\n\n[148  85 183  89 137 116  78 115 197 125 110 168 139 189 166 100 118 107\n 103 126  99 196 119 143 147  97 145 117 109 158  88  92 122 138 102  90\n 111 180 133 106 171 159 146  71 105 101 176 150  73 187  84  44 141 114\n  95 129  79   0  62 131 112 113  74  83 136  80 123  81 134 142 144  93\n 163 151  96 155  76 160 124 162 132 120 173 170 128 108 154  57 156 153\n 188 152 104  87  75 179 130 194 181 135 184 140 177 164  91 165  86 193\n 191 161 167  77 182 157 178  61  98 127  82  72 172  94 175 195  68 186\n 198 121  67 174 199  56 169 149  65 190]\n\n BloodPressure\n\n[ 72  66  64  40  74  50   0  70  96  92  80  60  84  30  88  90  94  76\n  82  75  58  78  68 110  56  62  85  86  48  44  65 108  55 122  54  52\n  98 104  95  46 102 100  61  24  38 106 114]\n\n SkinThickness\n\n[35 29  0 23 32 45 19 47 38 30 41 33 26 15 36 11 31 37 42 25 18 24 39 27\n 21 34 10 60 13 20 22 28 54 40 51 56 14 17 50 44 12 46 16  7 52 43 48  8\n 49 63 99]\n\n Insulin\n\n[  0  94 168  88 543 846 175 230  83  96 235 146 115 140 110 245  54 192\n 207  70 240  82  36  23 300 342 304 142 128  38 100  90 270  71 125 176\n  48  64 228  76 220  40 152  18 135 495  37  51  99 145 225  49  50  92\n 325  63 284 119 204 155 485  53 114 105 285 156  78 130  55  58 160 210\n 318  44 190 280  87 271 129 120 478  56  32 744 370  45 194 680 402 258\n 375 150  67  57 116 278 122 545  75  74 182 360 215 184  42 132 148 180\n 205  85 231  29  68  52 255 171  73 108  43 167 249 293  66 465  89 158\n  84  72  59  81 196 415 275 165 579 310  61 474 170 277  60  14  95 237\n 191 328 250 480 265 193  79  86 326 188 106  65 166 274  77 126 330 600\n 185  25  41 272 321 144  15 183  91  46 440 159 540 200 335 387  22 291\n 392 178 127 510  16 112]\n\n BMI\n\n[33.6 26.6 23.3 28.1 43.1 25.6 31.  35.3 30.5  0.  37.6 38.  27.1 30.1\n 25.8 30.  45.8 29.6 43.3 34.6 39.3 35.4 39.8 29.  36.6 31.1 39.4 23.2\n 22.2 34.1 36.  31.6 24.8 19.9 27.6 24.  33.2 32.9 38.2 37.1 34.  40.2\n 22.7 45.4 27.4 42.  29.7 28.  39.1 19.4 24.2 24.4 33.7 34.7 23.  37.7\n 46.8 40.5 41.5 25.  25.4 32.8 32.5 42.7 19.6 28.9 28.6 43.4 35.1 32.\n 24.7 32.6 43.2 22.4 29.3 24.6 48.8 32.4 38.5 26.5 19.1 46.7 23.8 33.9\n 20.4 28.7 49.7 39.  26.1 22.5 39.6 29.5 34.3 37.4 33.3 31.2 28.2 53.2\n 34.2 26.8 55.  42.9 34.5 27.9 38.3 21.1 33.8 30.8 36.9 39.5 27.3 21.9\n 40.6 47.9 50.  25.2 40.9 37.2 44.2 29.9 31.9 28.4 43.5 32.7 67.1 45.\n 34.9 27.7 35.9 22.6 33.1 30.4 52.3 24.3 22.9 34.8 30.9 40.1 23.9 37.5\n 35.5 42.8 42.6 41.8 35.8 37.8 28.8 23.6 35.7 36.7 45.2 44.  46.2 35.\n 43.6 44.1 18.4 29.2 25.9 32.1 36.3 40.  25.1 27.5 45.6 27.8 24.9 25.3\n 37.9 27.  26.  38.7 20.8 36.1 30.7 32.3 52.9 21.  39.7 25.5 26.2 19.3\n 38.1 23.5 45.5 23.1 39.9 36.8 21.8 41.  42.2 34.4 27.2 36.5 29.8 39.2\n 38.4 36.2 48.3 20.  22.3 45.7 23.7 22.1 42.1 42.4 18.2 26.4 45.3 37.\n 24.5 32.2 59.4 21.2 26.7 30.2 46.1 41.3 38.8 35.2 42.3 40.7 46.5 33.5\n 37.3 30.3 26.3 21.7 36.4 28.5 26.9 38.6 31.3 19.5 20.1 40.8 23.4 28.3\n 38.9 57.3 35.6 49.6 44.6 24.1 44.5 41.2 49.3 46.3]\n\n Age\n\n[50 31 32 21 33 30 26 29 53 54 34 57 59 51 27 41 43 22 38 60 28 45 35 46\n 56 37 48 40 25 24 58 42 44 39 36 23 61 69 62 55 65 47 52 66 49 63 67 72\n 81 64 70 68]\n\n Outcome\n\n[1 0]\n\n\nAu vu de ces valeurs, on peut dire que (vu qu‚Äôil n‚Äôy a aucune description des disponible sur kaggle):\n\npregnancies represente le nombre de grossesses contract√©es;\nglucose represente la quantit√© de glucose dans le sang;\nBloodPressure represente la pression sanguine;\nSkinThickness represente l‚Äô√©paisseur du pli cutan√© tricipital;\nBMI correspond √† l‚ÄôIndice de Masse Corporelle (IMC)\nAge de la patiente\nInsulin repr√©sente la concentration s√©rique d‚Äôinsuline mesur√©e (g√©n√©ralement en micro-unit√©s par millilitre (ŒºU/ml))\nDiabetesPedigreeFunction repr√©sente une mesure de la pr√©disposition g√©n√©tique au diab√®te\nOutcome represente l‚Äô√©tat de la patiente (atteinte ou non du diab√®te)"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#analyse-exploratoire-des-donn√©es",
    "href": "FORMATIONS/logistic_regression_diabetes.html#analyse-exploratoire-des-donn√©es",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Analyse exploratoire des donn√©es",
    "text": "Analyse exploratoire des donn√©es\n¬†¬†¬†¬†¬†¬†Cette analyse est effectu√©e dans l‚Äôoptique de mieux comprendre les donn√©es afin de pouvoir bien sp√©cifier le mod√®le logistique.\n\nAnalyse descriptives rapides (Voir la distribution des donn√©es)\n\n# Cr√©ation de la figure avec une grille 3 lignes x 2 colonnes\nfig, axes = plt.subplots(3, 2, figsize=(12, 12))\n\n# Premier sous-graphe : Distribution du nombre de grossesses\nsns.histplot(data=df['Pregnancies'], ax=axes[0, 0])\naxes[0, 0].set_title(\"Distribution du nombre de grossesses\")\n\n# Deuxi√®me sous-graphe : Distribution du niveau de glucose\nsns.histplot(data=df['Glucose'], ax=axes[0, 1])\naxes[0, 1].set_title(\"Distribution du niveau de glucose\")\n\n# Troisi√®me sous-graphe : Distribution de la pression sanguine\nsns.histplot(data=df['BloodPressure'], ax=axes[1, 0])\naxes[1, 0].set_title(\"Distribution de la pression sanguine\")\n\n# Quatri√®me sous-graphe : Distribution de l'√©paisseur du pli cutan√© (SkinThickness)\nsns.histplot(data=df['SkinThickness'], ax=axes[1, 1])\naxes[1, 1].set_title(\"Distribution de l'√©paisseur du pli cutan√©\")\n\n# Cinqui√®me sous-graphe : Distribution de l'insuline\nsns.histplot(data=df['Insulin'], ax=axes[2, 0])\naxes[2, 0].set_title(\"Distribution de l'insuline\")\n\n# Sixi√®me sous-graphe : Distribution de l'IMC (BMI)\nsns.histplot(data=df['BMI'], ax=axes[2, 1])\naxes[2, 1].set_title(\"Distribution de l'IMC\")\n\n# Ajustement automatique des espaces pour\n# √©viter le chevauchement des titres et labels\nplt.tight_layout()\n\n# Affichage de la figure\nplt.show()\n\n\n\n\n\n\nVerification de la colin√©arit√©\n¬†¬†¬†¬†¬†¬†En effet avant de sp√©cifier un mod√®le, il faut s‚Äôassurer qu‚Äôil n‚Äôy a pas multicolin√©arit√©. C‚Äôest-√†-dire verifier que les variables ne sont pas corr√©l√©es entre elles ce qui permettra d‚Äô√©viter de fausses estimations.\n\n# S√©lectionner que les variables num√©riques des donn√©es\ndf_variables_numeriques = df.select_dtypes(include=[np.number])\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(df_variables_numeriques.corr(), annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Heatmap de correlation des variables num√©riques du jeu de donn√©es')\nplt.tight_layout()\nplt.show()\n\n\n\n\n¬†¬†¬†¬†¬†¬†Ce corr√©lollogramme montre que les variables ne sont pas lin√©airement corr√©l√©es entre elle. Donc on peut ajuster le mod√®le de regression logistique."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#sp√©cification-et-√©valution-du-mod√®le-logistique",
    "href": "FORMATIONS/logistic_regression_diabetes.html#sp√©cification-et-√©valution-du-mod√®le-logistique",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Sp√©cification et √©valution du mod√®le logistique",
    "text": "Sp√©cification et √©valution du mod√®le logistique\n¬†¬†¬†¬†¬†¬†A ce niveau, j‚Äôai partitionn√© les donn√©es en ammont dans le but de faire du machine learning (ajustement, prediction et validation du mod√®le) plus tard (dans la section suivante). Nous avons les donn√©es d‚Äôentrainement qui constituent 80% des donn√©es et des donn√©es de test qui en constituent 20. Ici j‚Äôajuste juste un mod√®le de regression logistique aux donn√©es que j‚Äôessaie d‚Äôinterpreter.\n\n# Les biblioth√®ques de machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\nfrom sklearn.inspection import permutation_importance\n\n\n# Separation des variables explicatives and de la variable d√©pendante\n\n# X : matrice des variables explicatives\nX = df.drop('Outcome', axis=1)\n\n# y : variable d√©pendante\ny = df['Outcome']\n\n# partition des donn√©es en donn√©es de tests et d'entrainement\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nimport statsmodels.api as sm\n\n# Ajout d'une colonne de 1 pour l'intercept (obligatoire dans statsmodels)\nX_train_const = sm.add_constant(X_train)\n\n# Cr√©ation du mod√®le logistique\nmodel = sm.Logit(y_train, X_train_const)\n\n# Ajustement du mod√®le\nresult = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.467835\n         Iterations 6\n\n# Affichage du r√©sum√© avec les p-values\ndisplay(result.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                Outcome   No. Observations:                  614\nModel:                          Logit   Df Residuals:                      605\nMethod:                           MLE   Df Model:                            8\nDate:                Wed, 28 May 2025   Pseudo R-squ.:                  0.2752\nTime:                        21:13:00   Log-Likelihood:                -287.25\nconverged:                       True   LL-Null:                       -396.34\nCovariance Type:            nonrobust   LLR p-value:                 9.311e-43\n============================================================================================\n                               coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nconst                       -9.0359      0.837    -10.802      0.000     -10.675      -7.396\nPregnancies                  0.0645      0.036      1.791      0.073      -0.006       0.135\nGlucose                      0.0341      0.004      8.055      0.000       0.026       0.042\nBloodPressure               -0.0139      0.006     -2.260      0.024      -0.026      -0.002\nSkinThickness                0.0031      0.008      0.397      0.691      -0.012       0.019\nInsulin                     -0.0018      0.001     -1.782      0.075      -0.004       0.000\nBMI                          0.1026      0.017      5.948      0.000       0.069       0.136\nDiabetesPedigreeFunction     0.6945      0.330      2.107      0.035       0.049       1.341\nAge                          0.0371      0.011      3.400      0.001       0.016       0.058\n============================================================================================"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#qualit√©-dajustement",
    "href": "FORMATIONS/logistic_regression_diabetes.html#qualit√©-dajustement",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Qualit√© d‚ÄôAjustement",
    "text": "Qualit√© d‚ÄôAjustement\n\nLog-Likelihood : -287.25. Un log-vraisemblance plus √©lev√© (moins n√©gatif) indique un meilleur ajustement.\nPseudo R-squared : 0.2752. Cela signifie que le mod√®le explique environ 27.52% de la variabilit√© dans les donn√©es, ce qui indique un ajustement mod√©r√©. Dans les mod√®les lin√©aires g√©n√©ralis√©s, il est fr√©quent d‚Äôavoir des pseudo-R2 un peu faible.\nLLR p-value : 9.311e-43, tr√®s faible, indiquant que le mod√®le est significatif globalement."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#ad√©quation-du-mod√®le",
    "href": "FORMATIONS/logistic_regression_diabetes.html#ad√©quation-du-mod√®le",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Ad√©quation du Mod√®le",
    "text": "Ad√©quation du Mod√®le\n\nConvergence : Le mod√®le a converg√© en 6 it√©rations, sugg√©rant un bon comportement de l‚Äôalgorithme d‚Äôoptimisation.\nDf Model : 8, indiquant 8 variables explicatives."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#interpr√©tation-des-coefficients-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#interpr√©tation-des-coefficients-1",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Interpr√©tation des Coefficients",
    "text": "Interpr√©tation des Coefficients\nLa probabilit√© \\(p\\) que \\(y = 1\\) (c‚Äôest-√†-dire que la patiente ait le diab√®te) est donn√©e par la fonction sigmo√Øde :\n\\(p = \\frac{1}{1 + \\exp(-\\beta)}\\)\no√π : - \\(\\beta\\) est le coefficient du mod√®le de r√©gression logistique.\n\n\\(\\exp(-\\beta)\\) repr√©sente l‚Äôexponentielle de \\(-\\beta\\).\n\nAinsi, cette fonction transforme la valeur lin√©aire ( ) en une probabilit√© entre 0 et 1.\n\nIntercept (-9.0359) : Lorsque toutes les variables sont √† 0, la probabilit√© pr√©dite que y=1 est proche de 0.\nGlucose (0.0341, p&lt;0.001) : Une augmentation de 1 unit√© de glucose augmente significativement les odds de l‚Äôissue y=1.\nBMI (0.1026, p&lt;0.001) : Indique une relation positive forte entre l‚ÄôIMC et l‚Äôissue.\nBloodPressure (-0.0139, p=0.024) : Relation n√©gative significative, mais l‚Äôeffet est faible.\nDiabetesPedigreeFunction (0.6945, p=0.035) : Un ant√©c√©dent familial a un impact positif significatif.\nAge (0.0371, p=0.001) : L‚Äô√¢ge est un facteur significatif.\nSkinThickness et Insulin : Effet non significatif (au seuil de risque \\(\\alpha\\) = 0,05)."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#conclusion-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#conclusion-1",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Conclusion",
    "text": "Conclusion\nLe mod√®le a une bonne capacit√© pr√©dictive mais n‚Äôexplique pas toute la variabilit√©. Certaines variables sont significatives (Glucose, BMI, Age), alors que d‚Äôautres, comme l‚ÄôInsuline, ne le sont pas."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#machine-learning",
    "href": "FORMATIONS/logistic_regression_diabetes.html#machine-learning",
    "title": "Mod√©lisation des donn√©es √† variables d√©pendantes qualitatives : Regression logistique √† variable d√©pendante dichotomique",
    "section": "Machine learning",
    "text": "Machine learning\n\nAjustement du mod√®le aux donn√©es d‚Äôapprentissage\n\n# Initialisation et entrainnement du classificateur (Regression Logistique)\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000)\n\n# faire les prediction sur les donn√©es de test\ny_pred = model.predict(X_test)\n\n# calcul du score de l'exactitude\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy Score: {accuracy:.4f}')\n\nAccuracy Score: 0.7468\n\n\n¬†¬†¬†¬†¬†¬†L‚Äôaccuracy score de 0.7468 signifie que le mod√®le a correctement class√© 74.68% des √©chantillons dans le jeu de test. Cette m√©trique donne une indication de la proportion des pr√©dictions correctes par rapport au nombre total d‚Äôobservations. Plus l‚Äôaccuracy est proche de 1 (ou 100%), plus le mod√®le est performant.\n\n\nEvaluation du mod√®le\n\n\nMatrice de confusion\n\n\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Matrice de confusion')\nplt.xlabel('Donn√©es pr√©dites')\nplt.ylabel('Donn√©es observ√©es')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\\[\n\\textbf{Vrais Positifs (VP)} = 37 \\quad \\text{(Mod√®le pr√©dit que la patiente a le diab√®te et c'est correct)}\n\\] \\[\n\\textbf{Faux Positifs (FP)} = 21 \\quad \\text{(Mod√®le pr√©dit que la patiente a le diab√®te, mais c'est incorrect)}\n\\]\n\\[\n\\textbf{Faux N√©gatifs (FN)} = 18 \\quad \\text{(Mod√®le pr√©dit que la patiente n'a pas le diab√®te, mais c'est incorrect)}\n\\]\n\\[\n\\textbf{Vrais N√©gatifs (VN)} = 78 \\quad \\text{(Mod√®le pr√©dit que la patiente n'a pas le diab√®te et c'est correct)}\n\\]\n\n\n\nM√©triques de performance\n\n\n\n\\[\n\\text{Pr√©cision} = \\frac{\\text{VP}}{\\text{VP} + \\text{FP}} = \\frac{37}{37 + 21} = \\frac{37}{58} \\approx 0.6379\n\\]\n\\[\n\\textbf{Rappel} (Recall) :\n\\text{Rappel} = \\frac{\\text{VP}}{\\text{VP} + \\text{FN}} = \\frac{37}{37 + 18} = \\frac{37}{55} \\approx 0.6727\n\\]\n\\[\n\\textbf{Score F1} (F1-Score) :\n\\text{F1-Score} = 2 \\times \\frac{\\text{Pr√©cision} \\times \\text{Rappel}}{\\text{Pr√©cision} + \\text{Rappel}} = 2 \\times \\frac{0.6379 \\times 0.6727}{0.6379 + 0.6727} \\approx 0.6548\n\\]\n\\[\n\\textbf{Exactitude} (Accuracy) :\n\\text{Exactitude} = \\frac{\\text{VP} + \\text{VN}}{\\text{Total}} = \\frac{37 + 78}{37 + 78 + 21 + 18} = \\frac{115}{154} \\approx 0.7468\n\\]\n\n\nCourbe de ROC\n\n\n\ny_prob = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Courbe ROC (Aire = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('Taux de Faux Positifs')\nplt.ylabel('Taux de Vrais Positifs')\nplt.title('Caract√©ristique de Performance du Mod√®le (Courbe ROC)')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n¬†¬†¬†¬†¬†¬†La courbe ROC (Receiver Operating Characteristic) est un graphique qui permet d‚Äô√©valuer la performance d‚Äôun mod√®le de classification binaire. Elle trace la relation entre :\n\nLe Taux de Vrais Positifs (TPR, True Positive Rate) : La proportion des vrais positifs parmi les cas positifs r√©els.\nLe Taux de Faux Positifs (FPR, False Positive Rate) : La proportion des faux positifs parmi les cas n√©gatifs r√©els.\n\n¬†¬†¬†¬†¬†¬†La courbe ROC montre comment le mod√®le se comporte pour diff√©rents seuils de d√©cision. Un mod√®le parfait aura une courbe qui monte rapidement vers le coin sup√©rieur gauche (haute TPR et faible FPR), tandis qu‚Äôun mod√®le al√©atoire suivra la diagonale du graphique (FPR = TPR).\n¬†¬†¬†¬†¬†¬†L‚ÄôAire Sous la Courbe (AUC) mesure la qualit√© globale du mod√®le. Une AUC proche de 1 indique un excellent mod√®le, tandis qu‚Äôune AUC proche de 0.5 indique un mod√®le √©quivalent √† un choix al√©atoire.\nDans notre cas AUC vaut 0,81 donc notre mod√®le tient la route.\n\n\n\nVerifions qu‚Äôon a les m√™me coefficients que ceux de l‚Äôajustement √† la section pr√©c√©dente\n\n\n\n\nmodel.intercept_\n\narray([-9.00707993])\n\n\n\n# affichage des coefficients estim√©s du mod√®le\nmodel.coef_\n\narray([[ 0.06436473,  0.03410147, -0.01387533,  0.00326297, -0.00180169,\n         0.10262329,  0.62588811,  0.03708342]])\n\n\nEt oui on a les m√™mes coefficients.\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour √† la page d‚Äôaccueuil"
  },
  {
    "objectID": "FORMATIONS/PAYANTES/R/FormationR.html",
    "href": "FORMATIONS/PAYANTES/R/FormationR.html",
    "title": "Djamaldbz - Formations en R en pr√©sentiel et en ligne avec Djamal et Sa√Ød",
    "section": "",
    "text": "Ces formations sont con√ßues pour diff√©rents publics cibles : √©tudiants en pharmacie, m√©decine, biologie, statistiques et ceux aui sont dans des domaines n√©cessitant les stats ou pas. Chaque session dure 2 heures, avec une fr√©quence de 2 sessions par semaine. Les formations d√©butent le 22 f√©vrier 2025.\n\n\n\n\n5 000 FCFA par session de 2 heures.\nChaque formation compl√®te comprend 4 sessions, soit 20 000 FCFA par participant.\n\n\n\n\n\n\n\nCalendrier des Formations\n\n\nDate\nPublic.cible\nSujet\n\n\n\n\n22 f√©vrier\nPharmacie\nIntroduction √† R\n\n\n22 f√©vrier\nM√©decine\nIntroduction √† R\n\n\n22 f√©vrier\nBiologie\nIntroduction √† R\n\n\n26 f√©vrier\nStatistiques\nR pour les statisticiens\n\n\n\n\n\n\n\n\n\n\n\nObjectif : Apprendre √† g√©rer, analyser et visualiser des donn√©es pharmacologiques.\nSessions :\n\nIntroduction √† R.\nGestion des donn√©es pharmacologiques.\nVisualisation des donn√©es.\nAnalyse statistique (tests t, ANOVA).\n\n\n\n\n\n\nObjectif : Explorer des donn√©es cliniques et √©pid√©miologiques.\nSessions :\n\nIntroduction √† R.\nStatistiques descriptives.\nVisualisation des donn√©es m√©dicales.\n\n\n\n\n\n\nObjectif : Analyser des donn√©es biologiques\nSessions :\n\nIntroduction √† R.\nVisualisation des donn√©es biologiques.\nAnalyse statistique.\n\n\n\n\n\n\nObjectif : Approfondir les outils statistiques et analytiques.\nSessions :\n\nR pour les statisticiens.\nVisualisations avanc√©es avec ggplot2.\nMod√©lisation statistique (mod√®les lin√©aires, g√©n√©ralis√©s).\nProgrammation avanc√©e (cr√©ation de fonctions, etc ‚Ä¶).\n\n\n\n\n\n\n\n\nRappel\n\n\n\nPour celles et ceux qui ne font pas partie des domaines mentionn√©s, ne vous inqui√©tez pas : cette formation est con√ßue pour √™tre accessible et adapt√©e √† tous les profils. Vous en tirerez pleinement profit !\n\n\n\n\n\n\n\n\nLocalisation pour la formation en pr√©sentiel\n\n\n\nLes participants doivent √™tre au Burkina-Faso, plus pr√©cisement dans la ville de Bobo-Dioulasso. Les s√©ances en ligne interviendront rarement. Elles serviront √† donner certains details et seront une alternatives en cas d‚Äôemp√™chement !!!\n\n\n\n\n\n\n\n\nLocalisation pour la formation en pr√©sentiel\n\n\n\nPour les participants ayant des empechements (localisation g√©ographique, timing etc ‚Ä¶), une formartion ligne sera possible mais au lieu de 2h ce sera 1h30 !!!\n\n\n\n\n\n\nLes participants b√©n√©ficieront de formations pratiques, avec des cas d‚Äôutilisation adapt√©s √† leur domaine. Inscrivez-vous d√®s maintenant pour r√©server votre place! üòä\n\nPOUR PLUS D‚ÄôINFORMATIONS !!!\n\n¬†¬†¬†¬†¬†¬†Veuillez contacter le num√©ro whatsapp suivant : +226 57036356"
  },
  {
    "objectID": "FORMATIONS/PAYANTES/R/FormationR.html#plan-des-formations-en-r---niveau-1",
    "href": "FORMATIONS/PAYANTES/R/FormationR.html#plan-des-formations-en-r---niveau-1",
    "title": "Djamaldbz - Formations en R en pr√©sentiel et en ligne avec Djamal et Sa√Ød",
    "section": "",
    "text": "Ces formations sont con√ßues pour diff√©rents publics cibles : √©tudiants en pharmacie, m√©decine, biologie, statistiques et ceux aui sont dans des domaines n√©cessitant les stats ou pas. Chaque session dure 2 heures, avec une fr√©quence de 2 sessions par semaine. Les formations d√©butent le 22 f√©vrier 2025.\n\n\n\n\n5 000 FCFA par session de 2 heures.\nChaque formation compl√®te comprend 4 sessions, soit 20 000 FCFA par participant.\n\n\n\n\n\n\n\nCalendrier des Formations\n\n\nDate\nPublic.cible\nSujet\n\n\n\n\n22 f√©vrier\nPharmacie\nIntroduction √† R\n\n\n22 f√©vrier\nM√©decine\nIntroduction √† R\n\n\n22 f√©vrier\nBiologie\nIntroduction √† R\n\n\n26 f√©vrier\nStatistiques\nR pour les statisticiens\n\n\n\n\n\n\n\n\n\n\n\nObjectif : Apprendre √† g√©rer, analyser et visualiser des donn√©es pharmacologiques.\nSessions :\n\nIntroduction √† R.\nGestion des donn√©es pharmacologiques.\nVisualisation des donn√©es.\nAnalyse statistique (tests t, ANOVA).\n\n\n\n\n\n\nObjectif : Explorer des donn√©es cliniques et √©pid√©miologiques.\nSessions :\n\nIntroduction √† R.\nStatistiques descriptives.\nVisualisation des donn√©es m√©dicales.\n\n\n\n\n\n\nObjectif : Analyser des donn√©es biologiques\nSessions :\n\nIntroduction √† R.\nVisualisation des donn√©es biologiques.\nAnalyse statistique.\n\n\n\n\n\n\nObjectif : Approfondir les outils statistiques et analytiques.\nSessions :\n\nR pour les statisticiens.\nVisualisations avanc√©es avec ggplot2.\nMod√©lisation statistique (mod√®les lin√©aires, g√©n√©ralis√©s).\nProgrammation avanc√©e (cr√©ation de fonctions, etc ‚Ä¶).\n\n\n\n\n\n\n\n\nRappel\n\n\n\nPour celles et ceux qui ne font pas partie des domaines mentionn√©s, ne vous inqui√©tez pas : cette formation est con√ßue pour √™tre accessible et adapt√©e √† tous les profils. Vous en tirerez pleinement profit !\n\n\n\n\n\n\n\n\nLocalisation pour la formation en pr√©sentiel\n\n\n\nLes participants doivent √™tre au Burkina-Faso, plus pr√©cisement dans la ville de Bobo-Dioulasso. Les s√©ances en ligne interviendront rarement. Elles serviront √† donner certains details et seront une alternatives en cas d‚Äôemp√™chement !!!\n\n\n\n\n\n\n\n\nLocalisation pour la formation en pr√©sentiel\n\n\n\nPour les participants ayant des empechements (localisation g√©ographique, timing etc ‚Ä¶), une formartion ligne sera possible mais au lieu de 2h ce sera 1h30 !!!\n\n\n\n\n\n\nLes participants b√©n√©ficieront de formations pratiques, avec des cas d‚Äôutilisation adapt√©s √† leur domaine. Inscrivez-vous d√®s maintenant pour r√©server votre place! üòä\n\nPOUR PLUS D‚ÄôINFORMATIONS !!!\n\n¬†¬†¬†¬†¬†¬†Veuillez contacter le num√©ro whatsapp suivant : +226 57036356"
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html",
    "href": "FORMATIONS/poisson_paludisme.html",
    "title": "Mod√©lisation des donn√©es de comptage",
    "section": "",
    "text": "Le monde actuel est confront√© √† de multiples risques sanitaires, notamment ceux li√©s aux maladies vectorielles telles que le paludisme. En effet, le paludisme est la maladie la plus mortelle transmise par les moustiques dans le monde ((OMS) 2023). Selon l‚ÄôOMS, plusieurs millions de personnes ont √©t√© infect√©es par le paludisme en 2022 (environ 249 millions), entra√Ænant pr√®s de 608 000 d√©c√®s(Mondiale de la Sant√©) 2023).\nPlusieurs actions ont √©t√© men√©es pour lutter contre ce fl√©au, notamment la distribution de moustiquaires, les campagnes de sensibilisation √† l‚Äôhygi√®ne, la chimiopr√©vention saisonni√®re, ainsi que le traitement intermittent pour les femmes enceintes.\nDjamaland a √©t√© choisi comme pays pour la mise en oeuvre d‚Äôune intervention progressive, principalement en raison de sa forte incidence du paludisme. L‚Äôintervention comprend quatre phases et couvre l‚Äôensemble des r√©gions du pays.\nVoici la description de chaque phase :\n\nPhase 1 : Aucun village n‚Äôa re√ßu d‚Äôintervention.\nPhase 2 : Les quatre r√©gions ont b√©n√©fici√© de la distribution de moustiquaires.\nPhase 3 : En plus de la distribution de moustiquaires, des actions de sensibilisation sur les bonnes pratiques d‚Äôutilisation ont √©t√© mises en place.\nPhase 3 suite : En compl√©ment de la distribution et de la sensibilisation, un programme de partage des techniques de bonne hygi√®ne a √©t√© int√©gr√©.\n\nLa base contenait √©galement des informations sur les facteurs environnementaux (pression atmosph√©rique, vitesse du vent, indice UV, humidit√© relative).\nLe but de cette √©tude est donc d‚Äô√©valuer l‚Äôimpact de l‚Äôintervention durant ces diff√©rentes phases.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(MASS) \nlibrary(car)\n\n\nInformation sur les variables\n\n¬†¬†¬†¬†¬†¬†On affiche ici les informations sur les variables de la base de donn√©es. On voit qu‚Äôil y‚Äôa 19 colonnes (variables) et 1040 lignes (observations).\n\ndata %&gt;%\n  glimpse()\n\nRows: 1.040\nColumns: 19\n$ Semaine                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14‚Ä¶\n$ R√©gion                   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R‚Ä¶\n$ Saison                   &lt;chr&gt; \"Seche\", \"Seche\", \"Seche\", \"Seche\", \"Seche\", ‚Ä¶\n$ Phase                    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ `Taux de couverture`     &lt;dbl&gt; 0,1575155, 0,2576610, 0,1817954, 0,2766035, 0‚Ä¶\n$ Temp√©rature              &lt;dbl&gt; 25,52402, 31,06084, 28,69764, 28,11317, 30,57‚Ä¶\n$ Humidit√©                 &lt;dbl&gt; 43,75700, 44,61046, 51,25039, 49,40113, 48,49‚Ä¶\n$ Pluviom√©trie             &lt;dbl&gt; 16,0111516, 8,2997481, 44,7344354, 38,9232328‚Ä¶\n$ `Vitesse du vent`        &lt;dbl&gt; 6,239284, 7,855973, 3,604419, 6,324740, 2,649‚Ä¶\n$ `Pression atmosph√©rique` &lt;dbl&gt; 1015,6362, 1008,8631, 1008,3488, 1022,2085, 1‚Ä¶\n$ `Indice de chaleur`      &lt;dbl&gt; 19,83300, 25,96362, 23,67617, 22,94634, 25,61‚Ä¶\n$ `Couverture nuageuse`    &lt;dbl&gt; 82,280557, 57,209495, 32,469455, 14,619983, 1‚Ä¶\n$ `Vent en hauteur`        &lt;dbl&gt; 14,668663, 8,698154, 11,789413, 14,660355, 11‚Ä¶\n$ `Indice UV`              &lt;dbl&gt; 7,03602105, 10,09389904, 0,64470344, 5,463245‚Ä¶\n$ `Temp√©rature de l'eau`   &lt;dbl&gt; 33,22569, 28,26913, 28,00962, 29,71320, 28,66‚Ä¶\n$ `Humidit√© √† l‚Äôombre`     &lt;dbl&gt; 53,83643, 64,71406, 55,98915, 63,04538, 62,21‚Ä¶\n$ A√©rosols                 &lt;dbl&gt; 1,477347, 4,530642, 34,596087, 80,345317, 18,‚Ä¶\n$ `Cas palustres`          &lt;dbl&gt; 97, 62, 76, 66, 163, 67, 154, 168, 79, 58, 15‚Ä¶\n$ Dates                    &lt;date&gt; 2021-01-08, 2021-01-15, 2021-01-22, 2021-01-‚Ä¶\n\n\n¬†¬†¬†¬†¬†¬†On affiche ensuite un r√©sum√© statistique des variables dans le but de reperer certaines anomalies s‚Äôil y en a. Mais dans ce cas, il y‚Äôen a pas car j‚Äôai moi m√™me g√©n√©r√© les donn√©es et donc j‚Äôai veill√© √† ce qu‚Äôil n y ait pas de valeurs manquantes.\n\nlibrary(dplyr)\ndata %&gt;%\n  summary()\n\n    Semaine       R√©gion      Saison          Phase   Taux de couverture\n Min.   :  1,00   R1:260   Length:1040        0:516   Min.   :0,1001    \n 1st Qu.: 65,75   R2:260   Class :character   1:104   1st Qu.:0,2019    \n Median :130,50   R3:260   Mode  :character   2:208   Median :0,4023    \n Mean   :130,50   R4:260                      3:212   Mean   :0,4301    \n 3rd Qu.:195,25                                       3rd Qu.:0,6584    \n Max.   :260,00                                       Max.   :0,8998    \n  Temp√©rature       Humidit√©      Pluviom√©trie       Vitesse du vent\n Min.   :13,64   Min.   :35,41   Min.   :  0,04131   Min.   :2,001  \n 1st Qu.:23,97   1st Qu.:48,81   1st Qu.: 21,90016   1st Qu.:3,975  \n Median :27,09   Median :54,81   Median : 41,11740   Median :5,975  \n Mean   :27,08   Mean   :61,99   Mean   : 64,46301   Mean   :5,913  \n 3rd Qu.:30,21   3rd Qu.:78,54   3rd Qu.:106,59262   3rd Qu.:7,826  \n Max.   :40,19   Max.   :95,11   Max.   :199,59270   Max.   :9,998  \n Pression atmosph√©rique Indice de chaleur Couverture nuageuse Vent en hauteur \n Min.   : 980,4         Min.   : 8,405    Min.   : 0,0502     Min.   : 5,007  \n 1st Qu.:1006,5         1st Qu.:19,530    1st Qu.:25,7869     1st Qu.: 7,288  \n Median :1012,9         Median :22,434    Median :50,2530     Median : 9,831  \n Mean   :1012,9         Mean   :22,400    Mean   :50,5150     Mean   : 9,916  \n 3rd Qu.:1019,5         3rd Qu.:25,580    3rd Qu.:76,9193     3rd Qu.:12,654  \n Max.   :1041,6         Max.   :36,178    Max.   :99,9899     Max.   :14,995  \n   Indice UV         Temp√©rature de l'eau Humidit√© √† l‚Äôombre    A√©rosols       \n Min.   : 0,001759   Min.   :20,80        Min.   : 42,66     Min.   : 0,00251  \n 1st Qu.: 3,201972   1st Qu.:26,32        1st Qu.: 58,98     1st Qu.:23,49266  \n Median : 5,851642   Median :28,40        Median : 64,76     Median :47,47861  \n Mean   : 6,034127   Mean   :28,40        Mean   : 72,04     Mean   :48,53851  \n 3rd Qu.: 9,109514   3rd Qu.:30,54        3rd Qu.: 88,35     3rd Qu.:73,70956  \n Max.   :11,971607   Max.   :37,70        Max.   :104,38     Max.   :99,89207  \n Cas palustres       Dates           \n Min.   : 13,0   Min.   :2021-01-08  \n 1st Qu.: 58,0   1st Qu.:2022-04-06  \n Median : 93,0   Median :2023-07-03  \n Mean   :111,2   Mean   :2023-07-03  \n 3rd Qu.:145,2   3rd Qu.:2024-09-28  \n Max.   :466,0   Max.   :2025-12-26  \n\n\n¬†¬†¬†¬†¬†¬†Pour cette √©tude, la variable d‚Äôint√©r√™t est le nombre de nouveaux cas de paludisme enregistr√©s chaque semaine (t), avec des valeurs variant de 1 √† 260 dans les quatre r√©gions du pays."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#description-du-jeu-de-donn√©es",
    "href": "FORMATIONS/poisson_paludisme.html#description-du-jeu-de-donn√©es",
    "title": "Mod√©lisation des donn√©es de comptage",
    "section": "",
    "text": "Le monde actuel est confront√© √† de multiples risques sanitaires, notamment ceux li√©s aux maladies vectorielles telles que le paludisme. En effet, le paludisme est la maladie la plus mortelle transmise par les moustiques dans le monde ((OMS) 2023). Selon l‚ÄôOMS, plusieurs millions de personnes ont √©t√© infect√©es par le paludisme en 2022 (environ 249 millions), entra√Ænant pr√®s de 608 000 d√©c√®s(Mondiale de la Sant√©) 2023).\nPlusieurs actions ont √©t√© men√©es pour lutter contre ce fl√©au, notamment la distribution de moustiquaires, les campagnes de sensibilisation √† l‚Äôhygi√®ne, la chimiopr√©vention saisonni√®re, ainsi que le traitement intermittent pour les femmes enceintes.\nDjamaland a √©t√© choisi comme pays pour la mise en oeuvre d‚Äôune intervention progressive, principalement en raison de sa forte incidence du paludisme. L‚Äôintervention comprend quatre phases et couvre l‚Äôensemble des r√©gions du pays.\nVoici la description de chaque phase :\n\nPhase 1 : Aucun village n‚Äôa re√ßu d‚Äôintervention.\nPhase 2 : Les quatre r√©gions ont b√©n√©fici√© de la distribution de moustiquaires.\nPhase 3 : En plus de la distribution de moustiquaires, des actions de sensibilisation sur les bonnes pratiques d‚Äôutilisation ont √©t√© mises en place.\nPhase 3 suite : En compl√©ment de la distribution et de la sensibilisation, un programme de partage des techniques de bonne hygi√®ne a √©t√© int√©gr√©.\n\nLa base contenait √©galement des informations sur les facteurs environnementaux (pression atmosph√©rique, vitesse du vent, indice UV, humidit√© relative).\nLe but de cette √©tude est donc d‚Äô√©valuer l‚Äôimpact de l‚Äôintervention durant ces diff√©rentes phases.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(MASS) \nlibrary(car)\n\n\nInformation sur les variables\n\n¬†¬†¬†¬†¬†¬†On affiche ici les informations sur les variables de la base de donn√©es. On voit qu‚Äôil y‚Äôa 19 colonnes (variables) et 1040 lignes (observations).\n\ndata %&gt;%\n  glimpse()\n\nRows: 1.040\nColumns: 19\n$ Semaine                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14‚Ä¶\n$ R√©gion                   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R‚Ä¶\n$ Saison                   &lt;chr&gt; \"Seche\", \"Seche\", \"Seche\", \"Seche\", \"Seche\", ‚Ä¶\n$ Phase                    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ `Taux de couverture`     &lt;dbl&gt; 0,1575155, 0,2576610, 0,1817954, 0,2766035, 0‚Ä¶\n$ Temp√©rature              &lt;dbl&gt; 25,52402, 31,06084, 28,69764, 28,11317, 30,57‚Ä¶\n$ Humidit√©                 &lt;dbl&gt; 43,75700, 44,61046, 51,25039, 49,40113, 48,49‚Ä¶\n$ Pluviom√©trie             &lt;dbl&gt; 16,0111516, 8,2997481, 44,7344354, 38,9232328‚Ä¶\n$ `Vitesse du vent`        &lt;dbl&gt; 6,239284, 7,855973, 3,604419, 6,324740, 2,649‚Ä¶\n$ `Pression atmosph√©rique` &lt;dbl&gt; 1015,6362, 1008,8631, 1008,3488, 1022,2085, 1‚Ä¶\n$ `Indice de chaleur`      &lt;dbl&gt; 19,83300, 25,96362, 23,67617, 22,94634, 25,61‚Ä¶\n$ `Couverture nuageuse`    &lt;dbl&gt; 82,280557, 57,209495, 32,469455, 14,619983, 1‚Ä¶\n$ `Vent en hauteur`        &lt;dbl&gt; 14,668663, 8,698154, 11,789413, 14,660355, 11‚Ä¶\n$ `Indice UV`              &lt;dbl&gt; 7,03602105, 10,09389904, 0,64470344, 5,463245‚Ä¶\n$ `Temp√©rature de l'eau`   &lt;dbl&gt; 33,22569, 28,26913, 28,00962, 29,71320, 28,66‚Ä¶\n$ `Humidit√© √† l‚Äôombre`     &lt;dbl&gt; 53,83643, 64,71406, 55,98915, 63,04538, 62,21‚Ä¶\n$ A√©rosols                 &lt;dbl&gt; 1,477347, 4,530642, 34,596087, 80,345317, 18,‚Ä¶\n$ `Cas palustres`          &lt;dbl&gt; 97, 62, 76, 66, 163, 67, 154, 168, 79, 58, 15‚Ä¶\n$ Dates                    &lt;date&gt; 2021-01-08, 2021-01-15, 2021-01-22, 2021-01-‚Ä¶\n\n\n¬†¬†¬†¬†¬†¬†On affiche ensuite un r√©sum√© statistique des variables dans le but de reperer certaines anomalies s‚Äôil y en a. Mais dans ce cas, il y‚Äôen a pas car j‚Äôai moi m√™me g√©n√©r√© les donn√©es et donc j‚Äôai veill√© √† ce qu‚Äôil n y ait pas de valeurs manquantes.\n\nlibrary(dplyr)\ndata %&gt;%\n  summary()\n\n    Semaine       R√©gion      Saison          Phase   Taux de couverture\n Min.   :  1,00   R1:260   Length:1040        0:516   Min.   :0,1001    \n 1st Qu.: 65,75   R2:260   Class :character   1:104   1st Qu.:0,2019    \n Median :130,50   R3:260   Mode  :character   2:208   Median :0,4023    \n Mean   :130,50   R4:260                      3:212   Mean   :0,4301    \n 3rd Qu.:195,25                                       3rd Qu.:0,6584    \n Max.   :260,00                                       Max.   :0,8998    \n  Temp√©rature       Humidit√©      Pluviom√©trie       Vitesse du vent\n Min.   :13,64   Min.   :35,41   Min.   :  0,04131   Min.   :2,001  \n 1st Qu.:23,97   1st Qu.:48,81   1st Qu.: 21,90016   1st Qu.:3,975  \n Median :27,09   Median :54,81   Median : 41,11740   Median :5,975  \n Mean   :27,08   Mean   :61,99   Mean   : 64,46301   Mean   :5,913  \n 3rd Qu.:30,21   3rd Qu.:78,54   3rd Qu.:106,59262   3rd Qu.:7,826  \n Max.   :40,19   Max.   :95,11   Max.   :199,59270   Max.   :9,998  \n Pression atmosph√©rique Indice de chaleur Couverture nuageuse Vent en hauteur \n Min.   : 980,4         Min.   : 8,405    Min.   : 0,0502     Min.   : 5,007  \n 1st Qu.:1006,5         1st Qu.:19,530    1st Qu.:25,7869     1st Qu.: 7,288  \n Median :1012,9         Median :22,434    Median :50,2530     Median : 9,831  \n Mean   :1012,9         Mean   :22,400    Mean   :50,5150     Mean   : 9,916  \n 3rd Qu.:1019,5         3rd Qu.:25,580    3rd Qu.:76,9193     3rd Qu.:12,654  \n Max.   :1041,6         Max.   :36,178    Max.   :99,9899     Max.   :14,995  \n   Indice UV         Temp√©rature de l'eau Humidit√© √† l‚Äôombre    A√©rosols       \n Min.   : 0,001759   Min.   :20,80        Min.   : 42,66     Min.   : 0,00251  \n 1st Qu.: 3,201972   1st Qu.:26,32        1st Qu.: 58,98     1st Qu.:23,49266  \n Median : 5,851642   Median :28,40        Median : 64,76     Median :47,47861  \n Mean   : 6,034127   Mean   :28,40        Mean   : 72,04     Mean   :48,53851  \n 3rd Qu.: 9,109514   3rd Qu.:30,54        3rd Qu.: 88,35     3rd Qu.:73,70956  \n Max.   :11,971607   Max.   :37,70        Max.   :104,38     Max.   :99,89207  \n Cas palustres       Dates           \n Min.   : 13,0   Min.   :2021-01-08  \n 1st Qu.: 58,0   1st Qu.:2022-04-06  \n Median : 93,0   Median :2023-07-03  \n Mean   :111,2   Mean   :2023-07-03  \n 3rd Qu.:145,2   3rd Qu.:2024-09-28  \n Max.   :466,0   Max.   :2025-12-26  \n\n\n¬†¬†¬†¬†¬†¬†Pour cette √©tude, la variable d‚Äôint√©r√™t est le nombre de nouveaux cas de paludisme enregistr√©s chaque semaine (t), avec des valeurs variant de 1 √† 260 dans les quatre r√©gions du pays."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#description-du-nombre-de-cas-pour-chaque-r√©gion",
    "href": "FORMATIONS/poisson_paludisme.html#description-du-nombre-de-cas-pour-chaque-r√©gion",
    "title": "Mod√©lisation des donn√©es de comptage",
    "section": "Description du nombre de cas pour chaque r√©gion",
    "text": "Description du nombre de cas pour chaque r√©gion\n\np1 &lt;- ggplot(data, aes(x = Dates, y = `Cas palustres`, color = R√©gion)) +\n  geom_line() +\n  facet_wrap(~R√©gion, scales = \"free_y\") +\n  labs(title = \"\", x = \"Ann√©e\", y = \"Nombre de cas\") +\n  geom_vline(xintercept = as.numeric(as.Date(\"2023-06-30\")), \n           linetype = \"dashed\", color = \"darkred\", size = 0.5) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2023-12-29\")), \n           linetype = \"dashed\", color = \"darkblue\", size = 0.5) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2024-12-24\")), \n           linetype = \"dashed\", color = \"royalblue\", size = 0.5) +\n  theme_light() +\n  scale_x_date(date_breaks = \"12 months\", date_labels = \"%b %Y\") +  \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np_interactif1 &lt;- ggplotly(p1)\n\np_interactif1\n\n\n\nFigure 1 : Evolution du nombre de cas de paludisme entre 2021 et 2025\n\n\n¬†¬†¬†¬†¬†¬†La courbe des s√©ries temporelles des cas de paludisme de 2021 √† 2025 pour les quatre r√©gions de l‚Äô√©tude montre une tendance g√©n√©rale √† la baisse, particuli√®rement marqu√©e apr√®s la mise en place des interventions. L‚Äôinterpretation reste quasi pareille pour toute les regions.\nLe test de Mann-Kendall confirme statistiquement cette tendance d√©croissante significative (p-value &lt; 0.05), avec une diminution notable observ√©e dans chaque r√©gion d√®s l‚Äôimpl√©mentation de la premi√®re phase du projet (figure @ref{fig:evolution}).\nPar ailleurs, le test de Kruskal-Wallis appliqu√© aux diff√©rentes phases du projet r√©v√®le une diff√©rence significative entre le nombre de cas observ√©s avant et apr√®s les interventions (p-value &lt; 0.05), sugg√©rant un impact positif des mesures mises en place.\nEnfin, le pic √©pid√©mique le plus √©lev√© a √©t√© observ√© en 2021 dans les r√©gions 1, 3 et 4, avec respectivement 392, 396 et 466 cas de paludisme enregistr√©s aux mois de septembre et octobre. Pour la r√©gion 2, le pic a √©t√© atteint en 2022, avec 384 cas observ√©s (figure @ref{fig:evolution})."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#mod√©lisation",
    "href": "FORMATIONS/poisson_paludisme.html#mod√©lisation",
    "title": "Mod√©lisation des donn√©es de comptage",
    "section": "Mod√©lisation",
    "text": "Mod√©lisation\n\nAnalyse de la corr√©lation entre les variables m√©t√©orologiques\n¬†¬†¬†¬†¬†¬†L‚Äôanalyse de la corr√©lation entre les variables montre des liens de corr√©lation relativement faibles. De plus, le calcul de l‚Äôindice de KMO, permettant de v√©rifier l‚Äôad√©quation des donn√©es √† l‚Äôanalyse en composantes principales, a montr√© une valeur de 0,5, confirmant le faible niveau de corr√©lation entre les covariables et ne justifiant ainsi pas la r√©alisation d‚Äôune ACP.\n\ndata_meteo &lt;- data[ , c(6:10 , 13)]\n\n##-- Calcul de la matrice de corr√©lation\ncor_matrix &lt;- cor(data_meteo, use = \"complete.obs\")\n\n##-- Transformation de la matrice de corr√©lation en format long pour ggplot2\ncor_melted &lt;- melt(cor_matrix)\ncor_melted$value &lt;- round(cor_melted$value , 2)\ncolnames(cor_melted)[3] &lt;- \"Coefficient de corr√©lation\"\n\n##-- Cr√©ation de la heatmap\ncor_plot &lt;- ggplot(cor_melted, aes(x = Var1, y = Var2, fill = `Coefficient de corr√©lation`)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +  \n##-- Bleu pour n√©gatif, rouge pour positif\n  theme_light() +\n  labs(x = \"Variables\",\n       y = \"Variables\") +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"),\n    axis.text.y = element_text(size = 10, face = \"bold\"),\n    axis.title.x = element_text(size = 10, face = \"bold\"),\n    axis.title.y = element_text(size = 10, face = \"bold\")\n  )\nggplotly(cor_plot)\n\n\n\nFigure 2 : Heatmap des Corr√©lations entre Variables M√©t√©orologiques\n\n\n\nlibrary(psych)\ndata_meteo &lt;- data[, 6:17]\nKMO(data_meteo)\n\nError in solve.default(r) : \n  system is computationally singular: reciprocal condition number = 2.76177e-18\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = data_meteo)\nOverall MSA =  0,5\nMSA for each item = \n           Temp√©rature               Humidit√©           Pluviom√©trie \n                   0,5                    0,5                    0,5 \n       Vitesse du vent Pression atmosph√©rique      Indice de chaleur \n                   0,5                    0,5                    0,5 \n   Couverture nuageuse        Vent en hauteur              Indice UV \n                   0,5                    0,5                    0,5 \n  Temp√©rature de l'eau     Humidit√© √† l‚Äôombre               A√©rosols \n                   0,5                    0,5                    0,5 \n\n\n\n\nMod√©lisation\n\nmodel &lt;- glm(`Cas palustres` ~ Temp√©rature + `Taux de couverture` + Humidit√© + Pluviom√©trie + `Vitesse du vent` + `Pression atmosph√©rique` + Phase, \n             data = data, family = poisson())\n\n#summary(model, exponentiate = TRUE)\n\n\ntbl_regression(model, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"R√©sultats de la r√©gression de Poisson : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 1 : R√©sultats de la r√©gression de Poisson : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Temp√©rature\n1,00\n1,00, 1,00\n0,002\n    Taux de couverture\n1,13\n1,07, 1,20\n&lt;0,001\n    Humidit√©\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviom√©trie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,97, 0,98\n&lt;0,001\n    Pression atmosph√©rique\n1,00\n1,00, 1,00\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n    ¬†¬†¬†¬†0\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†1\n0,41\n0,39, 0,42\n\n    ¬†¬†¬†¬†2\n0,45\n0,43, 0,46\n\n    ¬†¬†¬†¬†3\n0,49\n0,47, 0,51\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\n√âvaluation du mod√®le de Poisson\n\n¬†¬†¬†¬†¬†¬†Ici le stepAIC permet de fournir une s√©lection de variables qui am√©liore le mod√®le (crit√®re d‚ÄôAIC que j‚Äôaborderai dans une autre publication). L‚Äôobjectif est de minimiser l‚ÄôAIC, donc nous devons conserver les variables dont la suppression entra√Æne une forte augmentation de l‚ÄôAIC.\n\nD√©cision de suppression des variables\n\nLa s√©lection des variables repose sur leur impact sur l‚ÄôAIC (Akaike Information Criterion). Plus l‚ÄôAIC augmente apr√®s suppression d‚Äôune variable, plus cette derni√®re est importante pour le mod√®le. Les variables sont class√©es en deux groupes : celles √† conserver absolument et celles qui ont un impact mod√©r√©. La fonction stepAIC permet de faire automatiquement la s√©lection des variables importante dans le mod√®le.\n\nmod1_poisson &lt;- stepAIC(model) \n\nStart:  AIC=22725,07\n`Cas palustres` ~ Temp√©rature + `Taux de couverture` + Humidit√© + \n    Pluviom√©trie + `Vitesse du vent` + `Pression atmosph√©rique` + \n    Phase\n\n                           Df Deviance   AIC\n&lt;none&gt;                           16104 22725\n- Temp√©rature               1    16114 22733\n- `Taux de couverture`      1    16121 22740\n- `Pression atmosph√©rique`  1    16132 22751\n- `Vitesse du vent`         1    16525 23144\n- Humidit√©                  1    17231 23850\n- Pluviom√©trie              1    18145 24763\n- Phase                     3    19109 25723\n\n\n\n\nA conserver absolument\n\n\nCes variables entra√Ænent une forte augmentation de l‚ÄôAIC si elles sont supprim√©es, ce qui indique qu‚Äôelles contribuent de mani√®re significative √† l‚Äôexplication des cas palustres.\n\nPhase : +2998 d‚ÄôAIC\nPluviom√©trie : +2038 d‚ÄôAIC\nHumidit√© : +1125 d‚ÄôAIC\n\n\n\nVariables mod√©r√©ment importantes\n\n\nCes variables ont un impact plus faible sur l‚ÄôAIC et peuvent potentiellement √™tre supprim√©es sans alt√©rer significativement la qualit√© du mod√®le.\n\nVitesse du vent : +419 d‚ÄôAIC\nPression atmosph√©rique : +26 d‚ÄôAIC\nTaux de couverture : +15 d‚ÄôAIC\nTemp√©rature : +8 d‚ÄôAIC\n\n\n\nD√©cision\n\n\nLes variables Phase, Pluviom√©trie et Humidit√© doivent imp√©rativement √™tre conserv√©es, car leur suppression entra√Æne une augmentation tr√®s importante de l‚ÄôAIC. En revanche, Vitesse du vent, Pression atmosph√©rique, Taux de couverture et Temp√©rature ont un impact plus limit√© et peuvent √™tre envisag√©es pour la suppression si n√©cessaire.\n\ntbl_regression(model, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"R√©sultats de la r√©gression de Poisson suite au stepAIC : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 2 : R√©sultats de la r√©gression de Poisson suite au stepAIC : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Temp√©rature\n1,00\n1,00, 1,00\n0,002\n    Taux de couverture\n1,13\n1,07, 1,20\n&lt;0,001\n    Humidit√©\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviom√©trie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,97, 0,98\n&lt;0,001\n    Pression atmosph√©rique\n1,00\n1,00, 1,00\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n    ¬†¬†¬†¬†0\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†1\n0,41\n0,39, 0,42\n\n    ¬†¬†¬†¬†2\n0,45\n0,43, 0,46\n\n    ¬†¬†¬†¬†3\n0,49\n0,47, 0,51\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\npar(mfrow = c(2,2))\nplot(mod1_poisson)\n\n\n\n\nFigure 3 : Graphiques de diagnostic du mod√®le de poisson ajust√©\n\n\n\n\n¬†¬†¬†¬†¬†¬†L‚Äôanalyse des diagnostics du mod√®le montre que les r√©sidus de Pearson pr√©sentent une r√©partition al√©atoire des points autour de z√©ro, sugg√©rant l‚Äôabsence de structure particuli√®re dans les erreurs.\nDe plus, dans le graphique Q-Q, les points suivent approximativement la ligne diagonale, indiquant que les r√©sidus sont normalement distribu√©s, ce qui est un bon signe pour la validit√© des hypoth√®ses du mod√®le.\nLa structure des erreurs standard de Pearson montre √©galement une r√©partition √©quilibr√©e autour de la ligne rouge de r√©f√©rence, et un motif al√©atoire est observ√© au niveau des √©carts types de Pearson.\nTous ces √©l√©ments sugg√®rent une bonne ad√©quation du mod√®le aux donn√©es et confirment que les hypoth√®ses sous-jacentes sont raisonnablement respect√©es.\n\n\nAnalyse de la surdispersion dans un mod√®le de Poisson\n\n\n\nmod1_poisson %&gt;% \n  performance::check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =    14.991\n  Pearson's Chi-Squared = 15441.190\n                p-value =   &lt; 0.001\n\n\n¬†¬†¬†¬†¬†¬†Ce resultat sugg√®re qu‚Äôil y‚Äôa surdispersion dans les donn√©es (p-values &lt; 0,05). Dans ce cas plusieurs alternatives sont possibles. Nous avons entre autres le mod√®le de regression binomiale n√©gative qui est m√©lange de poisson-gamma et donc prend en compte un param√®tre qui est celui de la dispersion. On a √©galement le mod√®le quasi-poisson qui lui supprime la surdispersion pr√©sente dans les donn√©es √† l‚Äôinverse du mod√®le binomial n√©gatif qui l‚Äôestime.\n\n\nAlternative : Le modele binomial negative\n\n\n¬†¬†¬†¬†¬†¬†En alternative au mod√®le de Poisson en cas de surdispersion, le mod√®le binomial n√©gatif a √©t√© mentionn√© (Cameron and Trivedi 2013). En effet, ce mod√®le int√®gre un param√®tre suppl√©mentaire qui permet de mieux capturer la variabilit√© excessive des donn√©es, offrant ainsi une estimation plus fiable et adapt√©e aux situations o√π la variance des observations est sup√©rieure √† la moyenne.\n\nmodel_nb &lt;- glm.nb(`Cas palustres` ~ Temp√©rature + `Taux de couverture` + Humidit√© + Pluviom√©trie + `Vitesse du vent` + `Pression atmosph√©rique` + Phase, \n                   data = data)\n\nmodel_nb &lt;- stepAIC(model_nb)\n\nStart:  AIC=10479,97\n`Cas palustres` ~ Temp√©rature + `Taux de couverture` + Humidit√© + \n    Pluviom√©trie + `Vitesse du vent` + `Pression atmosph√©rique` + \n    Phase\n\n                           Df   AIC\n- Temp√©rature               1 10478\n- `Pression atmosph√©rique`  1 10478\n- `Taux de couverture`      1 10479\n&lt;none&gt;                        10480\n- `Vitesse du vent`         1 10505\n- Humidit√©                  1 10544\n- Pluviom√©trie              1 10568\n- Phase                     3 10662\n\nStep:  AIC=10478,09\n`Cas palustres` ~ `Taux de couverture` + Humidit√© + Pluviom√©trie + \n    `Vitesse du vent` + `Pression atmosph√©rique` + Phase\n\n                           Df   AIC\n- `Pression atmosph√©rique`  1 10476\n- `Taux de couverture`      1 10477\n&lt;none&gt;                        10478\n- `Vitesse du vent`         1 10503\n- Humidit√©                  1 10547\n- Pluviom√©trie              1 10566\n- Phase                     3 10660\n\nStep:  AIC=10476,24\n`Cas palustres` ~ `Taux de couverture` + Humidit√© + Pluviom√©trie + \n    `Vitesse du vent` + Phase\n\n                       Df   AIC\n- `Taux de couverture`  1 10476\n&lt;none&gt;                    10476\n- `Vitesse du vent`     1 10502\n- Humidit√©              1 10545\n- Pluviom√©trie          1 10564\n- Phase                 3 10658\n\nStep:  AIC=10475,49\n`Cas palustres` ~ Humidit√© + Pluviom√©trie + `Vitesse du vent` + \n    Phase\n\n                    Df   AIC\n&lt;none&gt;                 10476\n- `Vitesse du vent`  1 10501\n- Humidit√©           1 10544\n- Pluviom√©trie       1 10563\n- Phase              3 11113\n\n\n\ntbl_regression(model_nb, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"R√©sultats de la r√©gression de Binomial n√©gative : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 3 : R√©sultats de la r√©gression de Binomial n√©gative : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Humidit√©\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviom√©trie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,96, 0,98\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n    ¬†¬†¬†¬†0\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†1\n0,44\n0,40, 0,47\n\n    ¬†¬†¬†¬†2\n0,47\n0,45, 0,51\n\n    ¬†¬†¬†¬†3\n0,51\n0,48, 0,55\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\n\nInterpretation des r√©sultats\n\n\n\n\n\n\n\n\nIntervalles de confiances des variables m√©t√©orologiques\n\n\n\nLes intervalles de confiance des variables m√©t√©orologiques sont aussi petits car les donn√©es ont √©t√© g√©n√©r√©es. Et donc du coup avec de vraies donn√©es, il est possible de se retrouver avec des intervalles de confiance qui pourraient ne pas ressembler √† ceux-ci.\n\n\n\nL‚Äôhumidit√© augmente le nombre de cas de paludisme de 1% tandis que la vitesse de vent diminue le nombre de cas de paludisme de 3% (IC =[2% ; 4%]) toute chose √©tant √©gale par ailleur (l‚Äôinfluence des autres variables √©tant retir√©e).\nLa premi√®re phase d‚Äôinterventions a permis de reduire le nombre de cas de paludisme de 56% (IC = [53% ; 60%]) par rapport √† la phase 0 pendant laquelle il n‚Äôy avait pas encore d‚Äôintervention toute chose etant √©gale par ailleurs.\nLa seconde phase d‚Äôinterventions a permis de reduire le nombre de cas de paludisme de 53% (IC = [49% ; 55%]) par rapport √† la phase 0 pendant laquelle il n‚Äôy avait pas encore d‚Äôintervention toute chose etant √©gale par ailleurs.\nLa troisi√®me phase d‚Äôinterventions a permis de reduire le nombre de cas de paludisme de 49% (IC = [45% ; 52%]) par rapport √† la phase 0 pendant laquelle il n‚Äôy avait pas encore d‚Äôintervention toute chose etant √©gale par ailleurs."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#annexes",
    "href": "FORMATIONS/poisson_paludisme.html#annexes",
    "title": "Mod√©lisation des donn√©es de comptage",
    "section": "Annexes",
    "text": "Annexes\n\nDiagnostic du mod√®le binomial n√©gatif\n\n\nAnalyse des r√©sidus\n\n\n\npar(mfrow = c(2,2))\nplot(model_nb)\n\n\n\n\nFigure 4 : Graphiques de diagnostic du mod√®le binomial n√©gatif ajust√©\n\n\n\n\n\n\nMulticolin√©arit√© du mod√®le binomial n√©gatif\n\n\n\nplot(performance::check_collinearity(model_nb))\n\n\n\n\nFigure 5 : VIF du mod√®le binomial n√©gatif\n\n\n\n\n¬†¬†¬†¬†¬†¬†On remarque que toutes les variables ont un faible VIF &lt; 5. Cela sugg√®re qu‚Äôil n‚Äôy a pas de multicolin√©arit√© entre les variables utilis√©es dans le mod√®le.\n\n\nTest de Mann-Kendall\n¬†¬†¬†¬†¬†¬†Ce test a √©t√© utilis√© avec les alternatives unilat√©rales droite et gauche pour tester la pr√©sence de tendances strictement croissantes ou strictement d√©croissantes de la serie nombre de cas hebdomadire de paludisme dans chaque r√©gion d‚Äôetudes.\nHypoth√®ses du test\n\\[\n\\begin{cases}\nH_0 : \\text{La s√©rie ne pr√©sente pas de tendance monotone (croissante ou d√©croissante).} \\\\\nH_1 : \\text{La s√©rie pr√©sente une tendance monotone (croissante ou d√©croissante).}\n\\end{cases}\n\\] Interpr√©tation\n\nSi la p-value est inf√©rieure au seuil de signification choisi (g√©n√©ralement 0,05),\nalors il y a suffisamment de preuves pour conclure que la s√©rie (nombre de cas de paludisme\nou incidences cumul√©es durant une phase) pr√©sente une tendance monotone.\n\nDans le cas contraire, on conclut que la s√©rie ne pr√©sente aucune tendance significative.\n\n\n\nDescription du mod√®le de Poisson\n¬†¬†¬†¬†¬†¬†Soit (\\(Y\\)) le nombre de cas de paludisme hebdomadire Il s‚Äôagit d‚Äôune variable quantitative discr√®te prenant ses valeurs dans un intervalle d√©fini. Supposons en outre que ces √©v√©nements sont ind√©pendants, c‚Äôest-√†-dire que l‚Äôoccurrence d‚Äôun premier cas n‚Äôaffecte pas la probabilit√© d‚Äôen observer un autre.\nDans ce contexte, la variable (\\(Y\\)) suit une distribution de Poisson, avec un param√®tre () repr√©sentant le taux moyen d‚Äôapparition d‚Äôun cas de paludisme. La probabilit√© d‚Äôobserver une valeur donn√©e de (\\(Y\\)), en fonction de (), est exprim√©e par la formule suivante :\n\\[ P(Y = y) = \\frac{\\lambda^y}{y!} e^{-\\lambda} \\]\nLa distribution de Poisson n‚Äôa qu‚Äôun param√®tre: () correspond √† la fois √† sa moyenne et √† sa variance.\n\\[E(\\lambda) = V(\\lambda)\\] Le mod√®le de Poisson a √©t√© utilis√© pour identifier les facteurs associ√©s √† la survenue du cas de paludisme, principalement en raison de la nature discr√®te de notre variable d√©pendante.\nLa r√©gression de Poisson s‚Äôinscrit dans le cadre des mod√®les lin√©aires g√©n√©ralis√©s, o√π la variable r√©ponse (\\(Y\\)) suit une distribution de Poisson :\n\\[ y \\sim \\text{Poisson}(\\lambda) \\]\nPuisque () doit √™tre un nombre positif, nous utiliserons la fonction de logarithme comme lien avec le pr√©dicteur lin√©aire.\n\\[ \\log{\\lambda} = \\eta = \\beta_0 + \\sum_{i = 1}^m \\beta_i x_i \\]\n\n\nEstimation des parametres\nL‚Äôestimation des param√®tres d‚Äôun mod√®le de Poisson repose sur la m√©thode du maximum de vraisemblance (MV). Voici les √©tapes essentielles de l‚Äôestimation :\n. Fonction de Vraisemblance\nLa fonction de vraisemblance pour (n) observations est donn√©e par :\n\\[L(\\beta) = \\prod_{i=1}^{n} \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!}\\]\nEn prenant le logarithme, on obtient la log-vraisemblance :\n\\[\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(\\lambda_i) - \\lambda_i - \\log(y_i!) \\right]\\]\nEn rempla√ßant ( _i ) par ( e^{X_i } ), on obtient :\n\\[\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i (X_i \\beta) - e^{X_i \\beta} - \\log(y_i!) \\right]\\]\nEstimation par Maximum de Vraisemblance\nL‚Äôestimation des param√®tres ( ) se fait en maximisant la log-vraisemblance. Comme il n‚Äôexiste pas de solution analytique simple, on utilise des m√©thodes num√©riques telles que l‚Äôalgorithme de Newton-Raphson ou la descente de gradient.\n\n\nAnalyse de la presence de surdispersion dans les donn√©es\nTel que mentionn√© plus haut, l‚Äôind√©pendance des observations est un pr√©requis du mod√®le de Poisson. Sa non-v√©rification peut entra√Æner une surdispersion des donn√©es. Cette surdispersion est quantifi√©e par un param√®tre ( ) qui multiplie la variance attendue : pour une moyenne ( ), la variance devient donc ( ).\nPlus rarement, il peut arriver que ( &lt; 1 ), ce qui correspond √† une sous-dispersion des observations. Contrairement √† la surdispersion, o√π les observations ont tendance √† √™tre regroup√©es, la sous-dispersion traduit une r√©partition plus r√©guli√®re que pr√©vu.\nAfin de s‚Äôassurer de la pertinence du mod√®le choisi, une analyse de la surdispersion a √©t√© r√©alis√©e √† l‚Äôaide du **test de surdispersion*. Les hypoth√®ses du test √©taient les suivantes :\n\nHypoth√®se nulle ((H_0)) : absence de surdispersion (le mod√®le de Poisson est appropri√©).\nHypoth√®se alternative ((H_1)) : pr√©sence de surdispersion (le mod√®le de Poisson n‚Äôest pas adapt√©).\n\nCrit√®re de d√©cision : Une p-value inf√©rieure √† 0,05 conduit au rejet de ( H_0), indiquant la pr√©sence d‚Äôune surdispersion et la n√©cessit√© d‚Äôenvisager un mod√®le alternatif (comme le quasi-Poisson ou le Poisson n√©gatif)."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#r√©f√©rence",
    "href": "FORMATIONS/poisson_paludisme.html#r√©f√©rence",
    "title": "Mod√©lisation des donn√©es de comptage",
    "section": "R√©f√©rence",
    "text": "R√©f√©rence\n\n\nCameron, A. Colin, and Pravin K. Trivedi. 2013. Regression Analysis of Count Data. 2nd ed. Econometric Society Monographs. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9781139013567.\n\n\nMondiale de la Sant√©), OMS (Organisation. 2023. ‚ÄúWorld Malaria-report 2023-briefing-kit-fre.pdf.‚Äù https://cdn.who.int/media/docs/default-source/malaria/world-malaria-reports/wmr2022-regional-briefing-kit-fre.pdf?sfvrsn=7cb400ed_6&download=true.\n\n\n(OMS), Organisation MOndiale de la sant√©. 2023. ‚ÄúGlobal technical strategy for malaria 2016-2030.‚Äù https://iris.who.int/handle/10665/176712."
  },
  {
    "objectID": "FORMATIONS/presentations.html",
    "href": "FORMATIONS/presentations.html",
    "title": "Comment faire une pr√©sentation avec R et Quarto",
    "section": "",
    "text": "packages &lt;- c(\"ggplot2\",\"haven\", \"gtsummary\", \"corrr\", \"MASS\",\n              \"dplyr\",\"haven\", \"rstatix\", \"tidyverse\", \"ggpubr\",\n              \"glue\", \"dplyr\",\"ggspatial\", \"ggrepel\",\n              \"readxl\", \"stringr\", \"colorspace\") \n            \nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "FORMATIONS/presentations.html#faire-ses-pr√©sentations-directement-avec-r-et-rstudio",
    "href": "FORMATIONS/presentations.html#faire-ses-pr√©sentations-directement-avec-r-et-rstudio",
    "title": "Comment faire une pr√©sentation avec R et Quarto",
    "section": "Faire ses pr√©sentations directement avec R et Rstudio",
    "text": "Faire ses pr√©sentations directement avec R et Rstudio\n\nPourquoi utiliser R et Rstudio pour ses pr√©sentations ?\n\n¬†¬†¬†¬†¬†¬†R et RStudio offrent des outils puissants pour cr√©er des pr√©sentations dynamiques, reproductibles et int√©gr√©es √† vos analyses de donn√©es. Voici quelques raisons :\n\nInt√©gration parfaite des analyses et des pr√©sentations :\n\nNous pouvons combiner code, graphiques, tableaux et explications textuelles dans un seul document. Cela garantit une reproductibilit√© totale : les r√©sultats sont automatiquement mis √† jour si vos donn√©es changent.\n\nFlexibilit√© avec RMarkdown :\n\nCr√©ez des pr√©sentations dans divers formats : HTML (slidy, reveal.js), PDF (Beamer), ou powerpoint ppt. Les formats sont hautement personnalisables pour r√©pondre √† vos besoins esth√©tiques et fonctionnels.\n\nSimplification du travail collaboratif :\n\nIl y‚Äôa une possibilit√© de garder un fichier .tex pour ceux qui sont √† l‚Äôaise avec latex.\n\n\nMaintenant allons-y !!!\n\n\n\n\n\nCommen√ßons par une pr√©sentation revaljs\n\n\n\n\nInstaller les packages n√©cessaires\n\nAssurez-vous d‚Äôavoir le package revealjs install√©. Si ce n‚Äôest pas le cas, installez-le avec :\ninstall.packages(\"revealjs\")\n\nCr√©er un fichier RMarkdown pour une pr√©sentation\n\nCr√©er un nouveau fichier RMarkdown :\n\nAllez dans : File &gt; New File &gt; Quarto presentation\nDans la fen√™tre qui s‚Äôouvre : Entrez un titre et un auteur. Dans l‚Äôoption Default Output Format, choisissez From Template &gt; Revealjs Presentation.\n\n\nChanger l‚Äôen-t√™te YAML\n\nEn image voici, un descriptif visuel des 04 petites √©tapes pour la cr√©ation du fichier avec des images :\n\n\n\n\n\n\nEtape 1\n\n\n\n\n\n\n\nEtape 2\n\n\n\n\n\n\n\n\n\nEtape 3\n\n\n\n\n\n\n\nEtape 4\n\n\n\n\n\n\n\n\n\nExplication de l‚Äôen-t√™te YAML"
  },
  {
    "objectID": "FORMATIONS/presentations.html#informations-g√©n√©rales",
    "href": "FORMATIONS/presentations.html#informations-g√©n√©rales",
    "title": "Comment faire une pr√©sentation avec R et Quarto",
    "section": "Informations g√©n√©rales",
    "text": "Informations g√©n√©rales\n\ntitle : Titre principal de la pr√©sentation\n\nIci : ‚ÄúANALYSE EXPLORATOIRE DES DONNEES MTCARS‚Äù. C‚Äôest ce qui s‚Äôaffiche en haut de la premi√®re diapositive.\n\nauthor : Nom(s) des pr√©sentateur(s)\n\nIci : ‚ÄúPresented by Djamal Toe‚Äù.\n\ninstitute : Institution ou organisation associ√©e\n\nIci : ‚ÄúNational School for Statistic and Data Analysis‚Äù.\n-date : Date de la pr√©sentation\nIci, elle est g√©n√©r√©e dynamiquement avec : 2025-05-28. Cela affichera automatiquement la date du jour o√π le fichier est tricot√©."
  },
  {
    "objectID": "FORMATIONS/presentations.html#format-et-personnalisation-reveal.js",
    "href": "FORMATIONS/presentations.html#format-et-personnalisation-reveal.js",
    "title": "Comment faire une pr√©sentation avec R et Quarto",
    "section": "Format et personnalisation (reveal.js)",
    "text": "Format et personnalisation (reveal.js)\nLa section format: revealjs: contient des options sp√©cifiques √† la biblioth√®que reveal.js, permettant de personnaliser la pr√©sentation.\n\nVitesse de transition:  transition-speed: fast d√©finit la vitesse des transitions entre les diapositives. Options possibles : slow, normal, fast.\nAspect ratio :  aspect_ratio: \"16:9\" sp√©cifie le ratio largeur/hauteur des diapositives. Le ratio ‚Äú16:9‚Äù est id√©al pour les √©crans modernes (√©cran large). Autres options possibles : ‚Äú4:3‚Äù, ‚Äú3:2‚Äù, etc.\nMarges : margin: 0.02 d√©finit l‚Äôespace vide autour du contenu de chaque diapositive. Une valeur faible (comme 0.02) maximise l‚Äôespace utilis√© sur chaque diapositive.\nCentrage : center: true permet de Centrer le contenu verticalement et horizontalement sur chaque diapositive.\nPied de page : footer: ‚ÄúEnglish classes with Milonnet‚Äù : Ajoute un texte en bas de chaque diapositive, comme une signature ou une note de contexte.\nLogo : logo: \"logo_ensai.png\" affiche un logo en haut √† droite de chaque diapositive. L‚Äôimage doit √™tre plac√©e dans le r√©pertoire sp√©cifi√© ou un chemin relatif correct doit √™tre utilis√©.\nCSS personnalis√© : css: style.css permet d‚Äôutiliser un fichier CSS externe pour personnaliser les styles. Exemple : changer les polices, couleurs, tailles, etc. Le fichier style.css doit √™tre dans le m√™me r√©pertoire ou le chemin appropri√© doit √™tre indiqu√©.\nGestion des figure : fig_caption: yes active l‚Äôaffichage des l√©gendes sous les graphiques ins√©r√©s.\nTable des mati√®res (ToC) : toc: true active l‚Äôaffichage d‚Äôune table des mati√®res, toc-expand: false exige que les sections de la table des mati√®res ne soient pas d√©velopp√©es par d√©faut, toc-depth: 1 d√©finit la profondeur de la hi√©rarchie affich√©e dans la table des mati√®res (seulement les titres principaux #)."
  },
  {
    "objectID": "FORMATIONS/presentations.html#pr√©visualition",
    "href": "FORMATIONS/presentations.html#pr√©visualition",
    "title": "Comment faire une pr√©sentation avec R et Quarto",
    "section": "Pr√©visualition",
    "text": "Pr√©visualition\n¬†¬†¬†¬†¬†¬†Pendant que vous faites la pr√©sentations sur Rstudio, vous pouvez la pr√©sualiser. Regardez les images ci-apr√®s :\n\n\n\n\n\n\nPrevisualisation : etape 1\n\n\n\n\n\n\n\nPrevisualisation : etape 2\n\n\n\n\n\n\n\n\n\nCompilation et Previsualisation : etape 3\n\n\n\n\n\n\n\n\n\n\n\nViewer ou Presenation ?\n\n\n\nA l‚Äô√©tape 2 de la pr√©visualisation, il se peut que la pr√©visualisation apparaisse dans la partie Presentation juste √† droite de l‚Äôonglet Viewer encercl√© en rouge sur l‚Äôimage."
  },
  {
    "objectID": "FORMATIONS/presentations.html#mise-en-forme-avec-le-fichier-css",
    "href": "FORMATIONS/presentations.html#mise-en-forme-avec-le-fichier-css",
    "title": "Comment faire une pr√©sentation avec R et Quarto",
    "section": "Mise en forme avec le fichier CSS",
    "text": "Mise en forme avec le fichier CSS\n¬†¬†¬†¬†¬†¬†Pour cette section ne vous inquietez pas si vous n‚Äôavez pas de connaissance en html ou en css, nous utiliserons juste un code css pour la mise en forme du titre."
  },
  {
    "objectID": "FORMATIONS/presentations.html#t√©l√©charger-le-fichier-de-la-pr√©sentation",
    "href": "FORMATIONS/presentations.html#t√©l√©charger-le-fichier-de-la-pr√©sentation",
    "title": "Comment faire une pr√©sentation avec R et Quarto",
    "section": "T√©l√©charger le fichier de la pr√©sentation",
    "text": "T√©l√©charger le fichier de la pr√©sentation\nAvant de t√©l√©charger le fichier, vous pouvez voir ce qu‚Äôil donne en cliquant sur ce lien\nVous pouvez t√©l√©charger le fichier d‚Äôanalyse exploratoire des donn√©es mtcars au format .qmd ci-dessous.\nT√©l√©charger le fichier .qmd\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour √† la page d‚Äôaccueuil"
  },
  {
    "objectID": "FORMATIONS/SIG.html",
    "href": "FORMATIONS/SIG.html",
    "title": "Cr√©ation des cartes chlorop√®tres avec R",
    "section": "",
    "text": "packages &lt;- c(\"ggplot2\",\"haven\", \"gtsummary\", \"corrr\", \"MASS\",\n              \"dplyr\",\"haven\", \"rstatix\", \"tidyverse\", \"ggpubr\",\n              \"glue\", \"dplyr\",\"ggspatial\", \"ggrepel\",\"marmap\", \n              \"readxl\", \"stringr\", \"colorspace\", \"sf\", \"viridis\",\n              \"tools\",\"ggspatial\",\"readxl\",\"openxlsx\",\"grid\",\n              \"outliers\",\"car\",\"ftExtra\",\"tibble\",\n              \"gtsummary\", \"wesanderson\", \"viridis\",\n              \"RColorBrewer\", \"knitr\", \"kableExtra\") \n            \nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "FORMATIONS/SIG.html#comment-faire-des-cartes-choropl√®thes-et-des-cartes-de-proportions-avec-r",
    "href": "FORMATIONS/SIG.html#comment-faire-des-cartes-choropl√®thes-et-des-cartes-de-proportions-avec-r",
    "title": "Cr√©ation des cartes chlorop√®tres avec R",
    "section": "Comment faire des cartes Choropl√®thes et des cartes de proportions avec R ?",
    "text": "Comment faire des cartes Choropl√®thes et des cartes de proportions avec R ?\n¬†¬†¬†¬†¬†¬†Les cartes choropl√®thes et les cartes de proportions sont des outils puissants pour visualiser des donn√©es g√©ospatiales dans R. Ces cartes permettent de repr√©senter des valeurs quantitatives (par exemple, des taux de population, des moyennes) sur des zones g√©ographiques, souvent des r√©gions administratives comme des d√©partements, des communes, ou des zones g√©ographiques personnalis√©es.\n\nIntroduction aux Cartes Choropl√®thes et Cartes de Proportions\n\nLes cartes choropl√®thes colorient les r√©gions g√©ographiques en fonction de valeurs num√©riques ou de proportions, facilitant l‚Äôanalyse spatiale et la compr√©hension des variations g√©ographiques. Elles sont couramment utilis√©es pour des donn√©es socio-√©conomiques, de sant√© publique, ou des analyses environnementales.\nLes cartes de proportions sont similaires mais mettent davantage l‚Äôaccent sur les ratios ou proportions par rapport √† une valeur totale, comme des pourcentages ou des fractions de populations.\n\nNotions de Base : Polygones, Shapefiles et Coordonn√©es Avant de cr√©er ces cartes, il est important de comprendre quelques notions de base, comme les polygones et les shapefiles :\n\n\n\n\n\n\n\nPolygones\n\n\n\nUne zone g√©ographique est souvent repr√©sent√©e par un polygone, une forme g√©om√©trique ferm√©e qui peut avoir plusieurs c√¥t√©s. Par exemple, une commune ou un d√©partement sur une carte peut √™tre repr√©sent√©e comme un polygone.\n\n\n\n\n\n\n\n\nShapefiles\n\n\n\nCe sont un format de fichier standard pour stocker des informations g√©ospatiales, y compris les coordonn√©es de points, de lignes et de polygones. Ils peuvent contenir les g√©om√©tries des entit√©s g√©ographiques ainsi que leurs attributs (valeurs associ√©es √† chaque r√©gion, comme le revenu moyen ou le taux de ch√¥mage).\n\n\n\n\n\n\n\n\nCoordonn√©es g√©ographiques\n\n\n\nLes coordonn√©es (latitude et longitude) permettent de positionner ces polygones sur une carte. En R, on utilise des syst√®mes de coordonn√©es g√©ographiques et projet√©es pour g√©rer et visualiser ces donn√©es.\n\n\nPlusieurs pakages permettent de visualiser les donn√©es avec les cartes, ici nous interessons aux packages glue et sf.\n\nZone d‚Äô√©tude\n\nSupposons que nous menions une √©tude au Burkina-Faso. Par exemple, nous m√©surer des indicateurs tels que le taux de mortalit√©, la couverture sanitaire etc ‚Ä¶ Le Burkina Faso est un pays qui compte 13 regions, mais notre etude s‚Äô√©tend seulement sur 8 regions. Il convient de montrer toutes les regions, puis de mettre en ex√®gue celles qui nous concernent.\n\nPlace au code\n\n\n\nvoir/cacher le code\n\n\n###---- Chargement des shapefiles src = GADM\nroot &lt;- getwd() ##-- la racine du repertoire\n\n##- La carte du pays sans les polygones des regions, communes et/ou departements\npath0 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_0.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath1 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_1.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath2 &lt;- paste0(\"/DATA_SIG/BFA2/gadm41_BFA_2.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath3 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_3.shp\")\n\n\n##-- selection des regions concern√©es\n\nstudy.area &lt;-  c(\"Boucle du Mouhoun\", \"Centre-Est\", \"Centre-Nord\",\n             \"Centre-Ouest\", \"Nord\", \"Sud-Ouest\",\n             \"Haut-Bassins\", \"Cascades\")\n\n##-- lecture des shapefiles\npays_shp &lt;- read_sf(glue(path0), quiet = T)\nregion_shp &lt;- read_sf(glue(path1), quiet = T)\n#commune_shp &lt;- read_sf(glue(path2), quiet = T)\n#province_shp &lt;- read_sf(glue(path3), quiet = T)\n\n##-- cr√©ation d'une sous base avec les polygones des regions s√©lectionn√©s\n\ndata_region &lt;- region_shp %&gt;% filter(NAME_1 %in% study.area)\n\n\n##-- Study area colors\nstudy_zone_colors &lt;- c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\",\n                       \"#3FE1B8\", \"#9467bd\", \"#8c564b\",\n                       \"#00008B\", \"#4B0082\")\n\nstudy_zone_map &lt;- ggplot() +\n  geom_sf(data = pays_shp, aes(linewidth = \"Burkina Faso\"),fill = \"white\", color = \"black\") +\n  geom_sf(data = region_shp, aes(fill = ifelse(\n    NAME_1 %in% study.area,\n    \"Regions d'√©tudes\",\n    \"Autres regions\"\n  ) )) +\n  geom_sf_text(data = region_shp, aes(label = ifelse(\n    NAME_1 %in% study.area,\n    study.area,\n    \"\"\n  )), size = 4)+\n  ggspatial::annotation_scale(\n    location = \"br\",\n    bar_cols = c(\"black\", \"white\")\n  )  +\n  theme_light()+\n  ggspatial::annotation_north_arrow(\n    location = \"tr\", which_north = \"true\",\n    pad_x = unit(0.05, \"in\"), pad_y = unit(0.05, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill = c(\"black\", \"white\"),\n      line_col = \"black\"\n    )\n  )+\n  xlab(\"\")+\n  ylab(\"\")+\n  scale_linewidth_manual(values = c(1.2), name = \"\")+\n  scale_fill_manual(values = c(\"white\",\"#1f77b4\"), name=\"Zone d'√©tude\")+\n  theme_light() + \n  guides(\n    linewidth = guide_legend(order = 1),\n    fill = guide_legend(order = 2),\n    color = guide_legend(order = 3)\n  )\n\n\n\nstudy_zone_map\n\n\n\n\nCartographie de la zone d‚Äô√©tude\n\n\n\n\n\nExpliquons le code √† pr√©sent\n\n\nCharger les fichier shapefiles :\n\nglue : pour preparer la structure du format (optionnel)\nreadsf : pour lire les fichiers shapefiles\n\nDefinir la zone d‚Äô√©tude : les fichier shapefile devient comme un dataframe, donc est manipulable au m√™me titre que les fichiers excel, csv etc ‚Ä¶\nOn trace d‚Äôabord la carte du pays, ensuite on ajoute la couche des regions (c‚Äôest-√†-dire le shapefile des regions). On pourrait le faire simplement avec le shapefile des regions sans celui du pays.\nEnsuite on ajoute la couleur pour la zone concern√©e et les noms des regions s√©lectionn√©es avec geom_sf_text\nannotation_scale permet d‚Äôajouter une barre d‚Äô√©chelle (scale bar) √† une carte avec la position br pour dire bottom rigth (en bas √† droite)\nannotation_north_arrow est utilis√©e pour ajouter une fl√®che du nord sur une carte cr√©√©e avec ggplot2\nPour le reste il s‚Äôagit des fonctions qu‚Äôon utilise couramment avec ggplot2\n\n\n\nAfficher/Masquer le tableau\n\n\n\n\n\nTableau 1 : Les 10 premi√®res lignes du shapefile\n\n\nGID_1\nGID_0\nCOUNTRY\nNAME_1\nVARNAME_1\nNL_NAME_1\nTYPE_1\nENGTYPE_1\nCC_1\nHASC_1\nISO_1\ngeometry\n\n\n\n\nBFA.1_1\nBFA\nBurkina Faso\nBoucle du Mouhoun\nNA\nNA\nR√©gion\nRegion\nNA\nBF.BO\nNA\nPOLYGON ((-2,73901 11,71249...\n\n\nBFA.2_1\nBFA\nBurkina Faso\nCascades\nNA\nNA\nR√©gion\nRegion\nNA\nBF.CD\nNA\nPOLYGON ((-4,591742 9,70225...\n\n\nBFA.7_1\nBFA\nBurkina Faso\nCentre\nNA\nNA\nR√©gion\nRegion\nNA\nBF.CT\nNA\nPOLYGON ((-1,2786 12,13921,...\n\n\nBFA.3_1\nBFA\nBurkina Faso\nCentre-Est\nNA\nNA\nR√©gion\nRegion\nNA\nBF.CE\nNA\nPOLYGON ((0,4371 11,67655, ...\n\n\nBFA.4_1\nBFA\nBurkina Faso\nCentre-Nord\nNA\nNA\nR√©gion\nRegion\nNA\nBF.CN\nNA\nPOLYGON ((-0,7773 12,66989,...\n\n\nBFA.5_1\nBFA\nBurkina Faso\nCentre-Ouest\nNA\nNA\nR√©gion\nRegion\nNA\nBF.CO\nNA\nPOLYGON ((-2,360162 11,0081...\n\n\nBFA.6_1\nBFA\nBurkina Faso\nCentre-Sud\nNA\nNA\nR√©gion\nRegion\nNA\nBF.CS\nNA\nPOLYGON ((-0,8624911 10,985...\n\n\nBFA.8_1\nBFA\nBurkina Faso\nEst\nNA\nNA\nR√©gion\nRegion\nNA\nBF.ES\nNA\nPOLYGON ((1,384436 11,44223...\n\n\nBFA.9_1\nBFA\nBurkina Faso\nHaut-Bassins\nNA\nNA\nR√©gion\nRegion\nNA\nBF.HB\nNA\nPOLYGON ((-4,08994 10,79044...\n\n\nBFA.10_1\nBFA\nBurkina Faso\nNord\nNA\nNA\nR√©gion\nRegion\nNA\nBF.NO\nNA\nPOLYGON ((-1,96586 12,67774...\n\n\n\na Source des donn√©es : GADM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCartes choropl√®thes\n\n¬†¬†¬†¬†¬†¬†Les cartes choropl√®thes sont des repr√©sentations graphiques qui utilisent des nuances de couleurs pour illustrer des donn√©es quantitatives ou qualitatives sur des zones g√©ographiques. Chaque zone est remplie d‚Äôune couleur qui correspond √† une valeur sp√©cifique ou √† une plage de valeurs, facilitant ainsi l‚Äôanalyse des variations spatiales des donn√©es.\nLes cartes choropl√®thes sont id√©ales pour repr√©senter des indicateurs comme le taux de mortalit√©, le revenu moyen, l‚Äôacc√®s √† l‚Äôeau potable, ou encore la couverture sanitaire par r√©gion.\n\nExemple de carte choropl√®the\nDans cet exemple, nous allons cr√©er une carte choropl√®the montrant la couverture sanitaire par r√©gion au Burkina Faso, en utilisant les donn√©es fictives cr√©√©es plus haut. pour les donn√©es, vous pouvez me contacter par email.\n\nEtape 1 : Charger les shapefiles et les donn√©es\n\nIci nous nous assurons que les shapefiles des r√©gions et les donn√©es sont correctement charg√©s et li√©s entre eux. Pour cela on fait une jointure externe.\n\n##-- Joindre les donn√©es au shapefile\nregion_data &lt;- region_shp %&gt;% \n  left_join(data, by = c(\"NAME_1\" = \"Region\"))\n\nAvant de passer √† l‚Äô√©tape 2, affichons les donn√©es g√©n√©r√©es avant jointure et ceux apr√©s jointures.\n\n\nAfficher/cacher le code\n\n\ntbl.avant.jointure &lt;- kbl(head(data,10)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\ntbl.apres.jointure &lt;- kbl(head(data,10)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des donn√©es :  GADM\")\n\n\n\n\nAfficher/Masquer le tableau\n\n\ntbl.avant.jointure\ntbl.apres.jointure\n\n\nLes 10 premi√®res lignes des tables\n\n\n\n\n\nAvant jointure\n\n\nRegion\nPopulation\nTaux_Mortalite\nCouverture_Sanitaire\nAcces_Eau_Potable\n\n\n\n\nBoucle du Mouhoun\n2094847\n13,98\n66,23\n56,72\n\n\nCascades\n1416250\n6,37\n68,66\n81,72\n\n\nCentre\n386906\n13,59\n85,61\n65,10\n\n\nCentre-Est\n2172354\n6,46\n60,31\n65,95\n\n\nCentre-Nord\n1875435\n7,31\n71,90\n86,77\n\n\nCentre-Ouest\n2350109\n9,51\n63,70\n59,19\n\n\nCentre-Sud\n2406323\n9,01\n82,16\n85,41\n\n\nEst\n1438897\n14,47\n82,98\n77,06\n\n\nHauts-Bassins\n1446675\n13,49\n68,80\n68,16\n\n\nNord\n1466191\n14,61\n62,99\n74,00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr√®s jointure\n\n\nRegion\nPopulation\nTaux_Mortalite\nCouverture_Sanitaire\nAcces_Eau_Potable\n\n\n\n\nBoucle du Mouhoun\n2094847\n13,98\n66,23\n56,72\n\n\nCascades\n1416250\n6,37\n68,66\n81,72\n\n\nCentre\n386906\n13,59\n85,61\n65,10\n\n\nCentre-Est\n2172354\n6,46\n60,31\n65,95\n\n\nCentre-Nord\n1875435\n7,31\n71,90\n86,77\n\n\nCentre-Ouest\n2350109\n9,51\n63,70\n59,19\n\n\nCentre-Sud\n2406323\n9,01\n82,16\n85,41\n\n\nEst\n1438897\n14,47\n82,98\n77,06\n\n\nHauts-Bassins\n1446675\n13,49\n68,80\n68,16\n\n\nNord\n1466191\n14,61\n62,99\n74,00\n\n\n\na Source des donn√©es : GADM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEtape 2 : Cr√©er la carte choropl√®the\n\nUtilisez ggplot2 et geom_sf() pour afficher les r√©gions et les colorer en fonction de la couverture sanitaire.\n\n##-  Carte choropl√®the\nchoropleth_map &lt;- ggplot(region_data) +\n  geom_sf(aes(fill = Couverture_Sanitaire), color = \"black\") +\n  scale_fill_viridis_c(\n    option = \"C\",\n    name = \"Couverture Sanitaire (%)\"\n  ) +\n  ggtitle(\"Carte choropl√®the : Couverture sanitaire par r√©gion\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5, face = \"bold\")\n  )\n\nchoropleth_map\n\n\n\n\nCouverture sanitaire par r√©gion\n\n\n\n\n\nEtape 3 :  Ajouter des √©l√©ments d√©coratifs\n\nAjoutons une barre d‚Äô√©chelle et une fl√®che du nord pour rendre la carte plus informative.\n\n##- Ajout des √©l√©ments d√©coratifs\nchoropleth_map &lt;- choropleth_map +\n  ggspatial::annotation_scale(location = \"br\") +\n  ggspatial::annotation_north_arrow(\n    location = \"tl\", style = north_arrow_nautical()\n  ) ###-- tl pour top-left (en haut √† gauche)\n\nchoropleth_map\n\n\n\n\n\nInterpr√©ter les r√©sultats\n\nExaminez la carte g√©n√©r√©e et r√©pondez aux questions suivantes : - Quelles r√©gions ont la meilleure couverture sanitaire ? - Quelles r√©gions doivent faire l‚Äôobjet d‚Äôune attention particuli√®re pour am√©liorer les conditions de vie ?\n\nExtensions possibles\n\nR√©alisez une carte choropl√®the pour le taux de mortalit√©.\nAjoutez des annotations pour les r√©gions ayant les valeurs extr√™mes.\nExp√©rimentez avec d‚Äôautres palettes de couleurs en utilisant scale_fill_brewer() ou scale_fill_manual() etc ‚Ä¶.\n\n\n\n\n\n\n\nDonn√©es discr√®tes ?\n\n\n\nIl se peut qu‚Äôil n‚Äôy ait pas une variabilit√© importante dans les donn√©es dans ce cas, au lieu d‚Äôavoir une palette, nous aurons juste des cases de couleurs comme s‚Äôagissait d‚Äôun indicateur discr√®t. Dans ce cas, recoder juste cet indicateur en un indicateur qualitatif (regrouper par classe) et ensuite utiliser scale_fill_manual() pour definir vos couleurs manuellement ou laisser R le faire tout seul. Le graphique ci-dessous en est un exemple.\n\n\n[Exemple de carte avec un indicateur recod√© : Indisponible pour l‚Äôinstant]\n\nCartes de proportions\n\n\n\n\nA suivre\n\n\n\n\nCartes de proportions avanc√©es\n\n\n\n\n\n\nRetour √† la page d‚Äôaccueuil"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "",
    "text": "Bienvenue sur le site Djamaldbz, d√©di√© √† mes travaux en statistiques et informatique. Explorez mes projets, mes recherches, et mes publications."
  },
  {
    "objectID": "index.html#qui-suis-je",
    "href": "index.html#qui-suis-je",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Qui suis-je ?",
    "text": "Qui suis-je ?\nJe m‚Äôappelle Djamal Y. TOE, et je suis passionn√© par les statistiques et l‚Äôinformatique. Mon parcours m‚Äôa permis de me concentrer sur l‚Äôanalyse des donn√©es et la r√©solution de probl√®mes avec des m√©thodes quantitatives. Je suis toujours en apprentissage et cherche √† d√©couvrir de nouvelles approches. Mon objectif est d‚Äôapporter des solutions pratiques, tout en restant ouvert √† l‚Äôapprentissage continu et √† l‚Äôam√©lioration dans le domaine des statistiques et de la science des donn√©es.\n\nEn savoir plus sur moi ici"
  },
  {
    "objectID": "index.html#mon-approche",
    "href": "index.html#mon-approche",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Mon approche",
    "text": "Mon approche\nJ‚Äôaime explorer les statistiques, la programmation, l‚Äôanalyse de donn√©es et le machine learning pour mieux comprendre des probl√©matiques et proposer des solutions adapt√©es. Je m‚Äôint√©resse particuli√®rement aux domaines suivants :\n\nAnalyses factorielles et visualisation\nMod√®les de r√©gression et pr√©visions\nApplications statistiques interactives\nD√©veloppement d‚Äôapplications\nVision par ordinateur"
  },
  {
    "objectID": "index.html#mes-projets-r√©cents",
    "href": "index.html#mes-projets-r√©cents",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Mes projets r√©cents",
    "text": "Mes projets r√©cents\nVoici quelques-uns de mes projets r√©cents dans le domaine des statistiques et de l‚Äôanalyse de donn√©es :\n\nAnalyse Factorielle et Visualisation Avanc√©e (en cours de publication)\nMod√©lisation de R√©gressions Mixtes pour des donn√©es complexes (en cours de publication)\nD√©veloppement d‚Äôoutils interactifs pour la visualisation de donn√©es\n\n\nVoir tous mes projets [Bient√¥t disponible sur le site]"
  },
  {
    "objectID": "index.html#derni√®res-publications",
    "href": "index.html#derni√®res-publications",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Derni√®res publications üìö",
    "text": "Derni√®res publications üìö\nVoici les derni√®res publications sur des sujets de statistiques et d‚Äôinformatique que j‚Äôai partag√©es :\n\nüìà Mod√©lisation statistique | Machine Learning | Deep Learning\n\nPr√©dire la dur√©e de carri√®re des joueurs NBA\n\nUne approche par r√©gression lin√©aire supervis√©e\n\nPr√©dire le diab√®te chez les femmes √† l‚Äôaide d‚Äôune r√©gression logistique\n\nOptimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning\n\n√âvaluation de l‚Äôimpact d‚Äôune intervention sur les cas de paludisme\n\nMod√©lisation des donn√©es de comptage par r√©gression de Poisson\n\nDiagnostic tumeurs c√©r√©brales - Reseau de neurones\n\nClassification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation d‚Äôun reseau de neuronnes convolutionnel\n\nReduction de dimensionnalit√©, clustering non supervis√©\n\nUtilisation de l‚ÄôAnalyse en Correspondance Principale et des K-moyennes pour classifier 167 pays selon leurs caract√©ristiques socio-√©conomiques\n\nDe l‚Äôanalyse multivari√©e √† la pr√©diction : un parcours combin√© entre ACP, KNN et r√©gression logistique avec des donn√©es m√©dicales\n\nMod√®le √† variable d√©pendante binaire appliqu√© √† un jeu de donn√©es m√©dical et machine learning\n\nClasser les gestes de la main (pierre, feuille, ciseau)\n\nUtilisation du reseau de neurones convolutionnel et de yolov8\n\n\n\n\n\nü§ñ Programmation et projets interactifs\n\nCr√©er un assistant virtuel avec commandes vocales en Python\n\nMini-projet m√™lant reconnaissance vocale, traduction et intelligence artificielle\n\nKedjeBoost ‚Äì Votre resto, en mode turbo !\n\nApplication desktop avec Java et Mysql\n\n\n\n\n\nüîç Analyse exploratoire et visualisation\n\nExploration des techniques d‚Äôanalyse factorielle\n\nPCA, AFC, ACM sur des jeux de donn√©es\n\nCr√©ation de cartes th√©matiques avec R (choropl√®thes, proportions)\n\nUtilisation des packages sf, tmap, leaflet\n\n\n\n\n\nüßë‚Äçüè´ Formations et bonnes pratiques\n\nR√©aliser des pr√©sentations dynamiques avec R et RStudio Utilisation de Quarto, Reveal.js et astuces pour pr√©senter efficacement\n\n\nVoir toutes les publications [Bient√¥t disponible]"
  },
  {
    "objectID": "index.html#publications-en-cours",
    "href": "index.html#publications-en-cours",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Publications en cours",
    "text": "Publications en cours\n\nFace attendance system : Faire le pointage automatique avec MTCNN, FaceNet et streamlit pour l‚Äôinterface (disponible bient√¥t)"
  },
  {
    "objectID": "index.html#contactez-moi",
    "href": "index.html#contactez-moi",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Contactez-moi",
    "text": "Contactez-moi\nSi vous avez des questions, des suggestions ou l‚Äôenvie d‚Äô√©changer autour d‚Äôun projet, je serais ravi de vous lire. N‚Äôh√©sitez pas √† me contacter via LinkedIn (Djamal TOE).\n\n\n\nA propos de nous\nEl√®ve en Science de donn√©es √† l‚ÄôEcole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation en France, titulaire d‚Äôune licence en Statistiques-informatique.\nR√©seaux sociaux\n\nLinkedIn\n\n\nCoordonn√©es\n\nAdresse : Rennes, 35000, France\nEmail : ******\nT√©l√©phone : +33 ** ** ** ** **\n\nHeures de services\n\n\n\nJour\nHoraire\n\n\n\n\nLundi\n8h30pm - 9:30pm\n\n\nMardi - Vendredi\n7pm - 8pm\n\n\nSamedi\n9:30am - 10:30am\n\n\nEn √©t√©\nTous les jours ouvr√©s de 7h √† 18h\n\n\n\n\n\n\n\nInformations suppl√©mentaires\n¬© 2024 DJAMAL DEV\nContact: ****\nLocalisation: Rennes, France\nT√©l√©phone: +33 ** ** ** ** **"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html",
    "title": "Djamaldbz - Cr√©e ton assistant virtuel en python !!!",
    "section": "",
    "text": "Comment cela fonctionne\n\n\n\nJe tiens tout d‚Äôabord √† rapperler que je n‚Äôutilise pas de mod√®le NLP pour cr√©er ce petit assistant virtuel. J‚Äôavais √©cris ce progamme en 2022, donc bien evidemment les outils utilis√©s ont √©volu√© et donc vous pourrez l‚Äôajuster √† votre guise. Le code source sera t√©l√©chargeable √† la fin de la page."
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#wolfram-alpha-cest-quoi-et-√†-quoi-√ßa-sert",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#wolfram-alpha-cest-quoi-et-√†-quoi-√ßa-sert",
    "title": "Djamaldbz - Cr√©e ton assistant virtuel en python !!!",
    "section": "Wolfram Alpha : C‚Äôest quoi et √† quoi √ßa sert ?",
    "text": "Wolfram Alpha : C‚Äôest quoi et √† quoi √ßa sert ?\nWolfram Alpha est un moteur de calcul et de r√©ponse bas√© sur l‚Äôintelligence artificielle et les algorithmes symboliques. Contrairement √† un moteur de recherche classique comme Google, qui fournit des liens vers des sites web, Wolfram Alpha g√©n√®re directement des r√©ponses pr√©cises bas√©es sur des bases de connaissances et des algorithmes math√©matiques avanc√©s. Il est souvent utilis√© pour des calculs, des questions scientifiques et des recherches bas√©es sur des donn√©es structur√©es.\n\nUtilit√© :\n\nR√©solution d‚Äô√©quations math√©matiques et scientifiques\n\nRecherche et analyse de donn√©es (statistiques, physique, chimie, finance, etc.)\n\nInterpr√©tation de requ√™tes en langage naturel\n\nG√©n√©ration de graphiques et de simulations\n\nüîó Cr√©er un compte Wolfram Alpha :\nSi vous souhaitez utiliser l‚ÄôAPI de Wolfram Alpha dans votre projet, vous devez cr√©er un compte via ce lien :\nüëâ Cr√©er un compte Wolfram Alpha Je posterai une demo sur comment creer son compte et recup√©rer un id pour une application. Car en effet, il existe plusieurs type d‚ÄôID qui servent √† diff√©rentes type d‚Äôapplications. Il fonctionne en Anglais donc nous allons √©crire une fonction pour la traduction du Francais en Anglais afin de poser des questions et une pour la traduction de l‚ÄôAnglais en Fran√ßais pour la reponse trouv√©e. Vous avez bien entendu besoin de connexion pour effectuer les recherches.\nExemple d‚Äôutilisation\n\nimport wolframalpha\nid_ = \"YOUR_WOLFRAMALPHA_ID\"\n##-- J'utliserai le mien que j'ai masqu√©\nid_ = r.id_\nclient = wolframalpha.Client(id_)\nqueries = [\"who is the president of France\",\n        \"2 times 2 times ln(2)\",\n        \"derivate xln(x)\",\n        \"integrate exponential of 2x between 2 and 4\"\n]\n\n\nans1 = client.query(queries[0])\nans1 = next(ans1.results).text\nans1\n\n'Emmanuel Macron (from 14/05/2017 to present)'\n\n\n\nans2 = client.query(queries[1])\nans2 = next(ans2.results).text\nans2\n\n'4 log(2)'\n\n\n\nans3 = client.query(queries[2])\nans3 = next(ans3.results).text\nans3\n\n'd/dx(x log(x)) = log(x) + 1'\n\n\n\nans4 = client.query(queries[3])\nans4 = next(ans4.results).text\nans4\n\n'integral_2^4 exp(2 x) dx = 1/2 e^4 (e^4 - 1)‚âà1463.2'\n\n\n\n\nExplication des parties techniques de votre code\nLe script commence par l‚Äôimportation des biblioth√®ques n√©cessaires :\n\nimport datetime\nimport webbrowser\nimport sys\nimport pywhatkit\nimport speech_recognition as sr\nimport pyttsx3 as ttx\nimport wikipedia\nfrom googletrans import Translator\nimport wolframalpha\n\n\ndatetime : gestion des dates et heures.\nwebbrowser : ouverture des pages web.\nsys : gestion des fonctionnalit√©s syst√®me.\npywhatkit : ex√©cution de commandes interactives comme la recherche YouTube.\nspeech_recognition : reconnaissance vocale.\npyttsx3 : synth√®se vocale.\nwikipedia : r√©cup√©ration d‚Äôinformations depuis Wikip√©dia.\ngoogletrans : traduction de texte.\nwolframalpha : moteur de r√©ponse √† des questions scientifiques et math√©matiques.\n\nIntaller les avec la commande :\n\nmodules = [\n    \"pywhatkit\", \"speechrecognition\", \"pyttsx3\",\n    \"wikipedia\", \"googletrans==4.0.0-rc1\", \"wolframalpha\", \"pyaudio\"\n]\n\nimport subprocess\nimport sys\ndef install_modules():\n    for module in modules:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", module])\n        except:\n            print(\"Quelque chose s'est mal pass√©e\")\n            \ninstall_modules()"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#configuration-du-moteur-de-synth√®se-vocale",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#configuration-du-moteur-de-synth√®se-vocale",
    "title": "Djamaldbz - Cr√©e ton assistant virtuel en python !!!",
    "section": "2. Configuration du moteur de synth√®se vocale",
    "text": "2. Configuration du moteur de synth√®se vocale\nLe code initialise pyttsx3 et affiche les voix disponibles :\n\nmoteur = ttx.init()\nvoix_disponibles = moteur.getProperty(\"voices\")\n\nfor index, voix in enumerate(voix_disponibles):\n    print(f\"Index {index} - ID: {voix.id} - Langue: {voix.languages} - Nom: {voix.name}\")\n\nIndex 0 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0 - Langue: [] - Nom: Microsoft David Desktop - English (United States)\nIndex 1 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0 - Langue: [] - Nom: Microsoft Zira Desktop - English (United States)\nIndex 2 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_FR-FR_HORTENSE_11.0 - Langue: [] - Nom: Microsoft Hortense Desktop - French\n\n\nEnsuite, une voix sp√©cifique est s√©lectionn√©e et test√©e :\n\nmoteur.setProperty(\"voice\", voix_disponibles[2].id)\nmoteur.say(\"Bonjour, ceci est un test avec une autre voix.\")\nmoteur.runAndWait()"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#d√©finition-de-la-classe-voxaassistant",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#d√©finition-de-la-classe-voxaassistant",
    "title": "Djamaldbz - Cr√©e ton assistant virtuel en python !!!",
    "section": "3. D√©finition de la classe voxaAssistant",
    "text": "3. D√©finition de la classe voxaAssistant\nLa classe voxaAssistant g√®re toutes les fonctionnalit√©s de l‚Äôassistant vocal.\n\n3.1 Initialisation\n\nclass voxaAssistant:\n    def __init__(self):\n        self.ecouteur = sr.Recognizer()\n        self.moteur = ttx.init()\n        self.voix_disponibles = self.moteur.getProperty(\"voices\")\n        self.moteur.setProperty(\"voice\", self.voix_disponibles[2].id)\n        self.moteur.setProperty(\"rate\", 170)\n        self.app_id = r.id_\n        self.client = wolframalpha.Client(self.app_id)\n\nCette m√©thode : - Initialise le moteur de reconnaissance vocale (speech_recognition). - Configure la synth√®se vocale avec pyttsx3. - D√©finit la cl√© API pour Wolfram Alpha.\n\n\n3.2 Fonction parler\nCette fonction g√©n√®re une sortie vocale √† partir d‚Äôun texte donn√©.\n\ndef parler(self, texte):\n    self.moteur.say(texte)\n    self.moteur.runAndWait()\n\n\n\n3.3 Fonction saluer\nCette fonction ajuste le message de salutation en fonction de l‚Äôheure.\n\ndef saluer(self):\n    heure_actuel = int(datetime.datetime.now().hour)\n    if 0 &lt;= heure_actuel &lt;= 12:\n        self.parler(\"Bonjour √† vous Djamal\")\n    else:\n        self.parler(\"Bonsoir √† vous Djamal\")"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#reconnaissance-et-traitement-des-requ√™tes-vocales",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#reconnaissance-et-traitement-des-requ√™tes-vocales",
    "title": "Djamaldbz - Cr√©e ton assistant virtuel en python !!!",
    "section": "4. Reconnaissance et Traitement des Requ√™tes Vocales",
    "text": "4. Reconnaissance et Traitement des Requ√™tes Vocales\n\nFonction voxa_requete\nCette fonction √©coute l‚Äôutilisateur et transcrit la parole en texte.\n\ndef voxa_requete(self):\n    with sr.Microphone() as parole:\n        print(\"Entrain d'√©couter ...\")\n        self.ecouteur.adjust_for_ambient_noise(parole, duration=1)\n        self.ecouteur.pause_threshold = 1.5\n        try:\n            voix = self.ecouteur.listen(parole, timeout=5, phrase_time_limit=5)\n            command = self.ecouteur.recognize_google(voix, language=\"fr\").lower()\n            print(\"Vous avez dit .... : \", command)\n            return command\n        except sr.UnknownValueError:\n            print(\"Je n'ai pas compris, veuillez r√©p√©ter.\")\n            return \"\"\n        except sr.RequestError:\n            print(\"Erreur avec le service de reconnaissance vocale.\")\n            return \"\"\n\n\n\nRecherche Google et YouTube\nSi l‚Äôutilisateur mentionne Google ou YouTube, la recherche est effectu√©e automatiquement.\n\nelif \"google\" in voix:\n    url = voix.split().index(\"google\")\n    elt_rechercher = voix.split()[url + 1:]\n    self.parler(\"D'accord, je lance la recherche\")\n    webbrowser.open(\"https://www.google.com/search?q=\" + \"+\".join(elt_rechercher), new=2)\n\n\nelif \"recherche sur youtube\" in voix or \"recherche sur youtube.com\" in voix:\n                url = voix.split().index(\"youtube\")\n                elt_rechercher = voix.split()[url + 1:]\n                self.parler(\"d'accord  je  lance  la  recherche\")\n                webbrowser.open(\n                    \"http://www.youtube.com/results?search_query=\"\n                    + \"+\".join(elt_rechercher),\n                    new=2,\n                )\n\n\n1Ô∏è‚É£ split() : Pourquoi l‚Äôutiliser ici ?\n\nurl = voix.split().index(\"google\")\nelt_rechercher = voix.split()[url + 1:]\n\n\nsplit() d√©coupe une cha√Æne de caract√®res en une liste de mots.\nIci, on cherche l‚Äôindex du mot ‚Äúgoogle‚Äù pour r√©cup√©rer les mots suivants, qui correspondent √† la requ√™te de l‚Äôutilisateur.\n\nExemple :\n\nEntr√©e : \"cherche sur google c'est quoi la capitale de la France\"\n\nApr√®s split() : [\"cherche\",\"sur\", \"google\", \"c\", \"'\", \"est\", \"quoi\", \"la\", \"capitale\", \"de\", \"la\", \"France\"]\n\nIndex du mot ‚Äúgoogle‚Äù : 3\n\nCe qui est recherch√© : [\"c\", \"'\", \"est\", \"quoi\", \"la\", \"capitale\", \"de\", \"la\", \"France\"] ‚Üí Ici, on devrait prendre les mots apr√®s ‚Äúgoogle‚Äù.\n\n\n\n\n2Ô∏è‚É£ Pourquoi y a-t-il des + dans l‚ÄôURL de Google et YouTube ?\n\nwebbrowser.open(\"https://www.google.com/search?q=\" + \"+\".join(elt_rechercher), new=2)\n\n\nelif \"youtube\" in voix:\n    s = voix.replace(\"youtube\", \"\")\n    self.parler(\"D'accord sans soucis\")\n    pywhatkit.playonyt(s)\n\n\nExplication du +.\n\nDans une URL, un espace est souvent remplac√© par + ou %20.\n\nExemple : Si l‚Äôutilisateur dit ‚Äúrecherche machine learning sur google‚Äù, on doit transformer \"machine learning\" en \"machine+learning\" pour que Google comprenne.\n\nAutre solution : \"%20\".join(elt_rechercher) aurait aussi pu √™tre utilis√©.\n\n\n\n\n\nRecherches Avanc√©es avec Wolfram Alpha et Wikip√©dia\n\nUtilisation de Wolfram Alpha pour r√©pondre aux questions g√©n√©rales\n\n    def question_generale(self, voix):\n        voix = self.translate_eng_fr(voix)\n        try:\n            reponse = self.client.query(voix)\n            res = next(reponse.results).text\n            res = self.translate_fr_eng(res)\n            print(\"Un instant ...\")\n            print(res)\n            self.parler(res)\n        except:\n            self.parler(\"Je n'ai pas trouv√© de r√©ponse.\")\n\n¬†¬†¬†¬†¬†¬†Ici, l‚Äôassistant vocal envoie la requ√™te √† Wolfram Alpha, r√©cup√®re la r√©ponse et la traduit en fran√ßais avant de la prononcer.\nSi aucune r√©ponse n‚Äôest trouv√©e, une recherche est effectu√©e sur Wikip√©dia.\n\n\nUtilisation de wikipedia pour r√©pondre aux questions g√©n√©rales\n\ntry:\n    wikipedia.set_lang(\"fr\")\n    info = wikipedia.summary(voix, 1)\n    self.parler(str(info))\nexcept:\n    self.parler(\"Je n'ai pas bien compris\")\n\n\n\n3Ô∏è‚É£ query : √Ä quoi √ßa sert dans Wolfram Alpha*\n\nreponse = self.client.query(voix)\nres = next(reponse.results).text\n\n\n.query(voix) : envoie la question de l‚Äôutilisateur √† Wolfram Alpha.\n\nnext(reponse.results).text : r√©cup√®re la premi√®re r√©ponse retourn√©e et extrait le texte.\n\nSi Wolfram Alpha trouve une r√©ponse pertinente, elle est lue √† haute voix.\n\n\n\n\nR√©sum√© des concepts cl√©s :\n\n\n\n\n\n\n\n√âl√©ment\nExplication\n\n\n\n\nWolfram Alpha\nMoteur de calcul intelligent r√©pondant √† des requ√™tes scientifiques et analytiques\n\n\nsplit()\nD√©coupe une phrase en liste de mots\n\n\nquery()\nEnvoie une requ√™te √† Wolfram Alpha\n\n\njoin(‚Äú+‚Äù)\nTransforme une liste de mots en requ√™te lisible par un moteur de recherche"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#test-du-code",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#test-du-code",
    "title": "Djamaldbz - Cr√©e ton assistant virtuel en python !!!",
    "section": "Test du code",
    "text": "Test du code"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#conclusion",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#conclusion",
    "title": "Djamaldbz - Cr√©e ton assistant virtuel en python !!!",
    "section": "Conclusion",
    "text": "Conclusion\nCe code met en place un assistant vocal capable de reconna√Ætre et d‚Äôex√©cuter des commandes vocales en fran√ßais, d‚Äôeffectuer des recherches sur le web, et de r√©pondre aux questions gr√¢ce √† Wolfram Alpha et Wikip√©dia. Il constitue une base quelque peu solide pour un assistant personnel plus ou moins intelligent.\nT√©l√©charger le fichier .python\nT√©l√©charger la vid√©o\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour √† la page d‚Äôaccueuil"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "",
    "text": "Les tumeurs c√©r√©brales repr√©sentent un enjeu majeur de sant√© publique en raison de leur complexit√© diagnostique et de leurs implications cliniques graves. Classifier pr√©cis√©ment ces tumeurs, notamment les m√©ningiomes, les gliomes et les tumeurs hypophysaires, est essentiel pour guider les d√©cisions th√©rapeutiques et am√©liorer le pronostic des patients (WHO2021?).\n¬†¬†¬†¬†¬†¬†Selon la 5e √©dition de la classification de l‚ÄôOrganisation Mondiale de la Sant√© (OMS), une approche int√©gr√©e reposant √† la fois sur des crit√®res histopathologiques et mol√©culaires est d√©sormais recommand√©e pour le diagnostic des tumeurs du syst√®me nerveux central (WHO2021?). Cependant, l‚Äôinterpr√©tation des images m√©dicales, en particulier des IRM c√©r√©brales, reste un d√©fi complexe et chronophage pour les professionnels de sant√©. Dans ce contexte, les m√©thodes d‚Äôintelligence artificielle, notamment les r√©seaux de neurones convolutifs (CNN), ont montr√© un potentiel prometteur pour automatiser la classification des tumeurs √† partir d‚Äôimages IRM.\n¬†¬†¬†¬†¬†¬†Une revue men√©e par Xie et al. (Xie et al. 2022) souligne les avanc√©es r√©centes dans l‚Äôapplication des CNN √† la classification des tumeurs c√©r√©brales, en insistant sur les d√©fis techniques rencontr√©s comme le surapprentissage, le d√©s√©quilibre des classes, ou encore la n√©cessit√© d‚Äôint√©grer la classification mol√©culaire. D‚Äôautres travaux, tels que celui de Rasheed et al. (Rasheed et al. 2023), proposent un mod√®le CNN personnalis√© pour diff√©rencier automatiquement les IRM de trois types de tumeurs avec une grande pr√©cision, tout en mettant en avant l‚Äôimportance du pr√©traitement des images pour am√©liorer la performance du mod√®le.\n¬†¬†¬†¬†¬†¬†En parall√®le, Tummala et al. (Tummala et al. 2022) introduisent une approche combin√©e utilisant les transformeurs visuels (Vision Transformers, ViT) avec les CNN pour augmenter la robustesse et la pr√©cision du mod√®le, d√©montrant ainsi la pertinence des mod√®les hybrides. Dans le m√™me esprit, Srinivasan et al. (Srinivasan et al. 2024) con√ßoivent un mod√®le profond et hybride adapt√© √† la classification multi-classes, en combinant plusieurs architectures CNN avec des strat√©gies d‚Äôoptimisation.\n¬†¬†¬†¬†¬†¬†Dans cette √©tude, nous proposons de d√©velopper un mod√®le bas√© sur un r√©seau de neurones convolutif (CNN) pour classifier les IRM c√©r√©brales en trois types de tumeurs : gliomes, m√©ningiomes et tumeurs hypophysaires. Cette approche vise √† fournir un outil efficace d‚Äôaide au diagnostic, en s‚Äôappuyant sur les m√©thodes r√©centes les plus performantes issues de la litt√©rature."
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#introduction",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#introduction",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "",
    "text": "Les tumeurs c√©r√©brales repr√©sentent un enjeu majeur de sant√© publique en raison de leur complexit√© diagnostique et de leurs implications cliniques graves. Classifier pr√©cis√©ment ces tumeurs, notamment les m√©ningiomes, les gliomes et les tumeurs hypophysaires, est essentiel pour guider les d√©cisions th√©rapeutiques et am√©liorer le pronostic des patients (WHO2021?).\n¬†¬†¬†¬†¬†¬†Selon la 5e √©dition de la classification de l‚ÄôOrganisation Mondiale de la Sant√© (OMS), une approche int√©gr√©e reposant √† la fois sur des crit√®res histopathologiques et mol√©culaires est d√©sormais recommand√©e pour le diagnostic des tumeurs du syst√®me nerveux central (WHO2021?). Cependant, l‚Äôinterpr√©tation des images m√©dicales, en particulier des IRM c√©r√©brales, reste un d√©fi complexe et chronophage pour les professionnels de sant√©. Dans ce contexte, les m√©thodes d‚Äôintelligence artificielle, notamment les r√©seaux de neurones convolutifs (CNN), ont montr√© un potentiel prometteur pour automatiser la classification des tumeurs √† partir d‚Äôimages IRM.\n¬†¬†¬†¬†¬†¬†Une revue men√©e par Xie et al. (Xie et al. 2022) souligne les avanc√©es r√©centes dans l‚Äôapplication des CNN √† la classification des tumeurs c√©r√©brales, en insistant sur les d√©fis techniques rencontr√©s comme le surapprentissage, le d√©s√©quilibre des classes, ou encore la n√©cessit√© d‚Äôint√©grer la classification mol√©culaire. D‚Äôautres travaux, tels que celui de Rasheed et al. (Rasheed et al. 2023), proposent un mod√®le CNN personnalis√© pour diff√©rencier automatiquement les IRM de trois types de tumeurs avec une grande pr√©cision, tout en mettant en avant l‚Äôimportance du pr√©traitement des images pour am√©liorer la performance du mod√®le.\n¬†¬†¬†¬†¬†¬†En parall√®le, Tummala et al. (Tummala et al. 2022) introduisent une approche combin√©e utilisant les transformeurs visuels (Vision Transformers, ViT) avec les CNN pour augmenter la robustesse et la pr√©cision du mod√®le, d√©montrant ainsi la pertinence des mod√®les hybrides. Dans le m√™me esprit, Srinivasan et al. (Srinivasan et al. 2024) con√ßoivent un mod√®le profond et hybride adapt√© √† la classification multi-classes, en combinant plusieurs architectures CNN avec des strat√©gies d‚Äôoptimisation.\n¬†¬†¬†¬†¬†¬†Dans cette √©tude, nous proposons de d√©velopper un mod√®le bas√© sur un r√©seau de neurones convolutif (CNN) pour classifier les IRM c√©r√©brales en trois types de tumeurs : gliomes, m√©ningiomes et tumeurs hypophysaires. Cette approche vise √† fournir un outil efficace d‚Äôaide au diagnostic, en s‚Äôappuyant sur les m√©thodes r√©centes les plus performantes issues de la litt√©rature."
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#m√©thodologie",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#m√©thodologie",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "M√©thodologie",
    "text": "M√©thodologie\n\nSource des donn√©es\n¬†¬†¬†¬†¬†¬†Les donn√©es ont √©t√© t√©l√©charg√©es sous forme d‚Äôimages depuis la plateforme Kaggle (en acc√®s libre Cliquez ici pour acc√©der √† la page). Elles sont r√©parties en trois sous-groupes :\n\nbrain_menin (2004 images) pour la m√©ningiome, une tumeur g√©n√©ralement b√©nigne des m√©ninges (les membranes entourant le cerveau);\nbrain_glioma (2004 images) pour le gliome, une tumeur maligne issue des cellules gliales, souvent infiltrante et agressive;\nbrain_tumor (2048 images) pour, √©ventuellement, les autres types de tumeurs c√©r√©brales, souvent malignes, incluant diverses localisations et origines cellulaires.\n\nAu total, la base de donn√©es contient donc 6056 images.\n\n\nTraitement des images\n¬†¬†¬†¬†¬†¬†Avant de commencer la phase de classification, les images ont √©t√© r√©parties al√©atoirement dans trois r√©pertoires selon les proportions suivantes:\n\ntrain : 70 % des images (entra√Ænement)\nval : 15 % des images (validation)\ntest : 15 % des images (test)\n\nPlus explicitement :\n\nLe dossier train sert √† entra√Æner le mod√®le.\nLe dossier val est utilis√© pour valider le mod√®le √† chaque it√©ration, ce qui permet d‚Äôajuster les param√®tres et de minimiser le score de perte (calcul√© √† partir de la fonction de perte/fonction objective) gr√¢ce √† l‚Äôoptimiseur (ici, Adam).\nLe dossier test permet d‚Äô√©valuer la performance finale du mod√®le sur des donn√©es qu‚Äôil n‚Äôa jamais vues.\n\nChaque r√©pertoire contient les trois classes de tumeurs c√©r√©brales: brain_menin, brain_tumor et brain_glioma que les donn√©es nous fournissaient.\n\n\n\nPr√©paration et chargement des images\n\n\n\nPour l‚Äôentra√Ænement et la validation, les images sont trait√©es √† l‚Äôaide de g√©n√©rateurs Keras (ImageDataGenerator):\n\nDimensionnement : toutes les images sont redimensionn√©es √† 224√ó224 pixels, taille d‚Äôentr√©e standard pour de nombreux r√©seaux pr√©-entra√Æn√©s.\nBatch size : on fixe le nombre d‚Äôimages trait√©es simultan√©ment √† chaque pas d‚Äôentra√Ænement.\nData augmentation :\n\nEntra√Ænement :\n\nNormalisation des pixels : passage de l‚Äô√©chelle [0, 255] √† l‚Äô√©chelle [0,1]\nRotation al√©atoire jusqu‚Äô√† ¬±15¬∞ (rotation_range=15)\nZoom al√©atoire jusqu‚Äô√† 20 % (zoom_range=0.2)\nFlip horizontal al√©atoire (horizontal_flip=True)\n\nValidation et test :\n\nSeule la normalisation des pixels (de [0, 255] √† [0, 1]), afin d‚Äô√©valuer le mod√®le sur des images aux orientations et √©chelles r√©elles.\n\n\n\n\n\nMod√®le utilis√©\n¬†¬†¬†¬†¬†¬†Pour r√©pondre √† la probl√©matique de classification des tumeurs c√©r√©brales √† partir d‚Äôimages, nous avons opt√© pour l‚Äôutilisation d‚Äôun r√©seau de neurones convolutifs (CNN). Plut√¥t que de construire un mod√®le √† partir de z√©ro ‚Äî ce qui aurait √©t√© risqu√© compte tenu de la taille relativement modeste du jeu de donn√©es et des ressources de calcul disponibles ‚Äî, nous avons choisi de recourir √† une approche de transfert d‚Äôapprentissage.\n¬†¬†¬†¬†¬†¬†Plus pr√©cis√©ment, nous avons utilis√© le mod√®le EfficientNetB5, un CNN pr√©entra√Æn√© sur le vaste ensemble de donn√©es ImageNet. Ce mod√®le pr√©sente un excellent compromis entre performance, rapidit√© et taille du mod√®le, ce qui le rend particuli√®rement adapt√© pour des t√¢ches de classification d‚Äôimages m√©dicales o√π les ressources peuvent √™tre limit√©es.\nDans le cadre de cette approche :\n\nLes couches convolutionnelles profondes du mod√®le ont √©t√© conserv√©es pour exploiter leur capacit√© √† extraire des caract√©ristiques visuelles de bas niveau (bords, textures, formes, etc.);\nLes couches sup√©rieures (√† partir de la 95e couche dans notre cas) ont √©t√© d√©sactiv√©es (non gel√©es) et r√©entra√Æn√©es sur notre propre base de donn√©es, afin d‚Äôadapter le mod√®le aux sp√©cificit√©s des tumeurs c√©r√©brales.\n\nCette technique permet de b√©n√©ficier des connaissances g√©n√©rales acquises par le mod√®le tout en l‚Äôadaptant finement √† notre probl√®me sp√©cifique. En effet, les mod√®les pr√©entra√Æn√©s comme EfficientNet ne sont pas directement adapt√©s aux t√¢ches cibl√©es des data scientists. Il est donc crucial de les affiner (fine-tuning) sur des donn√©es sp√©cifiques pour am√©liorer leur capacit√© √† d√©tecter des motifs propres au domaine m√©dical, tels que les contours et anomalies propres aux IRM c√©r√©brales.\nEnfin, construire un r√©seau de neurones enti√®rement personnalis√© aurait pu exposer notre solution √† des risques de surapprentissage ou √† des difficult√©s d‚Äôoptimisation, sans compter les contraintes computationnelles qui auraient ralenti consid√©rablement le processus.\n\n\n\nConstruction du mod√®le avec EfficientNetB5\n\n\n\n¬†¬†¬†¬†¬†¬†Pour la phase de mod√©lisation, nous avons utilis√© le mod√®le EfficientNetB5, pr√©entra√Æn√© sur ImageNet. Ce mod√®le est particuli√®rement performant pour la classification d‚Äôimages complexes et convient bien √† des t√¢ches m√©dicales exigeantes en pr√©cision.\nNous avons charg√© EfficientNetB5 sans ses couches de sortie (param√®tre include_top=False) afin de pouvoir personnaliser l‚Äôarchitecture en sortie. L‚Äôentr√©e du mod√®le est sp√©cifi√©e avec la taille (224, 224, 3) et le 3 correspond aux cannaux de couleurs (RGB : Rouge-Vert-Bleu) correspondant √† nos images redimensionn√©es. Cependant IRM sont affich√©es en niveaux de gris souvent pour mieux visualiser les structure de cerveau. Or EfficientNet attend normalement des images de la forme (3, H, W) et les images de niveaux gris sont de la forme (1, H, W). On serait donc tent√© de les convertir en ‚Äúfaux RGB‚Äù (3 canaux identiques). Toutefois, le mode des images a √©t√© v√©rifi√© et celles-ci sont bien en RGB. Elle sont en noires blancs, mais elles ont trois cannaux et chaque canal contiendrait les m√™mes valeurs ou une version identique. Ainsi les images ont √©t√© laiss√©es telles quelles.\nNous avons ensuite :\n\nGel√© les poids du mod√®le pr√©entra√Æn√© pour ne pas alt√©rer les connaissances acquises sur ImageNet lors d‚Äôun premier entra√Ænement ;\nAjout√© un GlobalAveragePooling2D, qui r√©duit la dimensionnalit√© tout en conservant les caract√©ristiques importantes ;\nAjout√© une couche dense de 128 neurones avec la fonction d‚Äôactivation ReLU ;\nEt enfin une couche de sortie avec 3 neurones, activ√©e par une fonction softmax pour la classification des trois types de tumeurs : m√©ningiome, gliome et autres tumeurs c√©r√©brales.\n\nLe mod√®le a √©t√© compil√© avec :\n\nL‚Äôoptimiseur Adam, tr√®s utilis√© pour sa rapidit√© de convergence,\nUne taux d‚Äôapprentissage tr√®s faible (0.00001) pour √©viter les grandes variations de poids √† cause du gel partiel,\nLa fonction de perte categorical_crossentropy, adapt√©e √† une classification multiclasse,\nEt comme m√©trique de performance : l‚Äôaccuracy.\n\n\n\n\nPhase de fine-tuning (d√©gel progressif)\n\n\n\n¬†¬†¬†¬†¬†¬†Pour mieux adapter le mod√®le aux sp√©cificit√©s de nos donn√©es, nous avons proc√©d√© √† un fine-tuning partiel :\n\nLe mod√®le a √©t√© rendu enti√®rement entra√Ænable (base_model.trainable = True) ;\nAfin d‚Äô√©viter une modification brutale des poids et une possible d√©gradation des performances, les 95 premi√®res couches ont √©t√© gel√©es, et seules les couches √† partir de la 96e ont √©t√© entra√Æn√©es. Cette technique permet au mod√®le de conserver ses caract√©ristiques basiques tout en affinant ses couches sup√©rieures pour s‚Äôadapter √† notre t√¢che sp√©cifique ;\nLe mod√®le a √©t√© compil√© avec le m√™me taux d‚Äôapprentissage tr√®s faible (1e-5) afin de permettre une phase de fine-tuning progressive, en √©vitant des modifications brusques des poids et en assurant une convergence stable ;\nUn entra√Ænement initialement pr√©vu sur 70 √©poques a √©t√© lanc√©, avec un callback EarlyStopping (monitor=val_loss, patience=3, restore_best_weights=True) pour arr√™ter automatiquement l‚Äôapprentissage au meilleur point de validation (apr√®s trois √©poques cons√©cutifs sans que la valeur du score de perte de la validation ne soit inf√©rieuer √† sa plus p√©tite valeure), et un ModelCheckpoint pour sauvegarder le mod√®le de val_loss minimal.\n\n\n\nEvaluation du mod√®le\n¬†¬†¬†¬†¬†¬†Une fois l‚Äôentra√Ænement termin√©, le mod√®le sera √©valu√© sur un jeu de donn√©es de test ind√©pendant afin de mesurer sa capacit√© √† classer correctement les diff√©rentes classes de tumeurs c√©r√©brales. Cette √©valuation repose sur plusieurs m√©triques standard qui quantifient la performance du mod√®le en termes de justesse, pr√©cision et rappel.\n\n\n\nD√©finitions des m√©triques de classification\n\n\n\n\nTP (Vrais positifs) : nombre d‚Äôimages bien class√©es dans leur vraie classe.\n\nFP (Faux positifs) : nombre d‚Äôimages mal class√©es dans cette classe alors qu‚Äôelles n‚Äôy appartiennent pas.\n\nFN (Faux n√©gatifs) : nombre d‚Äôimages appartenant √† cette classe mais mal class√©es dans une autre.\n\nTN (Vrais n√©gatifs) : nombre d‚Äôimages bien exclues de cette classe.\n\n\n\n\nFormules\n\n\n\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\n\\]\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP} \\quad\n\\]\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN} \\quad\n\\]\n\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\n\nCorrespondance des classes\n\n\n\n\nClasse 1 : Gliome\n\nClasse 2 : M√©ningiome\n\nClasse 3 : Autre tumeur"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#r√©sultats",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#r√©sultats",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "R√©sultats",
    "text": "R√©sultats\n\nEchantillons des images t√©l√©charg√©es\n¬†¬†¬†¬†¬†¬†Les images ci-dessous sont des √©chantillons de celles qui serviront de base pour l‚Äôentra√Ænement, la validation et le test du mod√®le. Celles affich√©es sont choisies al√©atoirement dans au sein de chaque classe.\n\nTumeur m√©ningiome\n\n\n\n\n\n\nFigure¬†1: Echantillons des images de la m√©ningiome collect√©es\n\n\n\n\n\nTumeur gliome\n\n\n\n\n\n\nFigure¬†2: Echantillons des images de la gliome collect√©es\n\n\n\n\n\nAutres types de tumeurs\n\n\n\n\n\n\nFigure¬†3: Echantillons des images des autres types de tumeurs collect√©es\n\n\n\n\n¬†¬†¬†¬†¬†¬†A l‚Äôoeil nu, il m‚Äôest personnellement impossible de pouvoir classer ses images sur la base de crit√®res solides.\n\n\nR√©sultats et validation du mod√®le\n\n\n\nEvolution des pertes et des exacatidues durant l‚Äôentra√Ænement\n\n\n\n\n\n\n\n\nFigure¬†4: √âvolution de l‚Äôexactitude et de la perte pendant l‚Äôentra√Ænement\n\n\n\n\n¬†¬†¬†¬†¬†¬†Le mod√®le s‚Äôest arr√©t√© apr√®s √©poque 37(patience=3). Sur la figure ci-dessus, on observe qu‚Äôau cours des premi√®res √©poques (environ jusqu‚Äô√† la troisi√®me), l‚Äôexactitude de l‚Äôentra√Ænement (courbe verte) √©tait l√©g√®rement sup√©rieure √† celle de la validation (courbe bleue). Toutefois, entre la quatri√®me et la onzi√®me √©poque, l‚Äôexactitude de validation a d√©pass√© de mani√®re notable celle de l‚Äôentra√Ænement. Cette progression peut s‚Äôexpliquer par l‚Äôajustement progressif des poids du mod√®le, sous l‚Äôeffet de l‚Äôoptimiseur, qui am√©liore les performances globales, y compris sur les donn√©es de validation.\n¬†¬†¬†¬†¬†¬†Sur la suite de l‚Äôentra√Ænement, on constate que l‚Äôexactitude de validation reste globalement l√©g√®rement sup√©rieure √† celle de l‚Äôentra√Ænement, tandis que la perte de validation (courbe orange) reste plus basse que la perte d‚Äôentra√Ænement (courbe rouge). Ce comportement, bien que contre-intuitif, peut s‚Äôexpliquer par un ensemble de validation plus homog√®ne ou moins bruit√©, ou encore par des effets de r√©gularisation implicites induits par la structure du mod√®le ou les callbacks utilis√©s.\n¬†¬†¬†¬†¬†¬†Enfin, √† partir de la vingti√®me √©poque environ, toutes les courbes se stabilisent autour de valeurs proches de 1 pour les exactitudes, et proches de 0 pour les pertes, ce qui t√©moigne d‚Äôune excellente capacit√© de g√©n√©ralisation du mod√®le sans signe apparent de surapprentissage (overfitting).\n\n\n\nMatrice de confusion (test)\n\n\n\n\n\n\n\n\nFigure¬†5: Matrice de confusion\n\n\n\n\n¬†¬†¬†¬†¬†¬†La matrice de confusion montre une excellente performance du mod√®le, avec seulement trois erreurs de classification sur 907 images de test. Le mod√®le atteint un rappel parfait (100 %) pour les classes gliome et autres tumeurs, et une pr√©cision parfaite (100 %) pour la classe m√©ningiome.\nLes F1-scores d√©passent 99 % dans chaque cas, confirmant une capacit√© remarquable √† diff√©rencier les types de tumeurs c√©r√©brales.\nCes r√©sultats t√©moignent d‚Äôun mod√®le bien entra√Æn√©, capable de g√©n√©raliser efficacement sur des donn√©es de validation, m√™me dans un contexte de classification multiclasse sensible comme celui des diagnostics de tumeurs c√©r√©brales.\n\n\n\nR√©sultats par classe\n\n\n\n\n\n\n\nTable¬†1: M√©triques en pourentage par classe\n\n\nClasse\nPr√©cision‚Ä¶.\nRappel‚Ä¶.\nF1.score‚Ä¶.\nSupport\n\n\n\n\nGliome\n99.34\n100\n99.67\n300\n\n\nM√©ningiome\n100.00\n99\n99.50\n300\n\n\nAutre tumeur\n99.68\n100\n99.84\n307\n\n\n\n\n\n\n\n\n\n\nTable¬†2: M√©triques globales en pourentage\n\n\nM√©trique\nPr√©cision‚Ä¶.\nRappel‚Ä¶.\nF1.score‚Ä¶.\nTotal\n\n\n\n\nExactitude (Accuracy)\nNA\nNA\n99.67\n907\n\n\nMoyenne macro\n99.67\n99.67\n99.67\n907\n\n\nMoyenne pond√©r√©e\n99.67\n99.67\n99.67\n907\n\n\n\n\n\n\n\n\n\nInterpr√©tation des m√©triques\n\n\n\n¬†¬†¬†¬†¬†¬†Les r√©sultats obtenus montrent que le mod√®le de classification des tumeurs c√©r√©brales atteint une excellente performance sur l‚Äôensemble de test, avec des scores de pr√©cision, rappel et F1-score sup√©rieurs √† 99 % pour chaque classe.\n\nClasse 1 (Gliome) :\n\nLa pr√©cision de 99,34 % indique que lorsque le mod√®le pr√©dit un gliome, il se trompe tr√®s rarement (environ 0,66 % des pr√©dictions positives sont fausses).\nLe rappel parfait √† 100 % signifie que toutes les images de gliome sont correctement d√©tect√©es, sans aucun faux n√©gatif.\nLe F1-score √©lev√© de 99,67 % traduit un excellent compromis entre pr√©cision et rappel, assurant une classification fiable pour cette classe.\n\nClasse 2 (M√©ningiome) :\n\nUne pr√©cision parfaite de 100 % montre que toutes les images class√©es comme m√©ningiome sont effectivement correctes, sans aucun faux positif.\nUn rappel de 99,00 % indique que quelques images de m√©ningiome ont √©t√© class√©es √† tort dans une autre cat√©gorie (quelques faux n√©gatifs).\nLe F1-score de 99,50 % confirme une tr√®s bonne performance globale pour cette classe, avec un √©quilibre solide entre pr√©cision et rappel.\n\nClasse 3 (Autre tumeur) :\n\nLa pr√©cision de 99,68 % montre que le mod√®le fait tr√®s peu d‚Äôerreurs positives pour cette cat√©gorie.\nLe rappel parfait √† 100 % signifie qu‚Äôaucune image de cette classe n‚Äôa √©t√© manqu√©e (pas de faux n√©gatifs).\nLe F1-score √©lev√© de 99,84 % souligne la grande qualit√© de la classification pour cette classe.\n\nMoyennes globales (macro et pond√©r√©e) :\n\nLes moyennes sup√©rieures √† 99,67 % pour la pr√©cision, le rappel et le F1-score indiquent que le mod√®le performe de fa√ßon tr√®s homog√®ne et robuste sur l‚Äôensemble des classes.\n\nLe fait que pr√©cision et rappel soient tr√®s proches pour toutes les classes sugg√®re un bon √©quilibre entre la capacit√© du mod√®le √† d√©tecter les tumeurs (rappel) et √† limiter les fausses alertes (pr√©cision).\n\n\n\n\n\nDiscussions des r√©sultats\n¬†¬†¬†¬†¬†¬†Notre mod√®le de classification des tumeurs c√©r√©brales a atteint une exactitude de 99,67 % sur les donn√©es de test, avec une valeur de perte (loss) tr√®s faible de 0,0144. La matrice de confusion r√©v√®le une excellente performance, avec tr√®s peu d‚Äôerreurs entre les classes. Par exemple, seules quelques images ont √©t√© mal class√©es, et aucune confusion n‚Äôa √©t√© observ√©e pour la classe des autres tumeurs.\nAu cours de l‚Äôentra√Ænement, le score de perte diminuait de mani√®re continue √† chaque it√©ration, aussi bien pour l‚Äôensemble d‚Äôapprentissage que pour celui de validation. Cette √©volution parall√®le et coh√©rente des courbes de perte constitue un indice fort d‚Äôabsence de surapprentissage (overfitting). Le mod√®le semble ainsi avoir trouv√© un bon compromis entre m√©morisation des donn√©es d‚Äôentra√Ænement et capacit√© de g√©n√©ralisation.\nCes r√©sultats se comparent favorablement √† ceux pr√©sent√©s dans la litt√©rature. Tummala et al.¬†((Tummala et al. 2022)), utilisant un ensemble de Vision Transformers, rapportent une pr√©cision de 99,12 %, tandis que (Rasheed et al. 2023) obtiennent 98,72 % avec un mod√®le CNN. D‚Äôautres √©tudes, comme celles de (Srinivasan et al. 2024) et (Xie et al. 2022), rapportent √©galement des pr√©cisions comprises entre 97 % et 99 %, mais sur des volumes de donn√©es souvent plus restreints.\n¬†¬†¬†¬†¬†¬†Enfin, il convient de souligner que notre jeu de donn√©es comportait environ 6000 images, soit un volume environ deux fois plus important que dans plusieurs des √©tudes pr√©c√©dentes, ce qui pourrait contribuer √† renforcer la fiabilit√© de l‚Äô√©valuation. Toutefois, bien que ces r√©sultats soient tr√®s encourageants, il reste important de rester prudent. Des facteurs tels que la diversit√© des images, la qualit√© des annotations, ou encore la s√©lection des hyperparam√®tres peuvent influencer les performances.\n¬†¬†¬†¬†¬†¬†Ainsi, m√™me si notre approche semble comp√©titive par rapport √† certaines m√©thodes r√©centes, une validation sur des jeux de donn√©es externes ou en conditions cliniques r√©elles serait n√©cessaire pour √©valuer pleinement sa robustesse et sa g√©n√©ralisabilit√©. Notre objectif n‚Äôest pas tant de surpasser les m√©thodes existantes que de proposer une solution fiable, reproductible, et adapt√©e au contexte sp√©cifique de notre √©tude."
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#conclusion",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#conclusion",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "Conclusion",
    "text": "Conclusion\n¬†¬†¬†¬†¬†¬†Les r√©sultats obtenus √† l‚Äôissue de l‚Äôentra√Ænement et de la validation du mod√®le indiquent que celui-ci :\n\nApprend efficacement les motifs caract√©ristiques des diff√©rentes classes de tumeurs c√©r√©brales √† partir des images IRM ;\nG√©n√©ralise correctement sur des donn√©es non vues, ce qui est essentiel dans une perspective d‚Äôapplication clinique ;\nNe pr√©sente pas de signe manifeste de surapprentissage, comme en t√©moigne la faible diff√©rence entre les m√©triques d‚Äôentra√Ænement et de validation (accuracy et loss).\n\nLa coh√©rence de l‚Äô√©volution des scores de perte durant l‚Äôentra√Ænement, tant sur les donn√©es d‚Äôapprentissage que de validation, confirme la stabilit√© du mod√®le et son bon ajustement au probl√®me de classification multiclasse.\nLe mod√®le est tr√®s performant pour distinguer les diff√©rentes classes de tumeurs c√©r√©brales sur les images IRM, avec tr√®s peu d‚Äôerreurs, ce qui est crucial dans un contexte clinique. Les faux positifs et faux n√©gatifs sont tr√®s faibles, ce qui minimise le risque d‚Äôerreur de diagnostic.\n\n\n\n√Ä nuancer\n\n\n\nLa tr√®s haute performance obtenue peut √™tre en partie li√©e √† la taille importante du jeu de donn√©es, qui a permis un apprentissage plus robuste. En effet, nous avons environ deux fois plus d‚Äôimages que dans certaines √©tudes comparables.\nCela rend la comparaison directe avec les r√©sultats des articles pr√©c√©dents plus d√©licate, car un jeu de donn√©es plus grand favorise g√©n√©ralement de meilleures performances, mais peut aussi cacher des variations dans la qualit√© ou la diversit√© des images.\nEnfin, malgr√© ces r√©sultats encourageants, il est essentiel de tester le mod√®le sur des donn√©es externes ind√©pendantes pour confirmer sa capacit√© √† g√©n√©raliser en conditions r√©elles.\n\n\n\n\nPerspectives\n\n\n¬†¬†¬†¬†¬†¬†Un prolongement naturel de ce travail consisterait √† explorer la localisation de la tumeur en compl√©ment de sa classification. En ce sens, l‚Äôentra√Ænement d‚Äôun mod√®le de type YOLO (You Only Look Once) pourrait permettre d‚Äôidentifier automatiquement les zones suspectes sur une IRM en encadrant pr√©cis√©ment la position de la tumeur.\nCependant, la mise en ≈ìuvre d‚Äôun tel mod√®le demanderait des ressources de calcul importantes, notamment en raison de la complexit√© des architectures de d√©tection et de la n√©cessit√© de disposer d‚Äôannotations spatiales pr√©cises (bounding boxes). Cela constitue un d√©fi technique, mais √©galement une √©tape prometteuse vers un outil d‚Äôaide au diagnostic plus complet."
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexes",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexes",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "Annexes",
    "text": "Annexes"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexes-1-data-augmentation",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexes-1-data-augmentation",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "Annexes 1 : Data augmentation",
    "text": "Annexes 1 : Data augmentation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    zoom_range=0.2,\n    horizontal_flip=True\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = train_datagen.flow_from_directory(\n    'data_ml_efficient_net/train',\n     target_size=(224, 224),\n     batch_size=32,\n     class_mode='categorical'\n)\n\nval_gen = val_datagen.flow_from_directory(\n    'data_ml_efficient_net/val',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\ntest_gen = train_gen.flow_from_directory(\n    'data_ml_efficient_net/val',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',\n    shuffle = False\n)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexe-2-construction-du-mod√®le",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexe-2-construction-du-mod√®le",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "Annexe 2 : Construction du mod√®le",
    "text": "Annexe 2 : Construction du mod√®le\n# chargement de EfficientNetB5 sans les couches de sortie\nbase_model = EfficientNetB5(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model.trainable = False  # Geler les couches du mod√®le de base\n\n# qjout des nouvelles couches\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\npredictions = Dense(3, activation='softmax')(x)  # 3 classes\n\n# cr√©ation du mod√®le complet\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n\nbase_model.trainable = True\n\n# geler les premi√®res couches pour ne pas tout r√©-entra√Æner (par exemple garder les 100 premi√®res gel√©es)\nfor layer in base_model.layers[:95]:\n    layer.trainable = False\n\n# compilation\nmodel.compile(optimizer=Adam(learning_rate=0.00001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n              \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# D√©finir les callbacks \nearly_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\ncheckpoint = ModelCheckpoint(\n    filepath='best_model.keras',\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n\n# relancement l'entra√Ænement avec les callbacks\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=70,\n    callbacks=[early_stop, checkpoint]\n)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexe-3-d√©tails-de-calculs-des-m√©triques",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexe-3-d√©tails-de-calculs-des-m√©triques",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "Annexe 3: D√©tails de calculs des m√©triques",
    "text": "Annexe 3: D√©tails de calculs des m√©triques\n\nMatrice de confusion (test)\n\n\\[\n\\begin{bmatrix}\n300 & 0 & 0 \\\\\n2 & 297 & 1 \\\\\n0 & 0 & 307 \\\\\n\\end{bmatrix}\n\\]\n\nClasse 1 (Gliome) :\n\n\\(TP = 300\\)\n\n\\(FP = 2\\) (images d‚Äôautres classes class√©es comme Gliome)\n\n\\(FN = 0\\) (images Gliome mal class√©es)\n\n\\[\n\\text{Pr√©cision} = \\frac{300}{300 + 2} = 0.9934\n\\]\n\\[\n\\text{Rappel} = \\frac{300}{300 + 0} = 1.0\n\\]\n\\[\nF1 = 2 \\times \\frac{0.9934 \\times 1.0}{0.9934 + 1.0} = 0.9967\n\\]\nClasse 2 (M√©ningiome) :\n\n\\(TP = 297\\)\n\n\\(FP = 0\\)\n\n\\(FN = 2 + 1 = 3\\)\n\n\\[\n\\text{Pr√©cision} = \\frac{297}{297 + 0} = 1.0\n\\]\n\\[\n\\text{Rappel} = \\frac{297}{297 + 3} = 0.99\n\\]\n\\[\nF1 = 2 \\times \\frac{1.0 \\times 0.99}{1.0 + 0.99} = 0.9950\n\\]\nClasse 3 (Autre tumeur) :\n\n\\(TP = 307\\)\n\n\\(FP = 1\\)\n\n\\(FN = 0\\)\n\n\\[\n\\text{Pr√©cision} = \\frac{307}{307 + 1} = 0.9968\n\\]\n\\[\n\\text{Rappel} = \\frac{307}{307 + 0} = 1.0\n\\]\n\\[\nF1 = 2 \\times \\frac{0.9968 \\times 1.0}{0.9968 + 1.0} = 0.9984\n\\]"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#moyennes-globales",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#moyennes-globales",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "Moyennes globales",
    "text": "Moyennes globales\nSupport total : \\(300 + 300 + 307 = 907\\)\n\nMacro-average (moyenne simple) :\n\n\\[\n\\text{Pr√©cision}_{macro} = \\frac{0.9934 + 1.0 + 0.9968}{3} = 0.9967\n\\]\n\\[\n\\text{Rappel}_{macro} = \\frac{1.0 + 0.99 + 1.0}{3} = 0.9967\n\\]\n\\[\nF1_{macro} = \\frac{0.9967 + 0.9950 + 0.9984}{3} = 0.9967\n\\]\n\nWeighted-average (moyenne pond√©r√©e) :\n\n\\[\n\\text{Pr√©cision}_{weighted} = \\frac{(300 \\times 0.9934) + (300 \\times 1.0) + (307 \\times 0.9968)}{907} = 0.9967\n\\]\n\\[\n\\text{Rappel}_{weighted} = \\frac{(300 \\times 1.0) + (300 \\times 0.99) + (307 \\times 1.0)}{907} = 0.9967\n\\]\n\\[\nF1_{weighted} = \\frac{(300 \\times 0.9967) + (300 \\times 0.9950) + (307 \\times 0.9984)}{907} = 0.9967\n\\]"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#liste-des-sigles-et-abr√©viations",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#liste-des-sigles-et-abr√©viations",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "LISTE DES SIGLES ET ABR√âVIATIONS",
    "text": "LISTE DES SIGLES ET ABR√âVIATIONS\n\n\n\n\n\n\n\nSigle\nSignification\n\n\n\n\nCNN\nConvolutional Neural Network (R√©seau de Neurones Convolutif)\n\n\nIRM\nImagerie par R√©sonance Magn√©tique\n\n\nOMS\nOrganisation Mondiale de la Sant√©\n\n\nViT\nVision Transformer\n\n\nReLU\nRectified Linear Unit (fonction d‚Äôactivation)\n\n\nRGB\nRouge, Vert, Bleu (canaux de couleur)\n\n\nYOLO\nYou Only Look Once"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#references-bibliographiques",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#references-bibliographiques",
    "title": "Classification des tumeurs c√©r√©brales √† partir d‚ÄôIRM : Mod√©lisation et √©valuation",
    "section": "REFERENCES BIBLIOGRAPHIQUES",
    "text": "REFERENCES BIBLIOGRAPHIQUES\nLIVRE : L‚Äôapprentissage Profond avec Python, Les meilleures pratiques de Fran√ßois Chollet (Une base en optimisation et m√©thodes de calculs num√©riques pourrait √™tre utile pour une compr√©hension moins superficielle du cont√©nu du livre)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "",
    "text": "Comment cela fonctionne\n\n\n\nJe tiens tout d‚Äôabord √† rapperler que je n‚Äôutilise pas de mod√®le NLP pour cr√©er ce petit assistant virtuel. J‚Äôavais √©cris ce progamme en 2021, donc bien evidemment les outils utilis√©s ont √©volu√© et donc vous pourrez l‚Äôajuster √† votre guise. Le code source sera t√©l√©chargeable √† la fin de la page."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#la-svm-cest-quoi-et-√†-quoi-√ßa-sert",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#la-svm-cest-quoi-et-√†-quoi-√ßa-sert",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "LA SVM : C‚Äôest quoi et √† quoi √ßa sert ?",
    "text": "LA SVM : C‚Äôest quoi et √† quoi √ßa sert ?\n\nUtilit√© :\n\n\nPrincipe de la SVM pour la classification binaire"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#test-du-code",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#test-du-code",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "Test du code",
    "text": "Test du code"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#conclusion",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#conclusion",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "Conclusion",
    "text": "Conclusion\nCe code met en place un assistant vocal capable de reconna√Ætre et d‚Äôex√©cuter des commandes vocales en fran√ßais, d‚Äôeffectuer des recherches sur le web, et de r√©pondre aux questions gr√¢ce √† Wolfram Alpha et Wikip√©dia. Il constitue une base quelque peu solide pour un assistant personnel plus ou moins intelligent.\nT√©l√©charger le fichier .python\nT√©l√©charger la vid√©o\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour √† la page d‚Äôaccueuil"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "",
    "text": "Cette √©tude pr√©sente une approche m√©thodologique combinant analyse en composantes principales (ACP), s√©lection de variables, et mod√©lisation par k plus proches voisins (KNN) ainsi que r√©gression logistique pour la classification de donn√©es m√©dicales. L‚Äôobjectif principal √©tait d‚Äôidentifier les variables les plus discriminantes et d‚Äô√©valuer la performance pr√©dictive des mod√®les. Les r√©sultats montrent que la r√©duction de dimension facilite l‚Äôinterpr√©tation, tandis que les deux mod√®les de classification offrent des performances satisfaisantes, avec un bon compromis entre pr√©cision et interpr√©tabilit√©. Cette m√©thode robuste peut √™tre √©tendue √† d‚Äôautres jeux de donn√©es similaires.\nCe projet est un exercice personnel visant √† mettre en pratique les notions acquises.\nMots-cl√©s : Analyse en composantes principales, S√©lection de variables, K plus proches voisins, R√©gression logistique, Classification, Donn√©es m√©dicales.\n\nThis study presents a methodological approach combining principal component analysis (PCA), variable selection, k-nearest neighbours (KNN) modelling and logistic regression for the classification of medical data. The main objective was to identify the most discriminating variables and to assess the predictive performance of the models. The results show that dimension reduction facilitates interpretation, while both classification models offer satisfactory performance, with a good compromise between accuracy and interpretability. This robust method can be extended to other similar datasets.\nThis project is a personal exercise to practice the concepts I have acquired.\nKeywords: Principal component analysis, Variable selection, K-nearest neighbours, Logistic regression, Classification, Medical data."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#exploration-des-donn√©es",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#exploration-des-donn√©es",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Exploration des donn√©es",
    "text": "Exploration des donn√©es\nL‚Äôexploration des donn√©es comprend :\n- Le r√©sum√© statistique descriptif des variables\n- La distribution de la variable cible\n- L‚Äô√©tude des corr√©lations entre variables\nCes √©tapes permettent de mieux comprendre la structure et les caract√©ristiques de la base avant mod√©lisation."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#description-des-variables",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#description-des-variables",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Description des variables",
    "text": "Description des variables\nLes variables de cette base sont construites √† partir de l‚Äôanalyse de noyaux de cellules d√©tect√©s dans des images m√©dicales.\n\nid (int) : Identifiant unique de l‚Äôobservation (patient).\ndiagnosis (cat√©gorielle) Variable cible binaire :\n\nM : Malignant (maligne)\nB : Benign (b√©nigne)\n\nCaract√©ristiques mesur√©es\n\nPour chaque noyau de cellule, 10 mesures statistiques ont √©t√© calcul√©es, puis la moyenne, l‚Äô√©cart-type (erreur standard, not√© se), et la valeur extr√™me (worst) ont √©t√© rapport√©s :\n\n\n\nMesures de base :\n\n\n\nCes mesures sont disponibles en 3 versions chacune : .mean, .se, .worst\n\n\n\n\n\n\n\nVariable de base\nSignification\n\n\n\n\nradius\nRayon moyen du noyau\n\n\ntexture\n√âcart-type des valeurs de niveaux de gris\n\n\nperimeter\nP√©rim√®tre du noyau\n\n\narea\nSurface du noyau\n\n\nsmoothness\nR√©gularit√© des contours (valeurs faibles = plus lisses)\n\n\ncompactness\nCompacit√© = (p√©rim√®tre¬≤ / surface) - 1.0\n\n\nconcavity\nGravit√© des concavit√©s dans les contours\n\n\nconcave points\nNombre de points concaves sur les contours\n\n\nsymmetry\nSym√©trie de la cellule\n\n\nfractal dimension\nMesure de la ‚Äúrugosit√©‚Äù des contours\n\n\n\nChaque mesure est donc d√©clin√©e en :\n\n*_mean\n*_se\n*_worst\n\nPar exemple :\n\nradius_mean, radius_se, radius_worst\ntexture_mean, texture_se, texture_worst\n‚Ä¶\nfractal_dimension_mean, fractal_dimension_se, fractal_dimension_worst\n\nCe qui donne au total 30 variables quantitatives."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#r√©sum√©-des-types-de-variables",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#r√©sum√©-des-types-de-variables",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "R√©sum√© des types de variables",
    "text": "R√©sum√© des types de variables\n\n\n\n\n\n\n\n\nType de variable\nNom\nNombre\n\n\n\n\nIdentifiant\nid\n1\n\n\nCible binaire\ndiagnosis\n1\n\n\nVariables num√©riques (√ó10 mesures √ó3 stats)\n*_mean, *_se, *_worst\n30"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#remarques",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#remarques",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Remarques",
    "text": "Remarques\n\nOn se focalisera uniquement sur les moyennes\nAucune valeur manquante n‚Äôest pr√©sente dans le jeu de donn√©es.\nLes variables sont toutes num√©riques √† l‚Äôexception de diagnosis.\nUn pr√©traitement est souvent n√©cessaire (standardisation, s√©lection de variables, etc.) avant d‚Äôentra√Æner un mod√®le."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#analyse-exploratoire",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#analyse-exploratoire",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Analyse exploratoire",
    "text": "Analyse exploratoire\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nQuelques statistiques descriptives\n\n\ndf = pd.read_csv('data.csv')\ndf.head()\n\n         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n0    842302         M  ...                  0.11890          NaN\n1    842517         M  ...                  0.08902          NaN\n2  84300903         M  ...                  0.08758          NaN\n3  84348301         M  ...                  0.17300          NaN\n4  84358402         M  ...                  0.07678          NaN\n\n[5 rows x 33 columns]\n\n\n\nprint('\\n')\ndf = df.loc[:, (df.columns.str.contains('mean')) | (df.columns == 'diagnosis')]\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 11 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   diagnosis               569 non-null    object \n 1   radius_mean             569 non-null    float64\n 2   texture_mean            569 non-null    float64\n 3   perimeter_mean          569 non-null    float64\n 4   area_mean               569 non-null    float64\n 5   smoothness_mean         569 non-null    float64\n 6   compactness_mean        569 non-null    float64\n 7   concavity_mean          569 non-null    float64\n 8   concave points_mean     569 non-null    float64\n 9   symmetry_mean           569 non-null    float64\n 10  fractal_dimension_mean  569 non-null    float64\ndtypes: float64(10), object(1)\nmemory usage: 49.0+ KB\n\n\n\ndf.isna().sum()\n\ndiagnosis                 0\nradius_mean               0\ntexture_mean              0\nperimeter_mean            0\narea_mean                 0\nsmoothness_mean           0\ncompactness_mean          0\nconcavity_mean            0\nconcave points_mean       0\nsymmetry_mean             0\nfractal_dimension_mean    0\ndtype: int64\n\n\n\nprint('\\n')\n# on saute la premi√®re colonne (id) et la derniere columns (unnamed)\ndf.iloc[:, 1:df.shape[1]].describe()\n\n       radius_mean  texture_mean  ...  symmetry_mean  fractal_dimension_mean\ncount   569.000000    569.000000  ...     569.000000              569.000000\nmean     14.127292     19.289649  ...       0.181162                0.062798\nstd       3.524049      4.301036  ...       0.027414                0.007060\nmin       6.981000      9.710000  ...       0.106000                0.049960\n25%      11.700000     16.170000  ...       0.161900                0.057700\n50%      13.370000     18.840000  ...       0.179200                0.061540\n75%      15.780000     21.800000  ...       0.195700                0.066120\nmax      28.110000     39.280000  ...       0.304000                0.097440\n\n[8 rows x 10 columns]\n\n\n¬†¬†¬†¬†¬†¬†On peut √©galement visualiser ce r√©sum√© statistique :\n\ndef plot_multiple_histograms(rows=4, cols=3, figsize=(12, 12)):\n    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n    axes = axes.flatten() \n    #total_plots = rows * cols\n    columns = df.columns\n\n    for i, col in enumerate(columns):\n        if i == 0: # faire un barplot pour la variable quat√©gorielle\n          sns.barplot(\n            x=df[col].value_counts().index,\n            y=df[col].value_counts().values, \n            ax=axes[i]\n          )\n          axes[i].set_title(str(col).capitalize(), fontsize=8)\n        sns.histplot(data=df[col], ax=axes[i])\n        axes[i].set_title(str(col).capitalize(), fontsize=8)\n        axes[i].set_xlabel(str(col), fontsize=6)\n        axes[i].set_ylabel(\"Fr√©quence\", fontsize=6)\n        axes[i].tick_params(axis='both', labelsize=6)\n\n    for j in range(len(columns), len(axes)):\n        fig.delaxes(axes[j])\n    plt.tight_layout()\n        \n    plt.show()\n      \n\n\n\nCode\nplot_multiple_histograms()\n\n\n\n\n\nDistribution des variables\n\n\n\n\n\nCorr√©logramme des variables quantitatives\n\n\n\nCode\ncorr_matrix = df.iloc[:, 1:(df.shape[1])].corr()\nplt.figure(figsize=(12, 12))\n# mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\nsns.heatmap(\n  corr_matrix, \n  annot=True, \n  fmt=\".2f\", \n  center=0,\n  linewidths=0.5,\n  cbar_kws={\"shrink\": .7},  # taille barre couleur\n  xticklabels=True,\n  yticklabels=True\n)\nplt.xticks(rotation=35, ha='right', fontsize=10)\n\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0.5, 0, 'radius_mean'), Text(1.5, 0, 'texture_mean'), Text(2.5, 0, 'perimeter_mean'), Text(3.5, 0, 'area_mean'), Text(4.5, 0, 'smoothness_mean'), Text(5.5, 0, 'compactness_mean'), Text(6.5, 0, 'concavity_mean'), Text(7.5, 0, 'concave points_mean'), Text(8.5, 0, 'symmetry_mean'), Text(9.5, 0, 'fractal_dimension_mean')])\n\n\nCode\nplt.yticks(rotation=0, fontsize=10)\n\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0, 0.5, 'radius_mean'), Text(0, 1.5, 'texture_mean'), Text(0, 2.5, 'perimeter_mean'), Text(0, 3.5, 'area_mean'), Text(0, 4.5, 'smoothness_mean'), Text(0, 5.5, 'compactness_mean'), Text(0, 6.5, 'concavity_mean'), Text(0, 7.5, 'concave points_mean'), Text(0, 8.5, 'symmetry_mean'), Text(0, 9.5, 'fractal_dimension_mean')])\n\n\nCode\nplt.subplots_adjust(bottom=0.4, left=0.3)\nplt.title(\"Corr√©logramme avec p-values\", fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nCorr√©logramme des variables quantitatives\n\n\n\n\nOn a des variables qui sont fortement corr√©l√©es et d‚Äôautres non. Toutefois nous ne pouvons pas nous prononcer sur la justification statistique du lien entres elles. Pour cela nous affichons le m√™me graphique avec les pvalues pour les associations significatives et des classes pour celles qui ne le sont pas.\n\nfrom scipy.stats import pearsonr\ncols = df.select_dtypes(include=np.number).columns  # seulement les colonnes num√©riques\nn = len(cols)\n\ncorr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)\npval_matrix = pd.DataFrame(np.ones((n, n)), columns=cols, index=cols)\n\nfor i in range(n):\n    for j in range(n):\n        if i &lt;= j:  # √©viter les doublons\n            r, p = pearsonr(df[cols[i]], df[cols[j]])\n            corr_matrix.iloc[i, j] = r\n            corr_matrix.iloc[j, i] = r\n            pval_matrix.iloc[i, j] = p\n            pval_matrix.iloc[j, i] = p\n            \n            \n# masquer les p-values non significatives\nmask_significant = pval_matrix &lt; 0.05\nannot = corr_matrix.round(2).astype(str) + \"\\np=\" + pval_matrix.round(3).astype(str)\nannot[~mask_significant] = \"\"\n\nplt.figure(figsize=(12, 12))\n\nsns.heatmap(\n  corr_matrix, \n  fmt=\"\",\n  annot=annot,\n  annot_kws={\"size\": 8},\n  center=0,\n  linewidths=0.5,\n  mask=~mask_significant,\n  cbar_kws={\"shrink\": .7},\n  xticklabels=True,\n  yticklabels=True\n)\n\nplt.xticks(rotation=35, ha='right', fontsize=9)\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0.5, 0, 'radius_mean'), Text(1.5, 0, 'texture_mean'), Text(2.5, 0, 'perimeter_mean'), Text(3.5, 0, 'area_mean'), Text(4.5, 0, 'smoothness_mean'), Text(5.5, 0, 'compactness_mean'), Text(6.5, 0, 'concavity_mean'), Text(7.5, 0, 'concave points_mean'), Text(8.5, 0, 'symmetry_mean'), Text(9.5, 0, 'fractal_dimension_mean')])\n\nplt.yticks(rotation=0, fontsize=9)\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0, 0.5, 'radius_mean'), Text(0, 1.5, 'texture_mean'), Text(0, 2.5, 'perimeter_mean'), Text(0, 3.5, 'area_mean'), Text(0, 4.5, 'smoothness_mean'), Text(0, 5.5, 'compactness_mean'), Text(0, 6.5, 'concavity_mean'), Text(0, 7.5, 'concave points_mean'), Text(0, 8.5, 'symmetry_mean'), Text(0, 9.5, 'fractal_dimension_mean')])\n\nplt.subplots_adjust(bottom=0.5, left=0.4)\nplt.title(\"Corr√©logramme : corr√©lations significatives uniquement (p &lt; 0.05)\", fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\nCorr√©logramme des variables quantitatives statistiquement significatives\n\n\n\n\n¬†¬†¬†¬†¬†¬†Sauf quelques unes ne sont pas significativement corr√©l√©es entre-elles mais le sont avec d‚Äôautres variables"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#analyse-en-composantes-principales",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#analyse-en-composantes-principales",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Analyse en composantes principales",
    "text": "Analyse en composantes principales\n¬†¬†¬†¬†¬†¬†Pour cette partie sp√©ciquement ainsi que pour la partie statistique du mod√®le logistique, nous utiliserons le langage R au lieu de python car il est plus facile √† prendre en main (avis personnel). Mais pour la partie machine learning nous utiliserons le langage Python.\nL‚Äô ACP permettra d‚Äô√©liminer les variables corr√©l√©es entre elles en ne gardant que les plus contributives √† la formation des axes que nous choisirons (pour plus de d√©tails visitez ma publication Reduction de dimensionnalit√©, clustering non supervis√©).\n\ndf_r &lt;- py$df\ndf_r$diagnosis &lt;- as.factor(df_r$diagnosis)\nstr(df_r)\n\n'data.frame':   569 obs. of  11 variables:\n $ diagnosis             : Factor w/ 2 levels \"B\",\"M\": 2 2 2 2 2 2 2 2 2 2 ...\n $ radius_mean           : num  18 20.6 19.7 11.4 20.3 ...\n $ texture_mean          : num  10.4 17.8 21.2 20.4 14.3 ...\n $ perimeter_mean        : num  122.8 132.9 130 77.6 135.1 ...\n $ area_mean             : num  1001 1326 1203 386 1297 ...\n $ smoothness_mean       : num  0.1184 0.0847 0.1096 0.1425 0.1003 ...\n $ compactness_mean      : num  0.2776 0.0786 0.1599 0.2839 0.1328 ...\n $ concavity_mean        : num  0.3001 0.0869 0.1974 0.2414 0.198 ...\n $ concave points_mean   : num  0.1471 0.0702 0.1279 0.1052 0.1043 ...\n $ symmetry_mean         : num  0.242 0.181 0.207 0.26 0.181 ...\n $ fractal_dimension_mean: num  0.0787 0.0567 0.06 0.0974 0.0588 ...\n - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=569, step=1)\n\n\n\n\nCode\nlibrary(FactoMineR) #install.packages(\"FactoMineR\")\nlibrary(factoextra) #install.packages(\"factoextra\")\nlibrary(cluster) #install.packages(\"cluster\")\n\nacp_model &lt;- PCA(df_r, quali.sup = \"diagnosis\", scale.unit = TRUE, graph = FALSE)\n\n\n\n\n\nValeurs proppres : Choix des dimensions d‚Äôanalyse\n\n\n\n\n\nCode\nfviz_eig(acp_model, geom = 'line') +\n  labs(title = \"Pourcentages des variances expliqu√©es par les composantes principales\",\n       y = \"Pourcentage d'inertie\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des variances expliqu√©es par les composantes principales\n\n\n\n\n¬†¬†¬†¬†¬†¬†On observe le coude √† partie de la troisi√®me dimension. Mais en se basant sur le crit√®re du taux d‚Äôinertie on a environ 80% de l‚Äôinformation cont√©nue dans les donn√©es. Par cons√©quent notre analyse sera ax√©e sur les deux premiers axes.\n\n\n\nAnalyses des variables\n\n\n\n\n\nCode\nfviz_pca_var(acp_model, col.var = \"contrib\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE) +\n  labs(title = \"Cercle de corr√©lation des variables\",\n       y = \"Dimension 2\", x = \"Dimension 1\") +\n  theme_light()\n\n\n\n\n\nCartes de la representation des variables sur les dimensions 1 et 2\n\n\n\n\n¬†¬†¬†¬†¬†¬†On voit que :"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#les-kmeans",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#les-kmeans",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Les Kmeans",
    "text": "Les Kmeans\n\n\nCode\nacp_model &lt;- PCA(\n  df_r[, c(\"diagnosis\", \"perimeter_mean\", \"smoothness_mean\", \"fractal_dimension_mean\", \"concave points_mean\")],\n  quali.sup = \"diagnosis\", \n  scale.unit = TRUE,\n  graph = FALSE\n)\n#| label: inertie-val\n#| code-fold: true\nfviz_eig(acp_model, geom = 'line') +\n  labs(title = \"Pourcentages des variances expliqu√©es par les composantes principales\",\n       y = \"Pourcentage d'inertie\", x = \"Composantes principales\")\n\n\n\n\n\n¬†¬†¬†¬†¬†¬†Pareil, on choisit les deux premi√®res dimensions (+ de 80% de la variance expliqu√©es).\nPour faire les Kmeans, nous prendrons 2 comme nombre de clusters.\n\ndataTocluster &lt;- scale(acp_model$ind$coord[,1:2])\nresKmeans &lt;- kmeans(dataTocluster, 2, nstart = 50)\nstr(resKmeans)\n\nList of 9\n $ cluster     : Named int [1:569] 1 1 1 2 1 2 1 2 1 2 ...\n  ..- attr(*, \"names\")= chr [1:569] \"1\" \"2\" \"3\" \"4\" ...\n $ centers     : num [1:2, 1:2] 1.118 -0.48 -0.624 0.268\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"1\" \"2\"\n  .. ..$ : chr [1:2] \"Dim.1\" \"Dim.2\"\n $ totss       : num 1136\n $ withinss    : num [1:2] 255 480\n $ tot.withinss: num 735\n $ betweenss   : num 401\n $ size        : int [1:2] 171 398\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\n\n\nCode\ntable(resKmeans$cluster)\n\n\n\n  1   2 \n171 398 \n\n\n\n# attribuer les noms au clusters\ntable(Cluster = resKmeans$cluster, Diagnosis = df_r$diagnosis)\n\n       Diagnosis\nCluster   B   M\n      1   8 163\n      2 349  49\n\n\n¬†¬†¬†¬†¬†¬†On voit clairement que :\n\nCluster 1 contient surtout des cas M (163 M vs 8 B) ‚Üí Cluster 1 ‚âà ‚ÄúMaligne‚Äù\nCluster 2 contient surtout des cas B (349 B vs 49 M) ‚Üí Cluster 2 ‚âà ‚ÄúB√©nigne‚Äù\n\n\n\n\nRepr√©sentations graphiques\n\n\n\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = factor(\n    ifelse(resKmeans$cluster == 1, \"Maligne\", \"B√©nigne\"),\n    levels = c(\"B√©nigne\", \"Maligne\")\n    ), diagnosis = df_r$diagnosis)\n\nggplot(dataTocluster, aes (x = Dim.1, y = Dim.2, color = classe)) +\n  geom_point() +\n  labs(\n    title = \"Classification des patients (Axe 1 - Axe 2)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 2\",\n    color = \"Classe/Cluster\"\n  )+\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs donn√©es socio-√©conomiques\n\n\n\n\nInterpretation :\n¬†¬†¬†¬†¬†¬†On observe que les patients sont pratiquement lin√©airement s√©parables dans le plan d√©fini par les deux premi√®res composantes principales.\nCela signifie que la projection sur ces deux dimensions met bien en √©vidence une s√©paration claire entre les deux groupes (b√©nins et malins), ce qui est coh√©rent avec la qualit√© du clustering obtenu par k-means.\nCette bonne s√©paration visuelle corrobore la pertinence des variables s√©lectionn√©es et confirme que les composantes principales r√©sument efficacement la variance utile √† la distinction des diagnostics. Nous pouvons passer √† pr√©sent aux diff√©rents mod√®les de pr√©diction (regression logistique et k plus plus proches voisins)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#logit-vs-knn",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#logit-vs-knn",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Logit vs KNN",
    "text": "Logit vs KNN\n\n\nCode\n# Importation des librairies n√©cessaires\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n\n# Chargement des donn√©es\n# Supposons que df est votre DataFrame et 'diagnosis' la variable cible binaire (B=0, M=1)\n# Par exemple, vous pouvez charger le dataset breast cancer from sklearn (exemple)\nfrom sklearn.datasets import load_breast_cancer\ndf = df.loc[:, [\"diagnosis\", \"perimeter_mean\", \"smoothness_mean\", \"fractal_dimension_mean\", \"concave points_mean\"]]\n\n# Variables explicatives et cible\nX = df.drop(columns=['diagnosis'])\ny = df['diagnosis']\n\n# S√©paration train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Mise √† l‚Äô√©chelle des variables\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# --- Mod√®le 1 : R√©gression Logistique ---\nlogit = LogisticRegression(random_state=42, max_iter=1000)\nlogit.fit(X_train_scaled, y_train)\n\n\nLogisticRegression(max_iter=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000, random_state=42)\n\n\nCode\ny_pred_logit = logit.predict(X_test_scaled)\ny_proba_logit = logit.predict_proba(X_test_scaled)[:,1]\n\n# --- Mod√®le 2 : KNN ---\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\nCode\ny_pred_knn = knn.predict(X_test_scaled)\ny_proba_knn = knn.predict_proba(X_test_scaled)[:,1]\n\n\n\n\nMatrices de confusion\n\n\n\n\nCode\ncm_logit = confusion_matrix(y_test, y_pred_logit)\ncm_knn = confusion_matrix(y_test, y_pred_knn)\n\n# Fonction pour tracer la matrice de confusion\ndef plot_cm(cm, title):\n    plt.figure(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['B√©nin (0)', 'Malin (1)'],\n                yticklabels=['B√©nin (0)', 'Malin (1)'])\n    plt.ylabel('Vraies √©tiquettes')\n    plt.xlabel('Pr√©dictions')\n    plt.title(title)\n    plt.show()\n\nplot_cm(cm_logit, \"Matrice de confusion - R√©gression Logistique\")\nplot_cm(cm_knn, \"Matrice de confusion - KNN\")\n\n\n\n\n\n\n\nMatrice de confusion - R√©gression Logistique\n\n\n\n\n\n\n\nMatrice de confusion - KNN\n\n\n\n\n\n\nComparaison des ML (Logit vs KNN - Matrices de confusion)\n\n\n\n\n¬†¬†¬†¬†¬†¬†On peut voir que le KNN pr√©dit mieux sur la base de la matrice de confusion car on a :\n\n104 Vrais n√©gatifs contre 102 pour le mod√®le logit\n3 Faux positifs contre 5 pour le mod√®le logit\n\n\n\nPr√©cision, recall, score F1, AUC\n\n\n\n\nCode\ndef eval_classif(y_true, y_pred, y_proba, model_name=\"Mod√®le\"):\n    print(f\"== R√©sultats pour {model_name} ==\")\n    print(\"\\nRapport de classification :\")\n    print(classification_report(y_true, y_pred, digits=4))\n    auc = roc_auc_score(y_true, y_proba)\n    print(f\"AUC ROC : {auc:.4f}\\n\")\n\neval_classif(y_test, y_pred_logit, y_proba_logit, \"R√©gression Logistique\")\n\n\n== R√©sultats pour R√©gression Logistique ==\n\nRapport de classification :\n              precision    recall  f1-score   support\n\n           B     0.9182    0.9439    0.9309       107\n           M     0.9016    0.8594    0.8800        64\n\n    accuracy                         0.9123       171\n   macro avg     0.9099    0.9017    0.9054       171\nweighted avg     0.9120    0.9123    0.9118       171\n\nAUC ROC : 0.9819\n\n\nCode\neval_classif(y_test, y_pred_knn, y_proba_knn, \"KNN\")\n\n\n== R√©sultats pour KNN ==\n\nRapport de classification :\n              precision    recall  f1-score   support\n\n           B     0.9346    0.9346    0.9346       107\n           M     0.8906    0.8906    0.8906        64\n\n    accuracy                         0.9181       171\n   macro avg     0.9126    0.9126    0.9126       171\nweighted avg     0.9181    0.9181    0.9181       171\n\nAUC ROC : 0.9632\n\n\n\nR√©sultats pour la R√©gression Logistique\nPr√©cision globale (accuracy) : 91.23%, ce qui signifie que le mod√®le classe correctement environ 91 patients sur 100.\n\nPr√©cision par classe :\n\nPour la classe b√©nigne (\\(B\\)), la pr√©cision est de 91.82%, indiquant une bonne d√©tection des cas b√©nins.\n\nPour la classe maligne (\\(M\\)), la pr√©cision est de 90.16%, √©galement satisfaisante.\n\n\nRappel (sensibilit√©) :\n\nClasse b√©nigne : 94.39%, montrant une bonne capacit√© √† d√©tecter les vrais positifs b√©nins.\n\nClasse maligne : 85.94%, un peu moins √©lev√©e, mais correcte.\n\n\nF1-score : 93.09% pour la classe b√©nigne et 88.00% pour la classe maligne, indiquant un bon √©quilibre global entre pr√©cision et rappel.\n\nAUC ROC : 0.9819, proche de 1, ce qui montre une excellente capacit√© de discrimination.\n\n\n\nR√©sultats pour le KNN (K plus proches voisins)\nPr√©cision globale (accuracy) : 91.81%, l√©g√®rement meilleure que la r√©gression logistique.\n\nPr√©cision par classe :\n\nPour la classe b√©nigne (\\(B\\)), la pr√©cision est de 93.46%, meilleure que la r√©gression logistique.\n\nPour la classe maligne (\\(M\\)), la pr√©cision est de 89.06%, l√©g√®rement inf√©rieure √† la r√©gression logistique.\n\n\nRappel (sensibilit√©) :\n\nClasse b√©nigne : 93.46%, un peu plus faible que le rappel du mod√®le logit.\n\nClasse maligne : 89.06%, meilleure que celui de la r√©gression logistique.\n\n\nF1-score : 93.46% pour la classe b√©nigne et 89.06% pour la classe maligne, un peu sup√©rieur √† la r√©gression logistique.\n\nAUC ROC : 0.9632, tr√®s √©lev√©, mais un peu inf√©rieur √† celui de la r√©gression logistique.\n\n\nConclusion : \nLes deux mod√®les pr√©sentent des performances comparables pour diff√©rencier les patients b√©nins et malins :\n\nLe KNN obtient une meilleure pr√©cision globale et un meilleur √©quilibre F1-score, notamment sur la classe b√©nigne.\nLa r√©gression logistique offre un rappel plus √©lev√© pour la classe b√©nigne et une meilleure AUC ROC, signe d‚Äôune tr√®s bonne capacit√© de discrimination globale.\n\nLe choix entre ces deux m√©thodes d√©pendra principalement :\n- De la n√©cessit√© d‚Äôune meilleure interpr√©tabilit√© et d‚Äôune m√©trique AUC sup√©rieure (avantage r√©gression logistique),\n- Ou d‚Äôune pr√©cision globale l√©g√®rement meilleure (avantage KNN)."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#annexe-1-hypoth√®ses-et-interpr√©tations-des-tests-statistiques",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#annexe-1-hypoth√®ses-et-interpr√©tations-des-tests-statistiques",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Annexe 1 : Hypoth√®ses et interpr√©tations des tests statistiques",
    "text": "Annexe 1 : Hypoth√®ses et interpr√©tations des tests statistiques\n\nTest de Shapiro-Wilk\nCe test permet de v√©rifier la normalit√© d‚Äôune distribution.\n\nHypoth√®ses : \\[\n\\begin{cases}\nH_0 : \\text{Les donn√©es suivent une loi normale} \\\\\nH_1 : \\text{Les donn√©es ne suivent pas une loi normale}\n\\end{cases}\n\\]\nInterpr√©tation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les donn√©es peuvent √™tre consid√©r√©es comme normales.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) les donn√©es ne sont pas normales.\n\n\n\n\n\nTest de Levene\nCe test permet de v√©rifier l‚Äôhomog√©n√©it√© des variances entre les groupes.\n\nHypoth√®ses : \\[\n\\begin{cases}\nH_0 : \\text{Les variances des groupes sont √©gales} \\\\\nH_1 : \\text{Les variances des groupes sont diff√©rentes}\n\\end{cases}\n\\]\nInterpr√©tation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les variances sont homog√®nes.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) les variances sont diff√©rentes.\n\n\n\n\n\nTest d‚ÄôANOVA classique\nCe test compare les moyennes de plusieurs groupes. Il n√©cessite que les donn√©es soient normales et que les variances soient homog√®nes.\n\nHypoth√®ses : \\[\n\\begin{cases}\nH_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_1 : \\exists \\, i \\ne j \\text{ tel que } \\mu_i \\ne \\mu_j\n\\end{cases}\n\\]\nInterpr√©tation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les moyennes sont statistiquement √©gales.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) au moins une moyenne est diff√©rente.\n\n\n\n\n\nTest d‚ÄôANOVA de Welch\nCe test est une version robuste de l‚ÄôANOVA utilis√©e lorsque l‚Äôhomog√©n√©it√© des variances n‚Äôest pas respect√©e, mais que les donn√©es restent normales.\n\nHypoth√®ses : \\[\n\\begin{cases}\nH_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_1 : \\exists \\, i \\ne j \\text{ tel que } \\mu_i \\ne \\mu_j\n\\end{cases}\n\\]\nInterpr√©tation de la \\(p\\)-valeur : identique √† celle du test ANOVA classique.\n\n\n\n\nTest de Kruskal-Wallis\nTest non param√©trique utilis√© en cas de non-normalit√© ou lorsque les donn√©es sont ordinales.\n\nHypoth√®ses : \\[\n\\begin{cases}\nH_0 : \\text{Les distributions des groupes sont identiques} \\\\\nH_1 : \\text{Au moins une distribution est diff√©rente}\n\\end{cases}\n\\]\nInterpr√©tation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les distributions sont consid√©r√©es comme similaires.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) au moins une des distributions diff√®re significativement.\n\n\n\nRemarque : Tous ces tests renvoient une \\(p\\)-valeur qui est compar√©e au seuil de signification habituellement fix√© √† 5% (\\(\\alpha = 0.05\\))."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#annexe-2",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#annexe-2",
    "title": "Optimisation de la classification m√©dicale par int√©gration de techniques statistiques et machine learning",
    "section": "Annexe 2:",
    "text": "Annexe 2:\nLe rapport de corr√©lation \\(\\eta^2\\) est une mesure de l‚Äôeffet qui quantifie la proportion de la variance expliqu√©e par un facteur.\nIl est d√©fini par la formule suivante :\n\\[\n\\eta^2 = \\frac{\\sum_{i=1}^{k} n_i (\\bar{y}_i - \\bar{y})^2}{\\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y})^2}\n\\]\no√π :\n\n\\(k\\) est le nombre de groupes,\n\\(n_i\\) est la taille du groupe \\(i\\),\n\\(\\bar{y}_i\\) est la moyenne du groupe \\(i\\),\n\\(\\bar{y}\\) est la moyenne globale,\n\\(y_{ij}\\) est l‚Äôobservation \\(j\\) du groupe \\(i\\).\n\nCette mesure permet d‚Äô√©valuer l‚Äôampleur de la diff√©rence entre les groupes, en indiquant la proportion de la variance totale attribuable √† la variation entre groupes."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "",
    "text": "KedjenouXpress est une application de gestion de ventes con√ßue pour simplifier les op√©rations commerciales dans la restauration. Elle a √©t√© d√©velopp√©e en 2023 pour remplacer les m√©thodes manuelles et offrir une solution num√©rique rapide, intuitive et efficace.\nL‚Äôinterface utilisateur b√©n√©ficie d‚Äôun design moderne et agr√©able, gr√¢ce √† l‚Äôutilisation de styles personnalis√©s en CSS pour am√©liorer l‚Äôexp√©rience visuelle et l‚Äôergonomie.\nSkills : Java, JavFx, MySql, Mod√©lisation UML, POO\n\n\n\nLes outils et technologies suivants ont √©t√© utilis√©s pour d√©velopper le logiciel :\n\nJava (version 8.2) : choisi pour sa stabilit√© et sa compatibilit√© avec le d√©veloppement d‚Äôapplications de bureau.\nNetBeans : utilis√© comme environnement de d√©veloppement int√©gr√© (IDE) pour sa simplicit√© et sa bonne int√©gration avec Java.\nWAMP : utilis√© pour h√©berger localement la base de donn√©es et assurer la communication entre l‚Äôapplication et les donn√©es, avec MySQL comme syst√®me de gestion de base de donn√©es relationnelle.\n\n\n\n\n\n\nJava (JDK 8) : T√©l√©charger Java JDK (version 8)\n\nNetBeans IDE : T√©l√©charger NetBeans\n\nWAMP Server : T√©l√©charger WAMP\n\n\n\n\n\nWAMP Server (Windows, Apache, MySQL, PHP) est un environnement de d√©veloppement web local. Avant de l‚Äôinstaller, il est important de s‚Äôassurer que certaines d√©pendances logicielles sont pr√©sentes sur votre syst√®me.\n\n1. D√©pendances √† installer\n\nWAMP Server n√©cessite plusieurs versions du Microsoft Visual C++ Redistributable, indispensables au bon fonctionnement d‚ÄôApache, MySQL et PHP. Ces biblioth√®ques sont parfois install√©es automatiquement, mais il est recommand√© de les v√©rifier avant.\n\n\n\nVersions requises les plus courantes\n\n\n\n\nVisual C++ 2008 (x86 et x64)\nVisual C++ 2010 (x86 et x64)\nVisual C++ 2012 (x86 et x64)\nVisual C++ 2013 (x86 et x64)\nVisual C++ 2015-2022 (x86 et x64)\n\nüîó T√©l√©charger toutes les versions n√©cessaires ici :\nhttps://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist\n\n2. T√©l√©chargement de WAMP\nAcc√©dez au site officiel : https://www.wampserver.com\nT√©l√©chargez la version correspondant √† votre syst√®me (32 bits ou 64 bits).\nEnregistrez le fichier .exe sur votre ordinateur.\n3. Installation\nLancez le fichier t√©l√©charg√© (clic droit &gt; Ex√©cuter en tant qu‚Äôadministrateur).\nSuivez les instructions de l‚Äôassistant d‚Äôinstallation :\n\nAcceptez le contrat de licence.\nChoisissez le r√©pertoire d‚Äôinstallation (par d√©faut : C:\\wamp64\\).\nS√©lectionnez votre navigateur par d√©faut si demand√©.\nChoisissez √©galement votre √©diteur de texte pr√©f√©r√© (Notepad++ par exemple).\n\nTerminez l‚Äôinstallation.\n4. D√©marrage de WAMP\nOuvrez WAMP Server via le menu D√©marrer.\nUne ic√¥ne appara√Æt dans la barre des t√¢ches :\n\nüü¢ Vert : tous les services fonctionnent.\nüü° Orange : un ou plusieurs services sont arr√™t√©s.\nüî¥ Rouge : aucun service ne fonctionne.\n\n5. V√©rification\nCliquez sur l‚Äôic√¥ne WAMP &gt; ‚ÄúLocalhost‚Äù : une page de bienvenue doit s‚Äôafficher.\nVous pouvez acc√©der √† phpMyAdmin pour cr√©er et g√©rer vos bases de donn√©es MySQL.\n\n\n\n\n\n¬†¬†¬†¬†¬†¬†L‚Äôarchitecture adopt√©e pour cette application repose sur le mod√®le MVC (Mod√®le-Vue-Contr√¥leur), un patron de conception logiciel qui vise √† s√©parer clairement les responsabilit√©s dans une application, particuli√®rement celles disposant d‚Äôune interface utilisateur graphique.\nCe d√©coupage permet d‚Äôobtenir un code plus lisible, maintenable, et facilement √©volutif. Cela facilite √©galement le travail collaboratif en isolant les t√¢ches : un d√©veloppeur peut travailler sur la logique m√©tier (mod√®le) pendant qu‚Äôun autre se concentre sur l‚Äôinterface (vue).\nLe mod√®le MVC est compos√© de trois √©l√©ments fondamentaux :\n\n1. Mod√®le (Model)\n\n¬†¬†¬†¬†¬†¬†Le mod√®le contient les donn√©es de l‚Äôapplication ainsi que la logique m√©tier. Il est responsable de la cr√©ation, la mise √† jour et la validation des donn√©es. Il ne s‚Äôoccupe jamais de l‚Äôaffichage.\nExemple\npublic class Personne {\n    private String nom;\n    private int age;\n\n    // Constructeur\n    public Personne(String nom, int age) {\n        this.nom = nom;\n        this.age = age;\n    }\n\n    // Accesseurs (getters)\n    public String getNom() {\n        return nom;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    // Modificateurs (setters)\n    public void setNom(String nom) {\n        this.nom = nom;\n    }\n\n    public void setAge(int age) {\n        if (age &gt;= 0) {\n            this.age = age;\n        } else {\n            System.out.println(\"L'√¢ge doit √™tre positif.\");\n        }\n    }\n\n    // M√©thode d'affichage (optionnelle)\n    public void afficherInfos() {\n        System.out.println(\"Nom : \" + nom + \", √Çge : \" + age);\n    }\n}\n\n2. Vue (View)\n\n¬†¬†¬†¬†¬†¬†La vue est responsable de l‚Äôaffichage des donn√©es √† l‚Äôutilisateur. Elle ne contient aucune logique m√©tier. Son objectif est uniquement de pr√©senter visuellement les informations issues du mod√®le et de transmettre les actions de l‚Äôutilisateur au contr√¥leur.\nExemple FXML (JavaFX) d‚Äôune vue simple avec un bouton :\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n&lt;?import javafx.scene.control.*?&gt;\n&lt;?import javafx.scene.layout.*?&gt;\n\n&lt;AnchorPane xmlns:fx=\"http://javafx.com/fxml\" fx:controller=\"monappli.controllers.PersonneController\"&gt;\n    &lt;children&gt;\n        &lt;Button text=\"Afficher les infos\" layoutX=\"100\" layoutY=\"80\" onAction=\"#afficherInfos\"/&gt;\n    &lt;/children&gt;\n&lt;/AnchorPane&gt;\nCe fichier FXML d√©finit une fen√™tre avec un bouton qui, lorsqu‚Äôil est cliqu√©, d√©clenche une m√©thode du contr√¥leur (nomm√©e afficherInfos).\n\n3. Contr√¥leur (Controller)\n\n¬†¬†¬†¬†¬†¬†Le contr√¥leur fait le lien entre la vue et le mod√®le. Il :\n\nintercepte les actions de l‚Äôutilisateur (clics, saisies, etc.),\nmet √† jour le mod√®le en fonction des actions,\ndemande √† la vue de s‚Äôactualiser.\n\nExemple de classe contr√¥leur JavaFX associ√©e √† la vue pr√©c√©dente :\npackage monappli.controllers;\n\nimport javafx.event.ActionEvent;\nimport javafx.fxml.FXML;\nimport monappli.models.Personne;\n\npublic class PersonneController {\n    private Personne personne;\n\n    public PersonneController() {\n        // Cr√©ation d'un objet Personne par d√©faut\n        this.personne = new Personne(\"Jean\", 28);\n    }\n\n    @FXML\n    public void afficherInfos(ActionEvent event) {\n        personne.afficherInfos();\n    }\n}\n\n\n\n\n¬†¬†¬†¬†¬†¬†Pour la gestion des donn√©es, le projet s‚Äôappuie sur MySQL, un syst√®me de gestion de base de donn√©es relationnelle (SGBDR) largement utilis√© dans les applications professionnelles. L‚Äôinteraction entre l‚Äôapplication Java et la base de donn√©es a √©t√© rendue possible gr√¢ce au package mysql-connector-java, t√©l√©charg√© puis int√©gr√© comme d√©pendance dans le projet.\nCela a permis de stocker localement les donn√©es, d‚Äôassurer leur persistence et d‚Äôy acc√©der efficacement via des requ√™tes SQL.\n\n\nRequ√™tes SQL utilis√©es\n\n\nPlusieurs types de requ√™tes ont √©t√© impl√©ment√©s dans le cadre de ce projet, notamment :\n\nINSERT INTO : pour l‚Äôinsertion de nouvelles donn√©es (ex. : ajout d‚Äôun produit ou d‚Äôun utilisateur) ;\nSELECT : pour la r√©cup√©ration et l‚Äôaffichage des donn√©es (liste des ventes, employ√©s, inventaire, etc.) ;\nUPDATE : pour la mise √† jour d‚Äôenregistrements (ex. : modifier un produit) ;\nDELETE : pour la suppression de donn√©es obsol√®tes ou incorrectes.\n\n\n\nOptimisation par jointures\n\n\n¬†¬†¬†¬†¬†¬†Des jointures (JOIN) ont √©galement √©t√© utilis√©es pour relier plusieurs tables par exemple, les ventes, les utilisateurs et les produits, afin de produire des rapports d√©taill√©s, et d‚Äôoptimiser les filtres et recherches complexes.\n\nRemarque : Cette architecture relationnelle a √©t√© choisie pour sa fiabilit√©, sa performance et sa compatibilit√© avec les outils de d√©veloppement Java."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#technologies-utilis√©es",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#technologies-utilis√©es",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "",
    "text": "Les outils et technologies suivants ont √©t√© utilis√©s pour d√©velopper le logiciel :\n\nJava (version 8.2) : choisi pour sa stabilit√© et sa compatibilit√© avec le d√©veloppement d‚Äôapplications de bureau.\nNetBeans : utilis√© comme environnement de d√©veloppement int√©gr√© (IDE) pour sa simplicit√© et sa bonne int√©gration avec Java.\nWAMP : utilis√© pour h√©berger localement la base de donn√©es et assurer la communication entre l‚Äôapplication et les donn√©es, avec MySQL comme syst√®me de gestion de base de donn√©es relationnelle."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#liens-de-t√©l√©chargement",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#liens-de-t√©l√©chargement",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "",
    "text": "Java (JDK 8) : T√©l√©charger Java JDK (version 8)\n\nNetBeans IDE : T√©l√©charger NetBeans\n\nWAMP Server : T√©l√©charger WAMP"
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#installation-de-wamp-server",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#installation-de-wamp-server",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "",
    "text": "WAMP Server (Windows, Apache, MySQL, PHP) est un environnement de d√©veloppement web local. Avant de l‚Äôinstaller, il est important de s‚Äôassurer que certaines d√©pendances logicielles sont pr√©sentes sur votre syst√®me.\n\n1. D√©pendances √† installer\n\nWAMP Server n√©cessite plusieurs versions du Microsoft Visual C++ Redistributable, indispensables au bon fonctionnement d‚ÄôApache, MySQL et PHP. Ces biblioth√®ques sont parfois install√©es automatiquement, mais il est recommand√© de les v√©rifier avant.\n\n\n\nVersions requises les plus courantes\n\n\n\n\nVisual C++ 2008 (x86 et x64)\nVisual C++ 2010 (x86 et x64)\nVisual C++ 2012 (x86 et x64)\nVisual C++ 2013 (x86 et x64)\nVisual C++ 2015-2022 (x86 et x64)\n\nüîó T√©l√©charger toutes les versions n√©cessaires ici :\nhttps://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist\n\n2. T√©l√©chargement de WAMP\nAcc√©dez au site officiel : https://www.wampserver.com\nT√©l√©chargez la version correspondant √† votre syst√®me (32 bits ou 64 bits).\nEnregistrez le fichier .exe sur votre ordinateur.\n3. Installation\nLancez le fichier t√©l√©charg√© (clic droit &gt; Ex√©cuter en tant qu‚Äôadministrateur).\nSuivez les instructions de l‚Äôassistant d‚Äôinstallation :\n\nAcceptez le contrat de licence.\nChoisissez le r√©pertoire d‚Äôinstallation (par d√©faut : C:\\wamp64\\).\nS√©lectionnez votre navigateur par d√©faut si demand√©.\nChoisissez √©galement votre √©diteur de texte pr√©f√©r√© (Notepad++ par exemple).\n\nTerminez l‚Äôinstallation.\n4. D√©marrage de WAMP\nOuvrez WAMP Server via le menu D√©marrer.\nUne ic√¥ne appara√Æt dans la barre des t√¢ches :\n\nüü¢ Vert : tous les services fonctionnent.\nüü° Orange : un ou plusieurs services sont arr√™t√©s.\nüî¥ Rouge : aucun service ne fonctionne.\n\n5. V√©rification\nCliquez sur l‚Äôic√¥ne WAMP &gt; ‚ÄúLocalhost‚Äù : une page de bienvenue doit s‚Äôafficher.\nVous pouvez acc√©der √† phpMyAdmin pour cr√©er et g√©rer vos bases de donn√©es MySQL."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#architecture-de-lapplication-mod√®le-mvc",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#architecture-de-lapplication-mod√®le-mvc",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "",
    "text": "L‚Äôarchitecture adopt√©e pour cette application repose sur le mod√®le MVC (Mod√®le-Vue-Contr√¥leur), un patron de conception logiciel qui vise √† s√©parer clairement les responsabilit√©s dans une application, particuli√®rement celles disposant d‚Äôune interface utilisateur graphique.\nCe d√©coupage permet d‚Äôobtenir un code plus lisible, maintenable, et facilement √©volutif. Cela facilite √©galement le travail collaboratif en isolant les t√¢ches : un d√©veloppeur peut travailler sur la logique m√©tier (mod√®le) pendant qu‚Äôun autre se concentre sur l‚Äôinterface (vue).\nLe mod√®le MVC est compos√© de trois √©l√©ments fondamentaux :\n\n1. Mod√®le (Model)\n\n¬†¬†¬†¬†¬†¬†Le mod√®le contient les donn√©es de l‚Äôapplication ainsi que la logique m√©tier. Il est responsable de la cr√©ation, la mise √† jour et la validation des donn√©es. Il ne s‚Äôoccupe jamais de l‚Äôaffichage.\nExemple\npublic class Personne {\n    private String nom;\n    private int age;\n\n    // Constructeur\n    public Personne(String nom, int age) {\n        this.nom = nom;\n        this.age = age;\n    }\n\n    // Accesseurs (getters)\n    public String getNom() {\n        return nom;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    // Modificateurs (setters)\n    public void setNom(String nom) {\n        this.nom = nom;\n    }\n\n    public void setAge(int age) {\n        if (age &gt;= 0) {\n            this.age = age;\n        } else {\n            System.out.println(\"L'√¢ge doit √™tre positif.\");\n        }\n    }\n\n    // M√©thode d'affichage (optionnelle)\n    public void afficherInfos() {\n        System.out.println(\"Nom : \" + nom + \", √Çge : \" + age);\n    }\n}\n\n2. Vue (View)\n\n¬†¬†¬†¬†¬†¬†La vue est responsable de l‚Äôaffichage des donn√©es √† l‚Äôutilisateur. Elle ne contient aucune logique m√©tier. Son objectif est uniquement de pr√©senter visuellement les informations issues du mod√®le et de transmettre les actions de l‚Äôutilisateur au contr√¥leur.\nExemple FXML (JavaFX) d‚Äôune vue simple avec un bouton :\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n&lt;?import javafx.scene.control.*?&gt;\n&lt;?import javafx.scene.layout.*?&gt;\n\n&lt;AnchorPane xmlns:fx=\"http://javafx.com/fxml\" fx:controller=\"monappli.controllers.PersonneController\"&gt;\n    &lt;children&gt;\n        &lt;Button text=\"Afficher les infos\" layoutX=\"100\" layoutY=\"80\" onAction=\"#afficherInfos\"/&gt;\n    &lt;/children&gt;\n&lt;/AnchorPane&gt;\nCe fichier FXML d√©finit une fen√™tre avec un bouton qui, lorsqu‚Äôil est cliqu√©, d√©clenche une m√©thode du contr√¥leur (nomm√©e afficherInfos).\n\n3. Contr√¥leur (Controller)\n\n¬†¬†¬†¬†¬†¬†Le contr√¥leur fait le lien entre la vue et le mod√®le. Il :\n\nintercepte les actions de l‚Äôutilisateur (clics, saisies, etc.),\nmet √† jour le mod√®le en fonction des actions,\ndemande √† la vue de s‚Äôactualiser.\n\nExemple de classe contr√¥leur JavaFX associ√©e √† la vue pr√©c√©dente :\npackage monappli.controllers;\n\nimport javafx.event.ActionEvent;\nimport javafx.fxml.FXML;\nimport monappli.models.Personne;\n\npublic class PersonneController {\n    private Personne personne;\n\n    public PersonneController() {\n        // Cr√©ation d'un objet Personne par d√©faut\n        this.personne = new Personne(\"Jean\", 28);\n    }\n\n    @FXML\n    public void afficherInfos(ActionEvent event) {\n        personne.afficherInfos();\n    }\n}"
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#gestion-de-la-base-de-donn√©es",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#gestion-de-la-base-de-donn√©es",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "",
    "text": "Pour la gestion des donn√©es, le projet s‚Äôappuie sur MySQL, un syst√®me de gestion de base de donn√©es relationnelle (SGBDR) largement utilis√© dans les applications professionnelles. L‚Äôinteraction entre l‚Äôapplication Java et la base de donn√©es a √©t√© rendue possible gr√¢ce au package mysql-connector-java, t√©l√©charg√© puis int√©gr√© comme d√©pendance dans le projet.\nCela a permis de stocker localement les donn√©es, d‚Äôassurer leur persistence et d‚Äôy acc√©der efficacement via des requ√™tes SQL.\n\n\nRequ√™tes SQL utilis√©es\n\n\nPlusieurs types de requ√™tes ont √©t√© impl√©ment√©s dans le cadre de ce projet, notamment :\n\nINSERT INTO : pour l‚Äôinsertion de nouvelles donn√©es (ex. : ajout d‚Äôun produit ou d‚Äôun utilisateur) ;\nSELECT : pour la r√©cup√©ration et l‚Äôaffichage des donn√©es (liste des ventes, employ√©s, inventaire, etc.) ;\nUPDATE : pour la mise √† jour d‚Äôenregistrements (ex. : modifier un produit) ;\nDELETE : pour la suppression de donn√©es obsol√®tes ou incorrectes.\n\n\n\nOptimisation par jointures\n\n\n¬†¬†¬†¬†¬†¬†Des jointures (JOIN) ont √©galement √©t√© utilis√©es pour relier plusieurs tables par exemple, les ventes, les utilisateurs et les produits, afin de produire des rapports d√©taill√©s, et d‚Äôoptimiser les filtres et recherches complexes.\n\nRemarque : Cette architecture relationnelle a √©t√© choisie pour sa fiabilit√©, sa performance et sa compatibilit√© avec les outils de d√©veloppement Java."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#demarrage-de-lapplication",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#demarrage-de-lapplication",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "Demarrage de l‚Äôapplication",
    "text": "Demarrage de l‚Äôapplication\n¬†¬†¬†¬†¬†¬†Apr√®s la compilation et la transformation en application ex√©cutable, les d√©pendances n√©cessaires notamment Java 8.2 pour le bon fonctionnement des packages requis, ainsi que le serveur WAMP pour la gestion des bases de donn√©es doivent √™tre install√©es sur la machine r√©ceptrice. Une fois ces √©tapes termin√©es, un raccourci de l‚Äôapplication est cr√©√© et plac√© sur le bureau.\n\n\n\n\n\nIcone de l‚Äô application\n\n\n\n\nPour d√©marrer l‚Äôapplication, il est recommand√© de faire un clic droit sur l‚Äôic√¥ne de l'application, puis de s√©lectionner Ex√©cuter en tant qu'administrateur. Cela ouvrira la premi√®re page, qui correspond √† la page d‚Äôaccueil."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#√©cran-daccueil",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#√©cran-daccueil",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "√âcran d‚Äôaccueil",
    "text": "√âcran d‚Äôaccueil\n¬†¬†¬†¬†¬†¬†Au d√©marrage de l‚Äôapplication, une interface d‚Äôaccueil a √©t√© d√©velopp√©e. Elle permet √† l‚Äôutilisateur de se connecter soit en mode administrateur, soit en mode utilisateur. Chaque mode requiert la saisie d‚Äôun identifiant et d‚Äôun mot de passe.\nUn bouton \"Quitter\" est √©galement disponible en bas de l‚Äô√©cran. En cas de tentative de fermeture de l‚Äôapplication, une bo√Æte de confirmation s‚Äôaffiche afin d‚Äô√©viter toute fermeture accidentelle.\nVid√©o de d√©monstration"
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#partie-administrateur",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#partie-administrateur",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "Partie Administrateur",
    "text": "Partie Administrateur\n¬†¬†¬†¬†¬†¬†La section Administrateur de l‚Äôapplication est constitu√©e de six interfaces principales, chacune ayant un r√¥le sp√©cifique dans la gestion et le suivi des activit√©s. Ces interfaces sont les suivantes (vous pouvez visualiser les vid√©os):\n1. Inventaires : permet l‚Äôenregistrement et la gestion des produits disponibles en stock.\n\n2. Employ√©s : destin√©e √† l‚Äôenregistrement et au suivi des employ√©s (vendeurs, caissiers, etc.).\n\n3. Ventes : affiche la liste des produits vendus et permet de suivre les transactions r√©alis√©es.\n\n4. Tableau de bord (Dashboard) : fournit une vue r√©capitulative des ventes et autres indicateurs cl√©s pour une vision globale de l‚Äôactivit√©.\n\n5. Rapports : g√©n√®re et permet d‚Äôimprimer des rapports de vente, facilitant ainsi l‚Äôanalyse et le suivi.\n\n6. Param√®tres : offre des options de personnalisation de l'application - changement du nom de l‚Äôentreprise, de l‚Äôadresse, des informations de contact, etc.\n\n\n\n\n\nInterface Admin- Param√®tre"
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#utilisateurs",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#utilisateurs",
    "title": "KedjeBoost ‚Äì Votre resto, en mode turbo !",
    "section": "Utilisateurs",
    "text": "Utilisateurs\n¬†¬†¬†¬†¬†¬†Cette interface a √©t√© con√ßue pour √™tre utilis√©e par les vendeurs ou caissiers op√©rant dans l‚Äôentreprise. Elle se compose de quatre sous-interfaces, chacune ayant un r√¥le bien d√©fini :\n1. Interface principale\nElle permet :\n\nde vendre un produit en stock,\nd‚Äôimprimer la facture de la commande une fois valid√©e,\nd‚Äôenregistrer le mode de paiement : en esp√®ces (cash), via des services de monnaie √©lectronique (Orange Money, Moov Money) ou encore d‚Äôenregistrer une commande diff√©r√©e, √† r√©gler ult√©rieurement.\n\n2. Interface \"Voir Commande\nCette interface permet de :\n\nrechercher une commande en attente,\nvalider ou annuler une commande lanc√©e pr√©c√©demment.\n\n3. Interface \"G√©rer les rapports de vente\nElle offre la possibilit√© de :\n\ns√©lectionner une date ou une plage de dates,\nimprimer les ventes r√©alis√©es par le caissier connect√©,\nafficher le total des ventes, √† la fois en montant (prix) et en quantit√© pour la p√©riode s√©lectionn√©e.\n\n\n4. Interface √âtat des serveurs\nCette derni√®re interface donne une vue d‚Äôensemble sur :\n\nles quantit√©s et totaux vendus,\nr√©partis par serveur (vendeur),\nafin de faciliter le suivi des performances individuelles.\n\nNB : Cette interface est √©galement accessible depuis la section Administrateur, ce qui permet √† la direction de superviser les activit√©s des vendeurs en temps r√©el."
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html",
    "title": "ShiFuMi IA ‚Äì Reconnaissance de gestes",
    "section": "",
    "text": "Ce projet vise √† cr√©er une intelligence artificielle capable de jouer au jeu ShiFuMi (pierre-papier-ciseaux) contre un humain. L‚Äôobjectif est de reconna√Ætre automatiquement les gestes d‚Äôune main via une webcam, et de r√©pondre en temps r√©el. Ce prototype est d√©velopp√© seul, en utilisant les outils suivants :\n\nCNN pr√©-entra√Æn√© (MobileNetV2) pour classifier les images de mains,\nTensorFlow/Keras pour l‚Äôentra√Ænement,\nYOLOv8 (en pr√©paration) pour la d√©tection en direct via webcam,"
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#pourquoi-entra√Æner-yolov8",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#pourquoi-entra√Æner-yolov8",
    "title": "ShiFuMi IA ‚Äì Reconnaissance de gestes",
    "section": "ü§î Pourquoi entra√Æner YOLOv8 ?",
    "text": "ü§î Pourquoi entra√Æner YOLOv8 ?\n¬†¬†¬†¬†¬†¬†L‚Äôobjectif principal de l‚Äôentra√Ænement de ce mod√®le est de d√©tecter les mains dans des images issues d‚Äôune webcam ou d‚Äôune vid√©o. Cette d√©tection constitue une premi√®re √©tape essentielle avant de transmettre la r√©gion d‚Äôint√©r√™t (ROI), c‚Äôest-√†-dire la main d√©tect√©e√† un mod√®le CNN pour effectuer la classification du geste (pierre, feuille, ciseaux).\nCette strat√©gie en deux √©tapes permet de :\n\nR√©duire le bruit visuel autour de la main (fond, visage, objets parasites)\nAugmenter la pr√©cision du classifieur CNN en se concentrant uniquement sur la main"
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#structure-des-donn√©es-yolo",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#structure-des-donn√©es-yolo",
    "title": "ShiFuMi IA ‚Äì Reconnaissance de gestes",
    "section": "üìö Structure des donn√©es YOLO",
    "text": "üìö Structure des donn√©es YOLO\nLe format d‚Äôattente de YOLO est sp√©cifique : chaque image d‚Äôentra√Ænement doit √™tre accompagn√©e d‚Äôun fichier d‚Äôannotation .txt contenant les informations de localisation des objets (ici, la main).\nL‚Äôorganisation des donn√©es se pr√©sente g√©n√©ralement ainsi :\ndatasets/yolo_hand/\n‚îú‚îÄ‚îÄ images/\n‚îÇ   ‚îú‚îÄ‚îÄ train/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_001.jpg\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_002.jpg\n‚îÇ   ‚îî‚îÄ‚îÄ val/\n‚îÇ       ‚îú‚îÄ‚îÄ img_101.jpg\n‚îÇ       ‚îú‚îÄ‚îÄ img_102.jpg\n‚îú‚îÄ‚îÄ labels/\n‚îÇ   ‚îú‚îÄ‚îÄ train/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_001.txt\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_002.txt\n‚îÇ   ‚îî‚îÄ‚îÄ val/\n‚îÇ       ‚îú‚îÄ‚îÄ img_101.txt\n‚îÇ       ‚îú‚îÄ‚îÄ img_102.txt\n¬†¬†¬†¬†¬†¬†Chaque fichier .txt contient une ou plusieurs lignes correspondant aux objets d√©tect√©s dans l‚Äôimage, selon le format suivant :\n&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\n\nToutes les valeurs sont normalis√©es entre 0 et 1 relativement √† la taille de l‚Äôimage.\nclass_id correspond ici √† la main, donc souvent 0 dans le cadre d‚Äôun probl√®me mono-classe."
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entra√Ænement-avec-ultralytics-exemple",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entra√Ænement-avec-ultralytics-exemple",
    "title": "ShiFuMi IA ‚Äì Reconnaissance de gestes",
    "section": "üöÄ Entra√Ænement avec Ultralytics (Exemple)",
    "text": "üöÄ Entra√Ænement avec Ultralytics (Exemple)\nPour entra√Æner le mod√®le YOLOv8, on utilise la commande suivante :\nyolo task=detect mode=train \\\n  model=yolov8n.pt \\\n  data=config.yaml \\\n  epochs=50 \\\n  imgsz=640\nO√π le fichier config.yaml contient :\npath: datasets/yolo_hand\ntrain: images/train\nval: images/val\n\nnames:\n  0: main\n\n\n\n\n\n\nPourquoi le notebook n‚Äôest pas publi√© ?\n\n\n\nL‚Äôoutil de d√©v√©loppement du site ne supporte pas la biblioth√®que tensorflow, du coup cela rend impossible le d√©ploiement du site car l‚Äôex√©cution des code du notebook ne passe pas. Toutefois, une fois les notebooks et scripts enti√®rement mis au propres, je les mettrai √† disposition sur un d√©pot public github. Neanmoins √©tant donn√© que les donn√©es ne m‚Äôappartiennent pas et que je n‚Äôai plus en ma possession certains liens directs vers celles-ci, elle ne seront pas publi√©es. Parcontre la structure des repertoires sera donn√©e."
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entra√Ænement-du-mod√®le-yolo-epochs33100",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entra√Ænement-du-mod√®le-yolo-epochs33100",
    "title": "ShiFuMi IA ‚Äì Reconnaissance de gestes",
    "section": "Entra√Ænement du mod√®le YOLO (Epochs=33/100)",
    "text": "Entra√Ænement du mod√®le YOLO (Epochs=33/100)\n¬†¬†¬†¬†¬†¬†Etant donn√© que le mod√®le √©tait entrain√© local et du fait qu‚Äôil mettait du temps, j‚Äôai volontairement stopp√© l‚Äôentra√Ænement √† la 33-i√®me it√©ration. Car √† ce stade les r√©sultats de la d√©tections des mains √©taient satisfaisants (MAP2 environ √©gale √† 0,94). Voici un extrait du r√©sultat :"
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entra√Ænement-du-mod√®le-cnn",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entra√Ænement-du-mod√®le-cnn",
    "title": "ShiFuMi IA ‚Äì Reconnaissance de gestes",
    "section": "Entra√Ænement du mod√®le CNN",
    "text": "Entra√Ænement du mod√®le CNN\n¬†¬†¬†¬†¬†¬†Une fois le mod√®le YOLO pr√™t √† detecter les mains dans une image, le boxe (pour dire le cadre/rectangle contenant la main) est envoy√© au mod√®le CNN afin qu‚Äôil puisse classer la main parmis les trois gestes : paper pour papier, rock pour pierre et scissors pour ciseau.\nLa vid√©o ci-apr√®s illustre le r√©sultat."
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#footnotes",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#footnotes",
    "title": "ShiFuMi IA ‚Äì Reconnaissance de gestes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe fine-tuning est une technique d‚Äôapprentissage supervis√© qui consiste √† prendre un mod√®le pr√©entra√Æn√© sur un grand jeu de donn√©es g√©n√©ral (comme ImageNet) et √† le r√©entra√Æner sur un jeu de donn√©es sp√©cifique √† un probl√®me particulier.‚Ü©Ô∏é\nMean Average Precision : C‚Äôest la m√©trique principale utilis√©e pour √©valuer les performances d‚Äôun mod√®le de d√©tection comme YOLO. Pr√©cision (Precision)‚Ü©Ô∏é"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-national-basketball-association",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-national-basketball-association",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "La National Basketball Association ?",
    "text": "La National Basketball Association ?\n\nLigue professionnelle de basketball la plus comp√©titive au monde\n30 √©quipes (Est et Ouest), 82 matchs de saison r√©guli√®re\nDonn√©es riches et vari√©es sur l‚Äôensemble de la ligue :\n\nsuivi tr√®s d√©taill√© des performances\ndes archives compl√®tes depuis plus de 75 ans\n\n\n\nProbl√©matique : Pr√©dire la dur√©e de carri√®re des joueurs nouvellement draft√©s"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#objectifs",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#objectifs",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Objectifs",
    "text": "Objectifs\n\nObjectif 1 : identifier des questions d√©crivant le jeu de donn√©es\n\nAnalyse exploratoire des donn√©es afin de r√©pondre √† 10 interrogations\nQuestions explorant diff√©rentes dimensions du basketball (√©quipes, les joueurs, les matchs, les play-offs ou encore la draft)\n\n\n\n\nObjectif 2 : pr√©dire la dur√©e de carri√®re des joueurs NBA\n\nSp√©cification et ajustement d‚Äôun mod√®le d‚Äôapprentissage automatique\n\n\n\n\n\nObjectif 3 : d√©velopper et d√©ployer une application\n\nCr√©ation d‚Äô interfaces interactives pour afficher les r√©ponses aux questions et d‚Äôautres informations sur la NBA"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#pr√©sentation-du-jeu-de-donn√©es-source-kaggle",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#pr√©sentation-du-jeu-de-donn√©es-source-kaggle",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Pr√©sentation du jeu de donn√©es (Source Kaggle)",
    "text": "Pr√©sentation du jeu de donn√©es (Source Kaggle)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravail de pr√©paration des donn√©es : \n\nInt√©gration manuelle des vainqueurs NBA manquants;\nharmonisation des noms de franchises :\n\n\nPhiladelphia Warriors ¬†‚áí¬† San Francisco Warriors ¬†‚áí¬† Golden State Warriors"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#pourquoi-avons-nous-utilis√©-des-classes",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#pourquoi-avons-nous-utilis√©-des-classes",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Pourquoi avons-nous utilis√© des classes ?",
    "text": "Pourquoi avons-nous utilis√© des classes ?"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#pr√©sentation-de-la-classe-reponse",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#pr√©sentation-de-la-classe-reponse",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Pr√©sentation de la classe Reponse",
    "text": "Pr√©sentation de la classe Reponse\n\n\n\nclass Reponse:\n    def __init__(self, data: dict[pd.DataFrame]):\n        # R√©alisons les tests n√©cessaire sur l'objet data\n\n        if (\n            not isinstance(data, dict)\n        ):\n            raise TypeError(\"L'argument data doit √™tre un dictionnaire.\")\n\n        if (\n            any(not isinstance(data[key], pd.DataFrame) for key in data.keys())\n        ):\n            raise TypeError(\"Toutes les valeurs des cl√©s doivents √™tre des \"\n                            \"pandas.DataFrame.\")\n\n        # Testons qu'on a bien la cl√© common_player_info dans le dictionnaire\n        if (\"draft_history\" not in data.keys()):\n            raise KeyError(\"La cl√© 'draft_history' ne fait pas parti du dictionnaire\")\n        if (\"common_player_info\" not in data.keys()):\n            raise KeyError(\"La cl√© 'common_player_info' ne fait pas parti du \"\n                           \"dictionnaire\")\n        if (\"game\" not in data.keys()):\n            raise KeyError(\"La cl√© 'game' ne fait pas parti du dictionnaire\")\n\n        self.data = copy.deepcopy(data)\n\n\n\n\nEntr√©e :\n\nDictionnaire de tables\n\nV√©rification des entr√©es :\n\nLe type des entr√©es est v√©rifi√©\n\nPr√©sence des tables d‚Äôint√©r√™t dans le dictionnaire\n\nExceptions lev√©es : TypError, KeyError"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-m√©thodes-de-la-classe-reponse",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-m√©thodes-de-la-classe-reponse",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Les m√©thodes de la classe Reponse",
    "text": "Les m√©thodes de la classe Reponse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†‚áê¬† Les m√©thodes index√©es :\n\n(1) retourne une table contenant le nombre de victoires ou de d√©faites pour chaque √©quipe entre les saisons donn√©es.\n(2) retourne une table listant les √©quipes ayant remport√© au moins le nombre de titres requis.\n(3) retourne un dictionnaire contenant deux tables, un pour chaque conf√©rence (Est et Ouest)."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-m√©thode-equip_victoires_defaites_saison-et-ses-usages",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-m√©thode-equip_victoires_defaites_saison-et-ses-usages",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "La m√©thode equip_victoires_defaites_saison et ses usages",
    "text": "La m√©thode equip_victoires_defaites_saison et ses usages\n\ndef equip_victoires_defaites_saison(self, annee_debut: int, annee_fin: int,\n                                        season_type: str = 'Regular Season',\n                                        defaite: bool = False) -&gt; pd.DataFrame:\n\nEntr√©es : p√©riode ‚Äì type de saison ‚Äì l‚Äôissue du match\nTraitement pour l‚Äôobtention du nombre de victoires ou de d√©faites :\n\n# D√©termination des √©quipes en fonction du r√©sultat souhait√©\n        game_chosen_season['Equipes'] = np.where(\n            ((game_chosen_season['wl_home'] == \"W\") & (not defaite)) |\n            ((game_chosen_season['wl_home'] == \"L\") & defaite),\n            game_chosen_season['team_name_home'],\n            game_chosen_season['team_name_away']\n        )\n\n        # Agr√©gation des r√©sultats\n        results = game_chosen_season.groupby([\"season_years\", \"Equipes\"]).aggregate({\n            'wl_home': 'count',\n            'Equipes': 'first',\n            'season_years': 'first'\n        }).reset_index(drop=True)"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-m√©thode-equipe_remporte_au_moins_n_fois_le_titre",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-m√©thode-equipe_remporte_au_moins_n_fois_le_titre",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "La m√©thode equipe_remporte_au_moins_N_fois_le_titre",
    "text": "La m√©thode equipe_remporte_au_moins_N_fois_le_titre\n\ndef equipe_remporte_au_moins_N_fois_le_titre(self, nb_victoire_min: int = 3,\n                                                 debut_periode: int = 1946,\n                                                 fin_periode: int = 2023\n                                                 ) -&gt; pd.DataFrame\n(1) nba_champions_manquant = {\n\"1957-1958\": \"Atlanta Hawks\", \"1958-1959\": \"Boston Celtics\", \"1960-1961\": \"Boston Celtics\", \n\"1964-1965\": \"Boston Celtics\", \"1968-1969\": \"Boston Celtics\", \"1993-1994\": \"Houston Rockets\", \n\"1995-1996\": \"Chicago Bulls\", \"1999-2000\": \"Los Angeles Lakers\", \"2001-2002\": \"Los Angeles Lakers\", \"2005-2006\": \"Miami Heat\"\n}\n\nS√©lection des donn√©es\nIdentification des vainqueurs de chaque saison\nAlimentation des r√©sultats (1)\nEdition de la table avec le nombre de titres gagn√©s sur la p√©riode par √©quipe\nRenvoi de la table avec les √©quipes avec au moins 3 titres NBA"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-m√©thode-classement_conferences",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-m√©thode-classement_conferences",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "La m√©thode classement_conferences",
    "text": "La m√©thode classement_conferences\n\ndef classement_conferences(self, season: str = '2022-2023',\n                               end: str = None) -&gt; dict[pd.DataFrame]:\n  . . .       \n\n  classement = {\"Conf√©rence Est\": classement_est,\n              \"Conf√©rence Ouest\": classement_ouest}\n\nS√©lection des donn√©es (saison r√©guli√®re)\nIdentification du nombre de victoire par √©quipe\nClassement selon la conf√©rence\nRenvoie les deux tables correspondantes aux conf√©rences Est et Ouest"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-√©quipes-ayant-remport√©-au-moins-n-titres-nba-entre-deux-p√©riodes-donn√©es",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-√©quipes-ayant-remport√©-au-moins-n-titres-nba-entre-deux-p√©riodes-donn√©es",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Les √©quipes ayant remport√© au moins N titres NBA, entre deux p√©riodes donn√©es",
    "text": "Les √©quipes ayant remport√© au moins N titres NBA, entre deux p√©riodes donn√©es\n\nclass Reponse:\n    def __init__(self, data: dict[pd.DataFrame]):\n      ...\n    ...\n    def equipe_remporte_au_moins_N_fois_le_titre(self, nb_victoire_min: int = 3,\n                                                 debut_periode: int = 1946,\n                                                 fin_periode: int = 2023\n                                                 ) -&gt; pd.DataFrame\n\n\n\n\n\n\n\nParam√®tres de la m√©thode equipe_remporte_au_moins_N_fois_le_titre"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-√©quipes-ayant-remport√©-au-moins-3-titres-nba",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-√©quipes-ayant-remport√©-au-moins-3-titres-nba",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Les √©quipes ayant remport√© au moins 3 titres NBA",
    "text": "Les √©quipes ayant remport√© au moins 3 titres NBA\n\n\n\n\nEquipes ayant remport√© au moins 3 titres NBA\n\n\nEquipe\nNombre.de.titre.NBA\n\n\n\n\nBoston Celtics\n17\n\n\nLos Angeles Lakers\n17\n\n\nGolden State Warriors\n7\n\n\nChicago Bulls\n6\n\n\nPhiladelphia 76ers\n6\n\n\nSan Antonio Spurs\n5\n\n\nMiami Heat\n3\n\n\nDetroit Pistons\n3\n\n\nHouston Rockets\n3"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#classement-des-conf√©rences-√†-la-fin-dune-saison-donn√©e",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#classement-des-conf√©rences-√†-la-fin-dune-saison-donn√©e",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Classement des conf√©rences √† la fin d‚Äôune saison donn√©e",
    "text": "Classement des conf√©rences √† la fin d‚Äôune saison donn√©e\n\nclass Reponse:\n    def __init__(self, data: dict[pd.DataFrame]):\n      ...\n    ...\n    def classement_conferences(self, season: str = '2022-2023',\n                               end: str = None) -&gt; dict[pd.DataFrame]:\n\n\n\n\n\n\n\nParam√®tres de la m√©thode classement_conferences"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#classement-des-conf√©rences-√†-la-fin-de-la-saison-2022-2023",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#classement-des-conf√©rences-√†-la-fin-de-la-saison-2022-2023",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Classement des conf√©rences √† la fin de la saison 2022-2023",
    "text": "Classement des conf√©rences √† la fin de la saison 2022-2023\n\n\n\n\n\n\nEquipe conf√©rence Est\nVictoires\nPoints\nEquipe conf√©rence Ouest\nVictoires\nPoints\n\n\n\n\nMilwaukee Bucks\n58\n9589\nDenver Nuggets\n53\n9495\n\n\nBoston Celtics\n57\n9671\nMemphis Grizzlies\n51\n9587\n\n\nPhiladelphia 76ers\n54\n9448\nSacramento Kings\n48\n9898\n\n\nCleveland Cavaliers\n51\n9205\nPhoenix Suns\n45\n9319\n\n\nNew York Knicks\n47\n9514\nGolden State Warriors\n44\n9753\n\n\nBrooklyn Nets\n45\n9295\nLA Clippers\n44\n9314\n\n\nMiami Heat\n44\n8977\nLos Angeles Lakers\n43\n9608\n\n\nAtlanta Hawks\n41\n9711\nMinnesota Timberwolves\n42\n9494\n\n\nToronto Raptors\n41\n9254\nNew Orleans Pelicans\n42\n9378\n\n\nChicago Bulls\n40\n9276\nOklahoma City Thunder\n40\n9633\n\n\nIndiana Pacers\n35\n9535\nDallas Mavericks\n38\n9366\n\n\nWashington Wizards\n35\n9279\nUtah Jazz\n37\n9600\n\n\nOrlando Magic\n34\n9136\nPortland Trail Blazers\n33\n9299\n\n\nCharlotte Hornets\n27\n9098\nSan Antonio Spurs\n22\n9269\n\n\nDetroit Pistons\n17\n9045\nHouston Rockets\n22\n9081"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#er-choix-de-la-draft-et-caract√©ristiques-physiques-des-joueurs",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#er-choix-de-la-draft-et-caract√©ristiques-physiques-des-joueurs",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "1er choix de la draft et caract√©ristiques physiques des joueurs",
    "text": "1er choix de la draft et caract√©ristiques physiques des joueurs\n\n\n\n\n\n\n1er choix de la draf NBA (2019-2023) \n\n\nSaison\nNom\nEquipe\nPays\n\n\n\n\n2019\nZion Williamson\nNew Orleans\nUSA\n\n\n2020\nAnthony Edwards\nMinnesota\nUSA\n\n\n2021\nCade Cunningham\nDetroit\nUSA\n\n\n2022\nPaolo Banchero\nOrlando\nUSA\n\n\n2023\nVictor Wembanyama\nSan Antonio\nFrance\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoids et tailles m√©dians par position \n\n\nposition\nTaille.cm\nPoids.Kg\n\n\n\n\nPivot\n210.82\n108.86\n\n\nPivot/Ailier fort\n210.82\n113.40\n\n\nAilier\n200.66\n98.88\n\n\nAilier fort/Pivot\n208.28\n108.86\n\n\nAilier/Meneur\n200.66\n99.79\n\n\nArri√®re/Meneur\n190.50\n86.18\n\n\nArri√®re/Ailier\n198.12\n95.25"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#quel-mod√®le-avons-nous-choisis",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#quel-mod√®le-avons-nous-choisis",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Quel mod√®le avons-nous choisis ?",
    "text": "Quel mod√®le avons-nous choisis ?\n\nParmi les mod√®les de machine, nous avons choisi un mod√®le d‚Äôapprentissage supervis√© et ce fut celui de la regression\n\n\n\n\n\n\nApprentissage supervis√© - Mod√®le de regression"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#mod√®le-de-regression-lin√©aire",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#mod√®le-de-regression-lin√©aire",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Mod√®le de regression lin√©aire",
    "text": "Mod√®le de regression lin√©aire\nPRINCIPE : Pr√©dire une variable quantitative √† l‚Äôaide d‚Äôune ou plusieurs variables explicatives (quantitatives ou qualitatives)\n\\[\\begin{equation}\n  y = \\beta X + \\epsilon\n\\end{equation}\\]\no√π \\(\\beta\\) est le coefficient associ√© aux variables explicatives \\(X\\) et \\(\\epsilon\\) le terme d‚Äôerreur.\n\nLe mod√®le de regression lin√©aire permet de pr√©dire une variable quantitative √† l‚Äôaide de variables explicatves app√©l√©e features. Son √©quation est la suivante : y la variable √† pr√©dire = beta x + epsilon ou x est l‚Äôensemble des features et epsilon les termes d‚Äôerreur"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#choix-des-variables-explicatives",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#choix-des-variables-explicatives",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Choix des variables explicatives",
    "text": "Choix des variables explicatives\n\nNotre regard s‚Äôest d‚Äôabord tourn√© vers les variables age √† la draft, poste occup√© sur le terrain, taille et poids du joueurs.\n\n\n\n\n\n\nVariables pr√©alables"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#choix-des-variables-explicatives-1",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#choix-des-variables-explicatives-1",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Choix des variables explicatives",
    "text": "Choix des variables explicatives\nVariables retenues\n\nLe poste occup√© sur le terrain ayant un lien qvec la taille et le poids du joueur, nous avons seulement gard√© le poste occup√© sur le terrain en plus de l‚Äô√¢ge √† la draft. Cela a permis d‚Äô√©viter √† un probl√®me de multicolin√©arit√© et donc d‚Äô√©viter un mauvais ajustement du mod√®le.\n\n\n\n\n\n\nVariables r√©t√©nues pour l‚Äôajustement du mod√®le"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#entrainement-du-mod√®le",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#entrainement-du-mod√®le",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Entrainement du mod√®le",
    "text": "Entrainement du mod√®le\n\nEntra√Ænement et √©valuation crois√©e\nNous lan√ßons cinq entra√Ænements successifs du mod√®le, chacun sur 80 % des donn√©es, en r√©servant √† chaque fois un pli diff√©rent pour la validation (donn√©es de tests).\n\n\n\n\n\n\nValidation crois√©e K-Fold (k = 5)"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#entrainement-du-mod√®le-1",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#entrainement-du-mod√®le-1",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Entrainement du mod√®le",
    "text": "Entrainement du mod√®le\n\n\n\n\n\nR√©sum√© de l‚Äôapprentissage automatique\n\n\n\n\n\n\nApr√®s avoir valid√© la robustesse du mod√®le via la CV, nous le r√©-entra√Ænons une derni√®re fois sur 100 % des observations disponibles, sans rien r√©server comme jeu de test.\n\nPourquoi ? Pour exploiter au maximum l‚Äôinformation disponible et obtenir un mod√®le final plus performant.\nComment s‚Äôassurer de sa fiabilit√© ? Nous nous appuyons enti√®rement sur la moyenne (4,59) et l‚Äô√©cart-type (¬± 0,12) des RMSE issus de la CV comme mesure de sa capacit√© de g√©n√©ralisation.\n\nPerformance confirm√©e\n\nGr√¢ce √† cette validation crois√©e, nous avons vu que le mod√®le g√©n√©ralise bien : les scores ne varient que de ¬± 0,12 autour de 4,59.\n\nInterpr√©tations et pr√©dictions\n\nForts de cette robustesse, nous pouvons passer sereinement √† l‚Äôanalyse de l‚Äôimportance des variables, √† l‚Äôinterpr√©tation des effets et en dernier lieu √† la production de pr√©dictions fiables sur de nouvelles donn√©es."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#methode-fit-de-la-classe-linearregression",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#methode-fit-de-la-classe-linearregression",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Methode fit de la classe LinearRegression",
    "text": "Methode fit de la classe LinearRegression\ndef fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Estime les coefficients de r√©gression par OLS.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            Matrice des pr√©dicteurs.\n        y : np.ndarray\n            Vecteur cible.\n\n        Returns\n        -------\n        np.ndarray\n            Coefficients estim√©s.\n        \"\"\"\n        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n            raise TypeError(\"X et Y doivent-√™tre de type np.ndarray\")\n\n        cond_number = np.linalg.cond(X.T @ X)\n        if cond_number &gt; 1e10:\n            warnings.warn(\n                \"Matrice X.T @ X mal conditionn√©e (cond &gt; 1e10).\"\n                \"Risque de multicolin√©arit√©.\"\n            )\n\n        X_X_inv = np.linalg.pinv(X.T @ X)\n        Beta = X_X_inv @ X.T @ y\n        return Betas\n\nPour repondre √† la probl√©matique pos√©e, nous avons √©crit une classe linearRegression qui entraine le mod√®le, le valide, et pr√©dit la dur√©e de carri√®re des joueurs. Avant d‚Äôajuster le mod√®le, nous nous assurons que la matrice est bien conditionn√©e. (Sur le code on peut voir qu‚Äôun nombre de condition &gt; 1e10 signifie un probl√®me de multicolin√©arit√©)"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#r√©sultats-de-lentra√Ænement",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#r√©sultats-de-lentra√Ænement",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "R√©sultats de l‚Äôentra√Ænement",
    "text": "R√©sultats de l‚Äôentra√Ænement\n\n\n\n\n\n\nR√©sultats de l‚Äôentra√Ænement du mod√®le \n\n\nVariable\nEstimation\nBorne.inf√©rieure\nBorne.sup√©rieure\n\n\n\n\nintercept\n13.0807938172894\n11.2139999315576\n14.9475877030211\n\n\nage_at_draft\n-0.304520510475786\n-0.385970108084788\n-0.223070912866783\n\n\nPivot-Ailier fort\n4.03469135857516\n2.58019430274557\n5.48918841440475\n\n\nAilier\n-0.723448794163931\n-1.24222365227521\n-0.204673936052656\n\n\nAilier fort-Pivot\n3.00118640834299\n1.80323628965945\n4.19913652702653\n\n\nAilier-Arri√®re\n1.31879081528329\n-0.203041070946694\n2.84062270151327\n\n\nArri√®re\n-0.779850850158738\n-1.30098176750811\n-0.258719932809363\n\n\nArri√®re-Ailier\n1.88960963320118\n0.805602797554438\n2.97361646884792\n\n\n\n\n\n\n\n\n\n\n\n\nAge √† la draft + 1 ¬†‚áí¬† diminution moyenne de la dur√©e de carri√®re de 0.30 ans\nUn pivot/ailier a plus de chance de durer √† la NBA que les joueurs occupant les autres postes\nUn arri√®re a moins de chance de durer √† la NBA que les joueurs occupant les autres postes\n\n\n\nPr√©diction : \\(exp = 13,08 - 0.305*Age_{draft} + \\beta_j*Poste_j\\)\nAge_a_la_draft = 18 ans et Poste = Pivot\n\nDur√©e de carri√®re : 7.6 ans\nIntervalle de confiance [4.3, 10.9]\n\n\n\n\n\n\n\n\nPar exemple pour un joueur qui s‚Äôest pr√©sent√© √† 18 ans la draft, et occupant le poste de pivot, le mod√®le pr√©dit une dur√©e de carri√®re de 7,6 ans avec un IC allant de 4,3 √† 10,9 ans]"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#synth√®se-de-l√©tude",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#synth√®se-de-l√©tude",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Synth√®se de l‚Äô√©tude",
    "text": "Synth√®se de l‚Äô√©tude\n\nCe projet entre pleinement dans le domaine de l‚Äôinformatique appliqu√©e aux donn√©es.\nSuivi du processus : nettoyage, exploration et mod√©lisation\nR√©ponses rigoureuses aux questions pos√©es\nConstruction d‚Äôun mod√®le supervis√© pour pr√©dire la dur√©e de carri√®re des joueurs"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#avantages-et-difficult√©s",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#avantages-et-difficult√©s",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "Avantages et difficult√©s",
    "text": "Avantages et difficult√©s\n\n\n\n\n\n\n\n\nAvantages\n\n\n\nAcquisition de comp√©tences transversales : manipulation de donn√©es, machine learning, visualisation interactive.\nIntervalle de confiance des pr√©dictions\nImpl√©mentation manuelle du mod√®le supervis√©\nModularisation du code facilitant la maintenance et la r√©utilisation.\nPrise d‚Äôinitiatives (cr√©ation d‚Äôune interface, traitement des noms d‚Äô√©quipes changeants, gestion des donn√©es manquantes)\n\n\n\n\n\n\n\n\n\n\n\n\nDifficult√©s\n\n\n\nErreur de pr√©diction plus ou moins √©lev√©e (4,6 ans)\nDes attentes initiales manquaient de clart√©\nQuelques difficult√©s √† identifier et corriger les incoh√©rences dans les donn√©es historiques"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#end",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#end",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA",
    "section": "END",
    "text": "END\n\n\n\n\n\n\n\n\n\n\n\n\nSoutenance du projet traitement de donn√©es 1A"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "",
    "text": "La NBA (National Basketball Association) est la ligue professionnelle de basketball la plus comp√©titive au monde. Depuis plus de 75 ans, elle r√©unit les meilleurs talents, entra√Æneurs et infrastructures. Aujourd‚Äôhui, la ligue est compos√©e de 30 √©quipes r√©parties entre deux conf√©rences (Est et Ouest), disputant chacune 82 matchs de saison r√©guli√®re. Les huit meilleures √©quipes de chaque conf√©rence acc√®dent ensuite aux s√©ries √©liminatoires (play-offs), structur√©es en quatre tours successifs en format au meilleur des sept matchs (Guan, Wang, and Yuan 2022; Teramoto and Cross 2010).\n¬†¬†¬†¬†¬†¬†Malgr√© son prestige, la carri√®re moyenne d‚Äôun joueur en NBA reste relativement courte. Plusieurs √©tudes montrent que cette long√©vit√© varie selon l‚Äôorigine des joueurs et leur parcours : par exemple, les joueurs √©trangers sans passage par une universit√© am√©ricaine semblent avoir une carri√®re plus br√®ve que ceux issus du syst√®me universitaire des USA (Groothuis and Hill 2018).\n¬†¬†¬†¬†¬†¬†Du point de vue des donn√©es, la NBA g√©n√®re une volum√©trie importante d‚Äôinformations sur les joueurs : caract√©ristiques physiques, parcours, statistiques de jeu, dur√©e de carri√®re, etc. Ces donn√©es sont une opportunit√© pour mener une analyse exploratoire orient√©e science des donn√©es, dans le but d‚Äôidentifier des patterns significatifs.\nL‚Äôobjectif de cette √©tude est donc d‚Äôutiliser les donn√©es disponibles pour pr√©dire la dur√©e de carri√®re d‚Äôun joueur NBA √† partir d‚Äôattributs personnels (√¢ge, position, parcours acad√©mique, etc.) √† l‚Äôaide de mod√®les de machine learning.\n\n\n\n¬†¬†¬†¬†¬†¬†Dans le prolongement de l‚Äôintroduction, cette section vise √† expliciter la m√©thodologie adopt√©e pour r√©pondre aux diff√©rentes questions pos√©es dans le cadre de ce projet, qu‚Äôil s‚Äôagisse des questions obligatoires ou de celles laiss√©es au choix de l‚Äô√©quipe.\n¬†¬†¬†¬†¬†¬†Nous commencerons par une pr√©sentation claire et synth√©tique de la classe que nous avons d√©velopp√©e pour structurer notre l‚Äôapprentissage automatique. Cette classe, que nous avons construite manuellement, int√®gre des fonctionnalit√©s avanc√©es telles que la gestion des variables cat√©gorielles, la d√©tection de la multicolin√©arit√©, la validation crois√©e, ainsi que la r√©gularisation de type Ridge. Elle se distingue notamment de l‚Äôimpl√©mentation classique LinearRegression de la biblioth√®que scikit-learn, en y ajoutant des m√©thodes personnalis√©es adapt√©es √† nos besoins sp√©cifiques.\n¬†¬†¬†¬†¬†¬†Ensuite, nous d√©taillerons les principales √©tapes du processus de mod√©lisation, en expliquant comment les m√©thodes de cette classe nous permettent de mettre en ≈ìuvre une r√©gression lin√©aire multiple de mani√®re compl√®te et contr√¥l√©e. Nous d√©crirons le r√¥le de chaque m√©thode dans le traitement des donn√©es, l‚Äôentra√Ænement du mod√®le, l‚Äô√©valuation des performances et l‚Äôinterpr√©tation des r√©sultats, notamment √† travers le calcul d‚Äôintervalles de confiance et d‚Äôindicateurs d‚Äôerreur comme le RMSE.\n¬†¬†¬†¬†¬†¬†Par ailleurs, une application web interactive a √©t√© d√©velopp√©e avec Shiny afin de rendre notre travail accessible √† travers une interface utilisateur conviviale. Nous pr√©senterons cette application en d√©tail : son objectif, son fonctionnement, et ses principales fonctionnalit√©s. Nous expliquerons √©galement les choix techniques qui ont guid√© son d√©veloppement, ainsi que la mani√®re dont elle permet de visualiser et d‚Äôinteragir avec les r√©sultats issus de notre mod√®le de r√©gression.\n¬†¬†¬†¬†¬†¬†Enfin, nous proposerons un retour critique sur notre exp√©rience de projet. Cette partie mettra en lumi√®re les d√©fis rencontr√©s tout au long du processus, les solutions que nous avons mises en ≈ìuvre, ainsi que la mani√®re dont nous avons organis√© et coordonn√© notre travail en √©quipe. Nous partagerons les enseignements tir√©s de cette collaboration, tant sur le plan technique que m√©thodologique et humain."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#introduction",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#introduction",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "",
    "text": "La NBA (National Basketball Association) est la ligue professionnelle de basketball la plus comp√©titive au monde. Depuis plus de 75 ans, elle r√©unit les meilleurs talents, entra√Æneurs et infrastructures. Aujourd‚Äôhui, la ligue est compos√©e de 30 √©quipes r√©parties entre deux conf√©rences (Est et Ouest), disputant chacune 82 matchs de saison r√©guli√®re. Les huit meilleures √©quipes de chaque conf√©rence acc√®dent ensuite aux s√©ries √©liminatoires (play-offs), structur√©es en quatre tours successifs en format au meilleur des sept matchs (Guan, Wang, and Yuan 2022; Teramoto and Cross 2010).\n¬†¬†¬†¬†¬†¬†Malgr√© son prestige, la carri√®re moyenne d‚Äôun joueur en NBA reste relativement courte. Plusieurs √©tudes montrent que cette long√©vit√© varie selon l‚Äôorigine des joueurs et leur parcours : par exemple, les joueurs √©trangers sans passage par une universit√© am√©ricaine semblent avoir une carri√®re plus br√®ve que ceux issus du syst√®me universitaire des USA (Groothuis and Hill 2018).\n¬†¬†¬†¬†¬†¬†Du point de vue des donn√©es, la NBA g√©n√®re une volum√©trie importante d‚Äôinformations sur les joueurs : caract√©ristiques physiques, parcours, statistiques de jeu, dur√©e de carri√®re, etc. Ces donn√©es sont une opportunit√© pour mener une analyse exploratoire orient√©e science des donn√©es, dans le but d‚Äôidentifier des patterns significatifs.\nL‚Äôobjectif de cette √©tude est donc d‚Äôutiliser les donn√©es disponibles pour pr√©dire la dur√©e de carri√®re d‚Äôun joueur NBA √† partir d‚Äôattributs personnels (√¢ge, position, parcours acad√©mique, etc.) √† l‚Äôaide de mod√®les de machine learning.\n\n\n\n¬†¬†¬†¬†¬†¬†Dans le prolongement de l‚Äôintroduction, cette section vise √† expliciter la m√©thodologie adopt√©e pour r√©pondre aux diff√©rentes questions pos√©es dans le cadre de ce projet, qu‚Äôil s‚Äôagisse des questions obligatoires ou de celles laiss√©es au choix de l‚Äô√©quipe.\n¬†¬†¬†¬†¬†¬†Nous commencerons par une pr√©sentation claire et synth√©tique de la classe que nous avons d√©velopp√©e pour structurer notre l‚Äôapprentissage automatique. Cette classe, que nous avons construite manuellement, int√®gre des fonctionnalit√©s avanc√©es telles que la gestion des variables cat√©gorielles, la d√©tection de la multicolin√©arit√©, la validation crois√©e, ainsi que la r√©gularisation de type Ridge. Elle se distingue notamment de l‚Äôimpl√©mentation classique LinearRegression de la biblioth√®que scikit-learn, en y ajoutant des m√©thodes personnalis√©es adapt√©es √† nos besoins sp√©cifiques.\n¬†¬†¬†¬†¬†¬†Ensuite, nous d√©taillerons les principales √©tapes du processus de mod√©lisation, en expliquant comment les m√©thodes de cette classe nous permettent de mettre en ≈ìuvre une r√©gression lin√©aire multiple de mani√®re compl√®te et contr√¥l√©e. Nous d√©crirons le r√¥le de chaque m√©thode dans le traitement des donn√©es, l‚Äôentra√Ænement du mod√®le, l‚Äô√©valuation des performances et l‚Äôinterpr√©tation des r√©sultats, notamment √† travers le calcul d‚Äôintervalles de confiance et d‚Äôindicateurs d‚Äôerreur comme le RMSE.\n¬†¬†¬†¬†¬†¬†Par ailleurs, une application web interactive a √©t√© d√©velopp√©e avec Shiny afin de rendre notre travail accessible √† travers une interface utilisateur conviviale. Nous pr√©senterons cette application en d√©tail : son objectif, son fonctionnement, et ses principales fonctionnalit√©s. Nous expliquerons √©galement les choix techniques qui ont guid√© son d√©veloppement, ainsi que la mani√®re dont elle permet de visualiser et d‚Äôinteragir avec les r√©sultats issus de notre mod√®le de r√©gression.\n¬†¬†¬†¬†¬†¬†Enfin, nous proposerons un retour critique sur notre exp√©rience de projet. Cette partie mettra en lumi√®re les d√©fis rencontr√©s tout au long du processus, les solutions que nous avons mises en ≈ìuvre, ainsi que la mani√®re dont nous avons organis√© et coordonn√© notre travail en √©quipe. Nous partagerons les enseignements tir√©s de cette collaboration, tant sur le plan technique que m√©thodologique et humain."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#cadre-du-projet",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#cadre-du-projet",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Cadre du projet",
    "text": "Cadre du projet\n¬†¬†¬†¬†¬†¬†Ce projet s‚Äôinscrit dans le cadre des travaux de fin d‚Äôann√©e des √©tudiants de premi√®re ann√©e √† l‚Äô√âcole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation (ENSAI).\nIl constitue le volet informatique appliqu√© aux donn√©es du programme acad√©mique.\nL‚Äôobjectif est de mobiliser les comp√©tences acquises en programmation, mod√©lisation et analyse pour r√©soudre une probl√©matique concr√®te √† partir de donn√©es r√©elles.\n‚û°Ô∏è Les slides de la pr√©sentation sont disponibles √† l‚Äôadresse suivante : Acc√©der aux slides"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#aspects-techniques-du-projet",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#aspects-techniques-du-projet",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Aspects techniques du projet",
    "text": "Aspects techniques du projet\n\nPr√©sentation du jeu de donn√©es\n¬†¬†¬†¬†¬†¬†L‚Äô√©tude repose sur un ensemble de donn√©es relatives au basketball, plus pr√©cis√©ment √† la ligue am√©ricaine : la NBA. Ces donn√©es nous ont √©t√© fournies sous forme de plusieurs fichiers CSV, que nous avons convertis en tables pour les besoins du traitement. Chaque fichier contient des informations sp√©cifiques sur les matchs, les √©quipes ou les joueurs de la ligue. Les donn√©es sur les confrontations entre √©quipes couvrent la p√©riode allant de 1946 √† 2023.\n¬†¬†¬†¬†¬†¬†La table common_player_info fournit des caract√©ristiques de certains joueurs telles que leur nom, pr√©nom, date de naissance, ann√©e de draft, statut de ‚Äúgreatest‚Äù ou non, dur√©e de carri√®re, taille, poids, etc. La table draft_combine_history contient quant √† elle des informations int√©ressantes sur la draft des joueurs : universit√© d‚Äôorigine, √©quipe de destination, rang de draft, etc. Enfin, la table game recense les matchs de la NBA de la saison 1946-1947 √† la saison 2022-2023**. On y trouve notamment des variables essentielles pour filtrer les donn√©es et mener nos analyses : saison, date de match, issue de la rencontre (victoire ou d√©faite), et nombre de points marqu√©s par chaque √©quipe lors des confrontations. C‚Äôest sur ces trois tables principalement que nous r√©aliserons notre traitement.\n\n\nExploration et harmonisation des donn√©es\n¬†¬†¬†¬†¬†¬†Lors de la phase exploratoire des donn√©es, nous avons constat√© que les informations de la table game pouvaient √©voluer d‚Äôune saison √† l‚Äôautre. En effet, il n‚Äôest pas rare qu‚Äôune franchise change de nom, notamment lorsqu‚Äôelle d√©m√©nage dans une nouvelle ville. Par exemple, les Seattle SuperSonics sont devenus les Oklahoma City Thunder apr√®s leur relocalisation, et les New Jersey Nets ont √©t√© renomm√©s Brooklyn Nets apr√®s leur installation √† Brooklyn.\nAfin d‚Äô√©viter de biaiser nos r√©sultats en consid√©rant √† tort que deux noms diff√©rents correspondent √† deux √©quipes distinctes, nous avons effectu√© des recherches sur les 30 franchises de la NBA, accompagn√©s par l‚ÄôIA, pour identifier celles ayant chang√© de nom entre 1946 et 2023.\n¬†¬†¬†¬†¬†¬†√Ä l‚Äôaide de manipulations de tables avec pandas et de dictionnaires, nous avons harmonis√© les noms afin d‚Äôattribuer √† chaque franchise son nom actuel. Cette √©tape nous permet ainsi de travailler avec un ensemble coh√©rent de 30 modalit√©s repr√©sentant les franchises existantes.\n¬†¬†¬†¬†¬†¬†De plus, dans la table game, les confrontations des playoffs sont absentes pour les saisons suivantes : 1958-1959, 1960-1961, 1964-1965, 1968-1969, 1993-1994, 1995-1996, 1999-2000, 2001-2002 et 2005-2006. Il est donc impossible de d√©terminer le vainqueur du titre pour ces saisons √† partir des seules donn√©es disponibles. Nous avons donc recherch√© et r√©cup√©r√© sur Internet les vainqueurs du titre NBA pour ces ann√©es. Ainsi, lorsque l‚Äôanalyse porte sur le nombre de titres NBA remport√©s par chaque franchise ou sur l‚Äôidentification du champion lors d‚Äôune saison pr√©cise, ces donn√©es externes sont int√©gr√©es √† l‚Äôanalyse.\n\n\nR√©ponses aux questions\n¬†¬†¬†¬†¬†¬†L‚Äôun des objectifs principaux de ce projet √©tait de mener une premi√®re analyse exploratoire des donn√©es afin de r√©pondre √† un certain nombre d‚Äôinterrogations. Parmi celles-ci, deux questions nous ont √©t√© impos√©es.\n\nPr√©sentation des questions pos√©es\n¬†¬†¬†¬†¬†¬†Les deux questions obligatoires d√©finies dans le cadre du projet sont les suivantes :\n\nQ1. Quelles sont les √©quipes ayant remport√© au moins 3 titres NBA, en pr√©cisant le nombre de titres pour chacune d‚Äôelles ?\nQ2. Quel √©tait le classement des conf√©rences Ouest et Est √† la fin de la saison r√©guli√®re 2022-2023 ?\n\n¬†¬†¬†¬†¬†¬†Les questions ci-dessous sont celles que nous avons jug√©es les plus int√©ressantes √† explorer et auxquelles nous avons choisi d‚Äôapporter des r√©ponses :\n\nQ3. Quelle est la taille et le poids m√©dians des joueurs selon leur poste ?\nQ4. Quelles sont les universit√©s ayant form√© le plus de joueurs √©voluant en NBA ?\nQ5. En dehors des √âtats-Unis, quels sont les pays d‚Äôorigine les plus repr√©sent√©s parmi les joueurs NBA ?\nQ6. Quelle est l‚Äô√©volution du nombre de rencontres sur p√©riodes donn√©es en NBA ?\nQ7. Qui sont les num√©ros 1 de la draft lors des 5 derni√®res saisons ?\nQ8. Quelles sont les √©quipes ayant obtenu le plus de victoires et de d√©faites durant les 5 derni√®res saisons r√©guli√®res ?\nQ9. Quels sont les champions NBA au cours des 8 derni√®res saisons ?\nQ10. Quelles sont les √©quipes ayant remport√© le titre NBA deux ann√©es cons√©cutives ?\n\n\n\nPr√©sentation des r√©ponses apport√©es\n¬†¬†¬†¬†¬†¬†Les r√©ponses aux questions sont toutes dans le notebook jupyter (reponses_aux_questions.ipynb). Dans le pr√©sent document, nous pr√©sentons celles concernant les questions obligatoires, ainsi qu‚Äôune question suppl√©mentaire que nous jugeons particuli√®rement pertinente.\n\nLes √©quipes ayant remport√© au moins 3 titres NBA\n\n¬†¬†¬†¬†¬†¬†Lorsqu‚Äôon observe le nombre de titres remport√©s par chaque franchise en NBA, on constate une grande variabilit√©. Par ailleurs, en se concentrant sur les √©quipes ayant remport√© au moins 3 titres, plusieurs franchises embl√©matiques ressortent, ayant marqu√© l‚Äôhistoire du basketball en NBA (cf.¬†tableau d‚Äôapr√®s). On observe que les Boston Celtics et les Los Angeles Lakers dominent largement le palmar√®s, avec chacun 17 titres sur la p√©riode 1946-2023. Ils sont suivis par les Golden State Warriors, qui en comptent 7.\n\n\n\nR√©capitulatif du nombre de titre gagn√© de chaque franchise\n\n\nEquipe\nNombre de titre NBA\n\n\n\n\nBoston Celtics\n17\n\n\nLos Angeles Lakers\n17\n\n\nGolden State Warriors\n7\n\n\nChicago Bulls\n6\n\n\nPhiladelphia 76ers\n6\n\n\nSan Antonio Spurs\n5\n\n\nMiami Heat\n3\n\n\nDetroit Pistons\n3\n\n\nHouston Rockets\n3\n\n\n\n\n\n\nLe classement des conf√©rences Ouest et Est √† la fin de la saison r√©guli√®re 2022-2023\n\n¬†¬†¬†¬†¬†¬†Pour d√©terminer les meilleures √©quipes de chaque conf√©rence, nous avons analys√© leur nombre de victoires durant la saison r√©guli√®re 2022-2023. Le classement inclut √©galement le total de points marqu√©s par chaque franchise. En cas d‚Äô√©galit√©, ce total permet de d√©partager les √©quipes. Ainsi, les vainqueurs des conf√©rences Est et Ouest sont respectivement les Milwaukee Bucks et les Denver Nuggets (cf.¬†le tableau ci-dessous).\n\n\n\nClassement des conf√©rences Ouest et Est √† la fin de la saison r√©guli√®re 2022-2023\n\n\nEquipe conf√©rence Est\nVictoires\nPoints\nEquipe conf√©rence Ouest\nVictoires\nPoints\n\n\n\n\nMilwaukee Bucks\n58\n9589\nDenver Nuggets\n53\n9495\n\n\nBoston Celtics\n57\n9671\nMemphis Grizzlies\n51\n9587\n\n\nPhiladelphia 76ers\n54\n9448\nSacramento Kings\n48\n9898\n\n\nCleveland Cavaliers\n51\n9205\nPhoenix Suns\n45\n9319\n\n\nNew York Knicks\n47\n9514\nGolden State Warriors\n44\n9753\n\n\nBrooklyn Nets\n45\n9295\nLA Clippers\n44\n9314\n\n\nMiami Heat\n44\n8977\nLos Angeles Lakers\n43\n9608\n\n\nAtlanta Hawks\n41\n9711\nMinnesota Timberwolves\n42\n9494\n\n\nToronto Raptors\n41\n9254\nNew Orleans Pelicans\n42\n9378\n\n\nChicago Bulls\n40\n9276\nOklahoma City Thunder\n40\n9633\n\n\nIndiana Pacers\n35\n9535\nDallas Mavericks\n38\n9366\n\n\nWashington Wizards\n35\n9279\nUtah Jazz\n37\n9600\n\n\nOrlando Magic\n34\n9136\nPortland Trail Blazers\n33\n9299\n\n\nCharlotte Hornets\n27\n9098\nSan Antonio Spurs\n22\n9269\n\n\nDetroit Pistons\n17\n9045\nHouston Rockets\n22\n9081\n\n\n\n\n\n\nLes √©quipes ayant remport√© le titre NBA deux ann√©es cons√©cutives\n\n¬†¬†¬†¬†¬†¬†Remporter le titre NBA deux ann√©es de suite est un exploit rare, compte tenu de la forte comp√©titivit√© entre les franchises. Parmi celles qui y sont parvenues, 8 √©quipes ont r√©ussi √† conserver leur titre deux saisons cons√©cutives : Los Angeles Lakers, Syracuse Nationals, Boston Celtics, Detroit Pistons, Chicago Bulls, Houston Rockets, Miami Heat, Golden State Warriors. Il est √† noter que les Boston Celtics d√©tiennent un record historique avec 8 titres cons√©cutifs entre les saisons 1958-1959 et 1965-1966.\n\n\nPr√©sentation des methodes utilis√©es\n¬†¬†¬†¬†¬†¬†Pour r√©pondre aux diff√©rentes questions, nous avons cr√©√© une classe Reponse dans laquelle chaque question fait l‚Äôobjet d‚Äôune m√©thode d√©di√©e. Ces m√©thodes int√®grent des param√®tres afin de permettre aux utilisateurs d‚Äôafficher les r√©sultats tout en pouvant appliquer des filtres personnalis√©s.\n¬†¬†¬†¬†¬†¬†Concernant la premi√®re question obligatoire, nous avons d√©fini une m√©thode prenant en param√®tres nb_victoire_min (le nombre minimal de titres √† remporter), debut_periode et fin_periode (ann√©es d√©limitant la p√©riode d‚Äô√©tude). Ces deux derniers param√®tres permettent de filtrer les donn√©es sur la p√©riode souhait√©e. Ensuite, nous r√©cup√©rons les vainqueurs de chaque saison et comptons les titres remport√©s par chaque franchise √† l‚Äôaide de manipulations avec pandas. Enfin, nous ne conservons que celles ayant remport√© un nombre de titres sup√©rieur ou √©gal √† celui indiqu√©.\n¬†¬†¬†¬†¬†¬†Pour la deuxi√®me question obligatoire, une autre m√©thode a √©t√© cr√©√©e avec les param√®tres season (saison d‚Äôint√©r√™t) et end (date de fin pour limiter les matchs pris en compte). Si le param√®tre end est fourni, le classement est calcul√© √† cette date pr√©cise ; sinon, le classement √† la fin de la saison r√©guli√®re est affich√©. Ces param√®tres permettent de filtrer les confrontations selon la saison choisie et, le cas √©ch√©ant, selon la date limite.\n¬†¬†¬†¬†¬†¬†Pour r√©pondre √† la dixi√®me question relative aux √©quipes ayant remport√© au moins 2 ann√©es cons√©cutives le titre, une m√©thode prenant les param√®tres N, debut_periode et fin_periode a √©t√© con√ßue. Elle permet d‚Äôidentifier les √©quipes ayant remport√© le titre au moins N ann√©es cons√©cutives dans une p√©riode d√©finie. Apr√®s filtrage des donn√©es, les vainqueurs sont extraits, et un traitement via la manipulation de dictionnaires permet de rep√©rer les franchises ayant remport√© le titre plusieurs ann√©es de suite.\n¬†¬†¬†¬†¬†¬†De fa√ßon g√©n√©rale, les autres m√©thodes de la classe Reponse suivent une logique similaire : elles exploitent des param√®tres pour affiner les r√©sultats et r√©pondre plus pr√©cis√©ment aux besoins de l‚Äôutilisateur, contrairement √† des m√©thodes fixes sans personnalisation. Ces traitements sont rendus possibles gr√¢ce aux fonctionnalit√©s offertes par le package pandas.\n\n\n\n\n\nDiagramme de classes\n\n\n\n\n\n\n\nPr√©sentation de la probl√©matique consid√©r√©e et de la r√©ponse apport√©e\n¬†¬†¬†¬†¬†¬†Comme √©nonc√© et justifi√© dans l‚Äôintroduction, notre probl√©matique est la suivante : Pr√©dire la dur√©e de carri√®re des joueurs en NBA.\n\nComment nous sommes parvenus √† r√©pondre √† cette probl√©matique ?\n¬†¬†¬†¬†¬†¬†Pour r√©pondre √† la probl√©matique, nous avons choisi d‚Äôutiliser un mod√®le de r√©gression lin√©aire multiple. Ce mod√®le permet de pr√©dire la dur√©e de carri√®re des joueurs √† partir de leurs caract√©ristiques individuelles.\nNous avons identifi√© la table common_player_info comme base pertinente pour l‚Äôentra√Ænement. Cette table contient plusieurs informations sur les joueurs. Les variables explicatives que nous avons s√©lectionn√©es sont : la date de naissance, l‚Äôann√©e de draft, le poste occup√© sur le terrain, la taille, le poids. La dur√©e de carri√®re est notre variable cible.\nPlut√¥t que d‚Äôutiliser directement le mod√®le de r√©gression lin√©aire de scikit-learn (LinearRegression), nous avons choisi d‚Äôimpl√©menter notre propre classe, nomm√©e LinearRegression. Cette classe permet :\n\nd‚Äôentra√Æner (ajuster) un mod√®le;\nde r√©aliser des pr√©dictions;\nd‚Äô√©valuer les performances du mod√®le √† l‚Äôaide de k-fold cross-validation 1.\n\nNous avons √©galement :\n\nint√©gr√© une m√©thode de r√©gression Ridge pour traiter d‚Äô√©ventuels probl√®mes de multicolin√©arit√©;\ncalcul√© les intervalles de confiance pour les pr√©dictions.\n\nUn diagramme de flux d√©taillant le fonctionnement de notre classe est pr√©sent√© en annexe.\nFinalement, pour des probl√®mes de multicolin√©arit√© entre certaines variables explicatives et des ajustements peu satisfaisants, nous avons choisi d‚Äôinclure l‚Äô√¢ge √† la draft ( ann√©e de draft - ann√©e de naissance) et la position (poste sur le terrain), ces deux variables s‚Äô√©tant r√©v√©l√©es plus pertinentes que la taille ou le poids.\nNotre mod√®le final avait donc pour formule :\n\\[\\begin{equation}\n  \\text{Dur√©e de carri√®re} = \\beta_0 + \\beta_1 \\cdot \\text{√Çge √† la draft} + \\sum_{j=1}^{k} \\beta_{j+1} \\cdot \\text{Position}_j + \\varepsilon\n(\\#eq:label)\n\\end{equation}\\]\no√π \\(\\beta_0\\) est l‚Äôordonn√©e √† l‚Äôorigine (intercept), \\(\\beta_1\\) est le coefficient associ√© √† l‚Äô√¢ge √† la draft, \\(\\text{Position}_j\\) repr√©sente les variables indicatrices pour les diff√©rentes positions, \\(\\beta_{j+1}\\) sont les coefficients qui leur sont associ√©s et \\(\\varepsilon\\) est le terme d‚Äôerreur. Les variables omises, telles que la taille et le poids, influencent directement la position d‚Äôun joueur. Par exemple, un joueur plus grand est souvent appel√© √† jouer au poste de pivot. De m√™me, un joueur poss√©dant une grande taille, un poids important et une bonne rapidit√© physique sera g√©n√©ralement affect√© aux postes d‚Äôailier fort et de pivot simultan√©ment. En gardant ces variables, il y‚Äôavait une incoh√©rence dans les r√©sultats certainement d√ªe √† la multicolin√©arit√© entre elles et la position. La reponse √† la question 3 confirme le lien entre le poste occup√© sur le terrain, la taille et le poids (cf.¬†figure en annexes).\nLe notebook apprentissage_automatique.ipynb permet de retrouver les r√©sultats dans la section d‚Äôapr√®s.\n\n\nReponse apport√©e √† la probl√©matique\n\nR√©sultats de l‚Äôentra√Ænement et interpr√©tations\nNous avons r√©entra√Æn√© notre mod√®le sur l‚Äôensemble des donn√©es disponibles, sans r√©server de jeu de test.\nPourquoi ? Pour exploiter toute l‚Äôinformation disponible et am√©liorer la pr√©cision du mod√®le final.\nMais comment garantir sa fiabilit√© ?\nNous nous basons enti√®rement sur les r√©sultats issus de la validation crois√©e : la moyenne des erreurs quadratiques moyennes (RMSE) √©tait de 4,59, avec une faible variabilit√© (√©cart-type de 0,12).\nCela indique une bonne capacit√© de g√©n√©ralisation du mod√®le.\n\n\nAnalyse des effets\nLe mod√®le confirme des tendances observ√©es dans la r√©alit√© : - Plus l‚Äô√¢ge √† la draft est √©lev√©, plus la dur√©e de carri√®re est courte. - Les joueurs occupant le poste de pivot ou ailier fort ont en moyenne une carri√®re plus longue, gr√¢ce √† leur polyvalence.\nSur la figure des resultats de l‚Äôentrainement du mod√®le (graphique a)), nous constatons que les performances de notre classe LinearRegression sont comparables √† celles du module linear_model de Scikit-learn.\n\n\nExemple de pr√©diction\nPour un joueur de 18 ans, s√©lectionn√© au poste de pivot, le mod√®le pr√©dit : - Dur√©e de carri√®re estim√©e : 7,6 ans - Intervalle de confiance : [4,3 ; 10,9]\n\n\nLimites\nM√™me si l‚Äôintervalle de confiance renforce la cr√©dibilit√© des r√©sultats, il convient de noter que la RMSE reste √©lev√©e (environ 4,6 ans), ce qui signifie que les erreurs de pr√©diction peuvent √™tre importantes dans certains cas.\n\n\n\n\n\n\nResultat de notre mod√®le VS celui de Sci-kit Learn\n\n\n\n\n\n\n\nMoyennes des erreurs quadratiques moyennes\n\n\n\n\n\n\nR√©sultats de l‚Äôentrainement du mod√®le\n\n\n\n\n\n\n\nLien entre les classe cr√©√©es et l‚Äôapplication\n¬†¬†¬†¬†¬†¬†Dans un souci de modularit√©, nous avons structur√© notre application autour de trois interfaces principales, chacune encapsul√©e dans une classe distincte.\n\nLa premi√®re interface est la page d‚Äôaccueil, qui ne fait pas directement appel aux classes LinearRegression ou Reponse. Elle sert principalement d‚Äôintroduction √† l‚Äôapplication mais est bas√©e sur une classe.\nLa seconde interface, Reponses aux questions, repose sur la classe Reponse (d√©taill√© en annexe) pour r√©pondre aux requ√™tes des utilisateurs. Elle permet √©galement d‚Äôappliquer des filtres sur les r√©sultats, comme l‚Äôaffichage des m√©dianes, moyennes selon le choix de l‚Äôutilisateur, ou encore un filtrage par date.\nEnfin, la page Machine Learning pr√©sente les r√©sultats d‚Äôentra√Ænement du mod√®le ajust√© ainsi que sa validation. Elle offre aussi la possibilit√© √† l‚Äôutilisateur de faire des pr√©dictions en renseignant les caract√©ristiques (features) d‚Äôun joueur de la NBA. Cette page utilise une classe CareerPrediction, laquelle d√©pend elle-m√™me de la classe LinearRegression.\n\n\n\nArchitecture du projet\n¬†¬†¬†¬†¬†¬†La figure ci-dessous pr√©sente l‚Äôarchitecture g√©n√©rale du projet. Chaque dossier a √©t√© con√ßu pour r√©pondre √† une pr√©occupation sp√©cifique. Pour que ces dossiers soient reconnus comme des paquets Python, des fichiers __init__.py y ont √©t√© ajout√©s avec la d√©finition des modules. Un fichier setup.py permet de g√©rer les d√©pendances internes et de faciliter les interactions entre les diff√©rents modules.\nEnfin, tous les packages n√©cessaires √† l‚Äôex√©cution des codes ainsi que leurs versions sont list√©s dans le fichier requirements.txt.\n\n\n\n\n\nArchitecture du projet\n\n\n\n\n\n\nPackages et biblioth√®ques utilis√©s\n¬†¬†¬†¬†¬†¬†Le projet repose sur plusieurs biblioth√®ques Python essentielles, parmi lesquelles :\n\nPandas : utilis√©e pour la manipulation de donn√©es tabulaires, elle facilite le traitement et la transformation des donn√©es structur√©es.\nMatplotlib : permet de cr√©er des visualisations d√©taill√©es et personnalis√©es sous forme de graphiques.\nNumPy : utile pour les op√©rations math√©matiques et la gestion efficace de tableaux multidimensionnels.\nScikit-learn : utilis√©e pour les t√¢ches de machine learning. Elle nous a permis de comparer nos impl√©mentations personnalis√©es avec les mod√®les standards de la biblioth√®que).\nShiny : a √©t√© utilis√©e pour d√©velopper l‚Äôinterface utilisateur de l‚Äôapplication.\n\nCes outils ont √©t√© essentiels pour mener √† bien les √©tapes d‚Äôanalyse, de mod√©lisation et de visualisation du projet.\n\n\nCouverture des tests\n¬†¬†¬†¬†¬†¬†La quasi-totalit√© des classes et fonctions a √©t√© test√©e, y compris celles li√©es aux pages de l‚Äôapplication web d√©velopp√©e.\nAu total, 214 tests ont √©t√© r√©alis√©s, assurant une couverture de plus de 94% du code.\n\n\n\n\n\nTests effectu√©s"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#utilisation-de-lapplication",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#utilisation-de-lapplication",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Utilisation de l‚Äôapplication",
    "text": "Utilisation de l‚Äôapplication\n\nPage d‚ÄôAccueil\nLa page Accueil pr√©sente des statistiques g√©n√©rales sur la NBA :\n\nNombre total de joueurs,\nNombre d‚Äô√©quipes,\nNombre d‚Äôuniversit√©s repr√©sent√©es.\n\nL‚Äôutilisateur peut √©galement explorer avec des filtres :\n\nLe nombre de joueurs Greatest 2,\nL‚Äô√©volution du nombre de matchs par saison,\nLa r√©partition des positions de joueurs entre deux ann√©es d√©finies.\n\nCes √©l√©ments sont illustr√©s en figure ci-dessous.\n\n\n\n\n\nPage d‚Äôaccueuil de l‚Äôapplication\n\n\n\n\n\n\nPage de reponses aux questions\nCette page permet √† l‚Äôutilisateur de :\n\nVisualiser les r√©sultats des analyses,\nInteragir avec la base via des champs de saisie dynamiques.\n\nExemple : pour la question ‚ÄúClasser les universit√©s selon le nombre de joueurs form√©s‚Äù, l‚Äôutilisateur peut :\n\nD√©finir une p√©riode,\nS√©lectionner le Top N universit√©s.\n\nLes r√©ponses sont regroup√©es par cat√©gories (Joueurs, √âquipes, etc.).\nLa figure ci-dessous illustre ce qui a √©t√© dis en ammont.\n\n\n\n\n\nPage de reponses aux questions de l‚Äôapplication\n\n\n\n\n\n\nPage des r√©sultats du mod√®le d‚Äôapprentissage supervis√© et de pr√©diction de la dur√©e de carri√®re : Machine Learning\n¬†¬†¬†¬†¬†¬†Sur cette page nous affichons les r√©sultats du mod√®le √† la suite de son entra√Ænement puis la validation du mod√®le. On voit que l‚Äôerreur quadratique moyenne 3 tourne autour de 4,6 en moyenne (le graphique √† droite sur la juste en bas). Ensuite pour permettre √† l‚Äôutilisateur de pr√©dire la dur√©e de carri√®re d‚Äôun joueur lambda, des champs de saisies sont affich√©s. Il pourra donc entrer la date de naissance du joueur, la date de sa draft et choisir sa position. M√™me s‚Äôil ne connait pas les date exacte, ce n‚Äôest pas grave, c‚Äôest l‚Äôann√©e qui importe. Ensuite en cliquant sur le bouton Predire il obtient la pr√©diction avec un intervalle de confiance.\nL‚Äôapplication a √©t√© d√©ploy√©e sur un server posit (version gratuite) et responsive donc s‚Äôadapte √† diff√©rentes tailles d‚Äô√©cran. Pour acceder √† l‚Äôapplication cliquez ici.\n\n\n\n\n\nPage des r√©sultats du mod√®le d‚Äôapprentissage supervis√© et de pr√©diction de la dur√©e de carri√®re"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#retour-dexp√©rience",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#retour-dexp√©rience",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Retour d‚Äôexp√©rience",
    "text": "Retour d‚Äôexp√©rience\n\nR√©ponse aux attentes\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†Au cours du projet, nous avons √©t√© confront√©s √† plusieurs d√©fis. En effet, au d√©part, les attentes concernant les questions et le rendu n‚Äô√©taient pas totalement claires. Toutefois, gr√¢ce aux explications de notre tutrice Manon EVAIN, les choses se sont rapidement clarifi√©es. Elle nous a notamment encourag√© √† structurer notre code en classes, √† les tester rigoureusement, et surtout √† impl√©menter nous-m√™mes le mod√®le d‚Äôapprentissage automatique.\nCes recommandations, combin√©es √† nos efforts, nous ont pouss√©s √† aller au-del√† des exigences initiales : nous avons ainsi modularis√© l‚Äôensemble du projet, afin d‚Äôen faciliter la maintenabilit√© et la r√©utilisabilit√©.\nConcernant les r√©ponses aux questions, il a fallu faire preuve de minutie et de rigueur, notamment pour identifier les observations manquantes susceptibles de fausser les r√©sultats par rapport aux donn√©es officielles disponibles en ligne. √Ä la fin du projet, nous avons aussi d√ª g√©rer un probl√®me de changement des noms d‚Äô√©quipes dans le temps. Pour y rem√©dier rapidement, nous avons d√©velopp√© une fonction sp√©cifique, √©vitant ainsi des erreurs dans nos analyses.\nEnfin, la mise en ≈ìuvre manuelle du mod√®le d‚Äôapprentissage automatique a √©t√© particuli√®rement b√©n√©fique. Elle nous a permis non seulement de renforcer nos comp√©tences en statistiques, mais aussi de mieux comprendre leur impl√©mentation informatique.\n\n\nTravail en √©quipe\nD√®s le d√©but, nous avons d√©fini les diff√©rentes questions, puis nous les avons r√©parties √©quitablement, chacun r√©pondant en moyenne √† trois questions. L‚Äôutilisation de GitHub a grandement facilit√© le travail collaboratif, notamment pour surmonter les contraintes de distance g√©ographique.\nPar ailleurs, lors de nos s√©ances de travail en autonomie, nous avons veill√© √† partager nos m√©thodes respectives, afin que chacun puisse comprendre et valider les approches adopt√©es par les autres. Cette coop√©ration constante nous a permis de rester align√©s tout au long du projet.\n\n\nSatisfaction du travail final\n¬†¬†¬†¬†¬†¬†Nous sommes satisfaits du travail accompli, bien que certains aspects puissent √™tre am√©lior√©s. Le projet a demand√© un fort investissement, mais a suscit√© un r√©el int√©r√™t. Nous avons explor√© de nouvelles comp√©tences, notamment en d√©veloppant une interface graphique. Cela nous a pouss√©s √† sortir de notre zone de confort, tout en veillant √† la qualit√© du rendu. En somme, ce fut une belle opportunit√© d‚Äôapprentissage, tant sur le plan technique qu‚Äôen √©quipe."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#conclusion",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#conclusion",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Conclusion",
    "text": "Conclusion\n¬†¬†¬†¬†¬†¬†Ce projet s‚Äôinscrit pleinement dans le domaine de l‚Äôinformatique appliqu√©e au traitement de donn√©es. √Ä travers l‚Äôanalyse des donn√©es issues de la NBA, nous avons mis en oeuvre une cha√Æne compl√®te de traitement : du nettoyage des donn√©es √† leur exploration statistique, jusqu‚Äô√† la mise en place d‚Äôun mod√®le de machine learning supervis√©.\nNous avons ainsi d√©velopp√© une solution permettant de pr√©dire la dur√©e de carri√®re des joueurs √† partir de variables explicatives pertinentes, en mobilisant des techniques issues de la r√©gression lin√©aire et en appliquant des m√©thodes de validation crois√©e. Ce travail nous a permis de mieux comprendre les enjeux li√©s √† la pr√©paration des donn√©es (feature engineering, gestion des valeurs manquantes, encodage des variables cat√©gorielles), √©tape cruciale en apprentissage automatique. Ces √©tapes nous ont √©galement permis d‚Äôapporter des r√©ponses correctes aux diff√©rentes questions pos√©es.\nPar ailleurs, le projet a donn√© lieu √† la cr√©ation d‚Äôune application interactive en Python (framework Shiny for Python), offrant une visualisation dynamique des donn√©es et des r√©sultats, et illustrant l‚Äôint√©r√™t d‚Äôoutils informatiques modernes pour valoriser les analyses.\nCe projet nous a permis de consolider nos comp√©tences en programmation, en traitement de donn√©es et en machine learning, tout en approfondissant notre capacit√© √† travailler en √©quipe sur un projet structur√©, dans une logique proche des pratiques professionnelles en data science."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#liste-des-sigles-et-abr√©viations",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#liste-des-sigles-et-abr√©viations",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Liste des sigles et abr√©viations",
    "text": "Liste des sigles et abr√©viations\n\nCSV : Comma-Separated Values\nFormat de fichier texte utilis√© pour stocker des donn√©es tabulaires avec des virgules comme s√©parateurs.\nIA : Intelligence Artificielle\nEnsemble de techniques visant √† simuler l‚Äôintelligence humaine par des machines.\nIMC : Indice de Masse Corporelle\nIndicateur utilis√© pour √©valuer la corpulence d‚Äôune personne √† partir de son poids et de sa taille.\nJPG : Joint Photographic Experts Group\nFormat de fichier image compress√© couramment utilis√©.\nNBA : National Basketball Association\nLigue professionnelle de basketball nord-am√©ricaine.\nOLS : Ordinary Least Squares (Moindres Carr√©s Ordinaires)\nM√©thode d‚Äôestimation en r√©gression lin√©aire consistant √† minimiser la somme des carr√©s des erreurs.\nPNG : Portable Network Graphics\nFormat d‚Äôimage sans perte de qualit√©, adapt√© au web.\nRMSE : Root Mean Squared Error (Erreur Quadratique Moyenne Racine)\nMesure statistique de l‚Äô√©cart type des r√©sidus de pr√©diction.\nUSA : √âtats-Unis d‚ÄôAm√©rique\nPays dont les franchises de la NBA sont majoritairement issues."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#sec:ml",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#sec:ml",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Annexe 1 : Algorithme utilis√© pour le machine learning Mod√®le de regression lin√©aire",
    "text": "Annexe 1 : Algorithme utilis√© pour le machine learning Mod√®le de regression lin√©aire\n\n\n\n\n\nDiagramme de flux des m√©thodes de la classe LinearRegression"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#sec:m2",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#sec:m2",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Annexe 2 : Description des classes",
    "text": "Annexe 2 : Description des classes\nClasse Reponse\n¬†¬†¬†¬†¬†¬†La m√©thode init() avant d‚Äôinitialiser la classe Reponse doit :\n\nV√©rifier que l‚Äôargument data est bien un dictionnaire.\nV√©rifier que toutes les valeurs sont des objets de type pandas.DataFrame.\n\n¬†¬†¬†¬†¬†¬†La m√©thode equip_victoires_defaites_saison() retourne une table contenant le nombre de victoires ou de d√©faites pour chaque √©quipe entre les saisons donn√©es.\n¬†¬†¬†¬†¬†¬†La m√©thode prop_joueurs_en_nba_selon_universite_de_formation() retourne une table contenant le Top N des universit√©s selon le nombre de leurs √©tudiants ayant √©volu√© en NBA. Si graph = True, la m√©thode renvoie √©galement un diagramme √† barres.\n¬†¬†¬†¬†¬†¬†La m√©thode stat_sur_taille_et_poids_par_poste() retourne une table contenant la statistique demand√©e sur la taille et le poids des joueurs selon leur poste.\n¬†¬†¬†¬†¬†¬†La m√©thode equipe_remporte_au_moins_N_fois_le_titre() retourne une table listant les √©quipes ayant remport√© au moins le nombre de titres requis.\n¬†¬†¬†¬†¬†¬†La m√©thode premiers_choix_draft_N_derniere_saison() retourne une table contenant les premiers choix de draft sur les N derni√®res saisons.\n¬†¬†¬†¬†¬†¬†La m√©thode vainqueur_titre_NBA_saisons() retourne une table contenant les vainqueurs du titre NBA sur la p√©riode choisie ou sur les saisons d‚Äôune p√©riode d√©finie.\n¬†¬†¬†¬†¬†¬†La m√©thode equipe_qui_remporte_N_fois_daffile_le_titre() retourne une table listant les √©quipes ayant remport√© au moins N titres cons√©cutifs.\n¬†¬†¬†¬†¬†¬†La m√©thode nombre_victoires_ou_defaites_equipe_les_N_dernieres_saisons() retourne une table listant les √©quipes ayant obtenu le plus de victoires ou de d√©faites pour chaque saison d‚Äôune p√©riode donn√©e. Elle renvoie le nombre de d√©faites si defaite = True.\n¬†¬†¬†¬†¬†¬†La m√©thode top_N_nb_joueurs_par_pays() retourne une table indiquant la r√©partition des joueurs NBA selon leur pays d‚Äôorigine. Si graph = True, elle retourne √©galement un diagramme √† barres.\n¬†¬†¬†¬†¬†¬†La m√©thode classement_conferences() retourne un dictionnaire contenant deux tables, un pour chaque conf√©rence (Est et Ouest). Pour chaque table, on retrouve le classement des √©quipes selon leur nombre de victoires obtenues en saison r√©guli√®re.\nClasse HomeFunction\n¬†¬†¬†¬†¬†¬†La m√©thode init() initialise la classe avec un dictionnaire de DataFrames contenant les donn√©es √† visualiser dans l‚Äôinterface.\n¬†¬†¬†¬†¬†¬†La m√©thode create_line_chart_nb_match() retourne un graphique repr√©sentant l‚Äô√©volution du nombre de matchs jou√©s par saison, sur une p√©riode donn√©e.\n¬†¬†¬†¬†¬†¬†La m√©thode return_data_home() retourne les donn√©es principales affich√©es sur la page d‚Äôaccueil dans l‚Äôinterface.\n¬†¬†¬†¬†¬†¬†La m√©thode create_dunut_chart_of_position_distribution() retourne un graphique en forme de donut chart illustrant la r√©partition des joueurs selon leur poste, pour une p√©riode donn√©e.\n¬†¬†¬†¬†¬†¬†La m√©thode nombre_univ() retourne un entier repr√©sentant le nombre total d‚Äôuniversit√©s ayant form√© des joueurs NBA pr√©sent dans la base de donn√©es.\n¬†¬†¬†¬†¬†¬†La m√©thode return_greatest_players() retourne une table des joueurs les plus marquants (statut de greatest) sur une p√©riode donn√©e, ainsi que le nombre de joueurs inclus.\n¬†¬†¬†¬†¬†¬†La m√©thode return_nb_players() retourne un entier repr√©sentant le nombre total de joueurs dans la base.\n¬†¬†¬†¬†¬†¬†La m√©thode return_nb_teams() retourne un entier repr√©sentant le nombre total d‚Äô√©quipes pr√©sentes dans les donn√©es.\nClasse ReponseFunction\n¬†¬†¬†¬†¬†¬†La m√©thode init() initialise la classe avec un dictionnaire de DataFrames contenant les donn√©es n√©cessaires √† l‚Äôaffichage.\n¬†¬†¬†¬†¬†¬†La m√©thode prop_university() retourne un diagramme repr√©sentant les universit√©s ayant form√© le plus de joueurs NBA, selon une p√©riode et un top N d√©finis.\n¬†¬†¬†¬†¬†¬†La m√©thode statistique_taille_poids() retourne une table contenant une statistique (moyenne, m√©diane, etc.) sur la taille ou le poids des joueurs NBA, utilis√©e pour une repr√©sentation tabulaire dans l‚Äôinterface.\n¬†¬†¬†¬†¬†¬†La m√©thode au_moins_N_fois_le_titre() retourne une table listant les √©quipes ayant remport√© au moins un certain nombre de titres NBA sur une p√©riode donn√©e. Les donn√©es sont destin√©es √† un affichage dans l‚Äôinterface.\n¬†¬†¬†¬†¬†¬†La m√©thode remporter_titre() retourne une table avec les √©quipes ayant remport√© le titre NBA au cours de la p√©riode sp√©cifi√©e. Affichage sous de tableau dans l‚Äôinterface.\n¬†¬†¬†¬†¬†¬†La m√©thode nombre_victoires_defaites() retourne une table indiquant l‚Äô√©quipe avec le plus de nombre de victoires ou de d√©faites sur la saison r√©guli√®re, pour une p√©riode donn√©e.\n¬†¬†¬†¬†¬†¬†La m√©thode numero_1_draft() retourne une table listant les joueurs s√©lectionn√©s en premier lors de la draft pour les nb derni√®res ann√©es. Id√©al pour un tableau historique.\n¬†¬†¬†¬†¬†¬†La m√©thode distribution_pays() retourne √† la fois une table et un diagramme illustrant la r√©partition des joueurs NBA par pays d‚Äôorigine, selon un Top N d√©fini.\n¬†¬†¬†¬†¬†¬†La m√©thode recup_min_max_date() retourne deux cha√Ænes de caract√®res repr√©sentant la date de d√©but et de fin d‚Äôune saison. Elle est utilis√©e pour ajuster dynamiquement les filtres temporels dans l‚Äôinterface.\n¬†¬†¬†¬†¬†¬†La m√©thode display_conference() retourne le classement des √©quipes dans les conf√©rences Est et Ouest pour une saison donn√©e.\nClasse LinearRegression\n¬†¬†¬†¬†¬†¬†La m√©thode init() : v√©rifie types, colonnes, seuil et intercept ; nettoie df ; cr√©e X et y\n¬†¬†¬†¬†¬†¬†La m√©thode create_x_y(): dichotomise les variables cat√©gorielles ; ajoute un intercept si demand√©\n¬†¬†¬†¬†¬†¬†La m√©thode get_dummies() : transforme une variable cat√©gorielle en colonnes indicatrices\n¬†¬†¬†¬†¬†¬†La m√©thode fit() : estimation OLS par pseudoinverse ; avertit si multicolin√©arit√©\n¬†¬†¬†¬†¬†¬†La m√©thode fit_ridge() : estimation Ridge avec param√®tre alpha ; intercept non p√©nalis√©\n¬†¬†¬†¬†¬†¬†La m√©thode compute_confidence_interval() : calcule les intervalles de confiance des coefficients\n¬†¬†¬†¬†¬†¬†La m√©thode compute_rmse() : calcule la racine de l‚Äôerreur quadratique moyenne\n¬†¬†¬†¬†¬†¬†La m√©thode predict() : g√©n√®re les pr√©dictions via X %*% Beta\n¬†¬†¬†¬†¬†¬†La m√©thode perfom_linear_reg() : pipeline complet (fit, predict, IC, RMSE) en OLS ou Ridge\n¬†¬†¬†¬†¬†¬†La m√©thode create_k_fold(I) : g√©n√®re al√©atoirement k sous-√©chantillons pour validation crois√©e\n¬†¬†¬†¬†¬†¬†La m√©thode k_fold() : ex√©cute la validation crois√©e k-fold et renvoie les ***RMSE}\nClasse CareerPrediction\n¬†¬†¬†¬†¬†¬†La m√©thode init(data, use_ridge=FALSE, x_vars=list(), alpha=1.0, k=5) v√©rifie que data est un data.frame, initialise les attributs (use_ridge, alpha, k, etc.), clone les donn√©es et appelle prepare_data().\n¬†¬†¬†¬†¬†¬†La m√©thode prepare_data() convertit draft_year et extrait birth_year de birthdate, remplace ‚Äúundrafted‚Äù par 0, filtre les joueurs dont la carri√®re s‚Äôach√®ve \\(\\leq\\) 2022 et garde ceux dont age_at_draft &gt; 0. Elle d√©finit x_vars = c(‚Äúage_at_draft‚Äù, ‚Äúposition‚Äù) et instancie un mod√®le LinearRegression.\n¬†¬†¬†¬†¬†¬†La m√©thode run_regression() reconstruit le mod√®le LinearRegression avec y_var = ‚Äúseason_exp‚Äù et x_vars = c(‚Äúage_at_draft‚Äù,‚Äúposition‚Äù). Elle Lance perfom_linear_reg(use_ridge, alpha), renvoie les r√©sultats (coefficients, IC, RMSE).\n¬†¬†¬†¬†¬†¬†La m√©thode plot_k_fold() ex√©cute k_fold(k) pour obtenir les RMSE} par fold, puis cr√©e un graphique matplotlib*** et retourne l‚Äôobjet figure.\n¬†¬†¬†¬†¬†¬†La m√©thode predict_career_duration(birthdate, draft_year, position, coef_df) calcule age-at-draft, construit le vecteur de features (dont les dummy variables de position), r√©cup√®re l‚Äôintercept et les coefficients estim√©s dans coef_df. Elle agr√®ge contributions de chaque variable pour produire duree_predite et son intervalle_confiance.\nClasse ExportFiles\n¬†¬†¬†¬†¬†¬†La m√©thode init() initialise l‚Äôexporteur (aucun param√®tre requis)\n¬†¬†¬†¬†¬†¬†La m√©thode normalize_path(path) normalise un chemin en rempla√ßant les backslashes par des slashs.\n¬†¬†¬†¬†¬†¬†La m√©thode export_to_csv_format(table, path) v√©rifie que table est un DataFrame ou Series et que path est une cha√Æne, normalise le chemin et exporte la table en ***CSV} (sans index)\n¬†¬†¬†¬†¬†¬†La m√©thode export_to_xlsx_format(table, path) effectue les m√™mes contr√¥les de type que pour le CSV, convertit une Series en DataFrame si n√©cessaire et exporte au format Excel .xlsx\n¬†¬†¬†¬†¬†¬†La m√©thode export_to_jpg(img, path) v√©rifie que img est une matplotlib.figure.Figure et que path est une cha√Æne, normalise le chemin et enregistre la figure au format JPG\n¬†¬†¬†¬†¬†¬†La m√©thode export_to_png(img, path) effectue m√™mes contr√¥les que pour JPG normalise le chemin et enregistre la figure au format PNG\n\nAnnexes 3 : Lien visuel √©ventuel entre le poste occup√© sur le terrain, la taille et le poids des joueurs\n\n\n\n\n\nTaille et poids m√©dians par poste en NBA\n\n\n\n\n¬†¬†¬†¬†¬†¬†Sur cette figure, on peut voir que les ailiers forts/pivots, pivots et les pivots/ailiers forts sont les plus lourds et les plus grands en m√©diane. Ce qui traduirait une corr√©lation intrins√®que entre le poste, la taille et le poids des joueurs."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#footnotes",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#footnotes",
    "title": "Pr√©diction de la dur√©e de carri√®re des joueurs NBA sur la base de leurs caract√©ristiques et parcours individuels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLa validation crois√©e est une technique permettant d‚Äô√©valuer la performance d‚Äôun mod√®le en le testant sur plusieurs sous-ensembles des donn√©es. Elle permet de mieux estimer la g√©n√©ralisation du mod√®le (Run, C√©va√´r, and Dub√© 2023).‚Ü©Ô∏é\nUn joueur Greatest est un joueur faisant partie de la s√©lection des 75 meilleurs joueurs de l‚Äôhistoire de la NBA, d√©sign√©e √† l‚Äôoccasion du 75e anniversaire de la ligue.‚Ü©Ô∏é\nL‚Äôerreur quadratique moyenne (Root Mean Squared Error ou RMSE) est d√©finie comme la racine carr√©e de la moyenne des carr√©s des √©carts entre les valeurs pr√©dites \\(\\hat{y}_i\\) et les valeurs r√©elles \\(y_i\\) : \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2}\n\\] Plus la RMSE est faible, plus le mod√®le est pr√©cis.‚Ü©Ô∏é"
  }
]